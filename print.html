<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">数据存储</a></li><li class="chapter-item expanded affix "><li class="part-title"></li><li class="chapter-item expanded "><div><strong aria-hidden="true">1.</strong> oracle</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="oracle/Oracle小知识点.html"><strong aria-hidden="true">1.1.</strong> Oracle小知识点</a></li><li class="chapter-item expanded "><a href="oracle/oracledocker快速搭建.html"><strong aria-hidden="true">1.2.</strong> oracledocker快速搭建</a></li><li class="chapter-item expanded "><a href="oracle/oracle多表更新方法.html"><strong aria-hidden="true">1.3.</strong> oracle多表更新方法</a></li></ol></li><li class="chapter-item expanded "><a href="MongoDB/index.html"><strong aria-hidden="true">2.</strong> MongoDB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="MongoDB/mongodb基本操作.html"><strong aria-hidden="true">2.1.</strong> mongodb基本操作</a></li><li class="chapter-item expanded "><a href="MongoDB/mongodb高级语法.html"><strong aria-hidden="true">2.2.</strong> mongodb高级语法</a></li><li class="chapter-item expanded "><a href="MongoDB/权限管理.html"><strong aria-hidden="true">2.3.</strong> 权限管理</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.</strong> mysql</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="mysql/mysqlfunction.html"><strong aria-hidden="true">3.1.</strong> mysqlfunction</a></li><li class="chapter-item expanded "><a href="mysql/mysqlpdump.html"><strong aria-hidden="true">3.2.</strong> mysqlpdump</a></li><li class="chapter-item expanded "><a href="mysql/mysqlzip安装.html"><strong aria-hidden="true">3.3.</strong> mysqlzip安装</a></li><li class="chapter-item expanded "><a href="mysql/mysql公用表达式.html"><strong aria-hidden="true">3.4.</strong> mysql公用表达式</a></li><li class="chapter-item expanded "><a href="mysql/mysql学习.html"><strong aria-hidden="true">3.5.</strong> mysql学习</a></li><li class="chapter-item expanded "><a href="mysql/mysql执行计划.html"><strong aria-hidden="true">3.6.</strong> mysql执行计划</a></li><li class="chapter-item expanded "><a href="mysql/空值处理函数.html"><strong aria-hidden="true">3.7.</strong> 空值处理函数</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">3.8.</strong> mysql官方文档</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="mysql/mysql官方文档/mysql程序.html"><strong aria-hidden="true">3.8.1.</strong> mysql程序</a></li><li class="chapter-item expanded "><a href="mysql/mysql官方文档/客户端.html"><strong aria-hidden="true">3.8.2.</strong> 客户端</a></li><li class="chapter-item expanded "><a href="mysql/mysql官方文档/mysql程序/index.html"><strong aria-hidden="true">3.8.3.</strong> mysql程序</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="mysql/mysql官方文档/mysql程序/使用mysql相关程序.html"><strong aria-hidden="true">3.8.3.1.</strong> 使用mysql相关程序</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="mysql/mysql日志/index.html"><strong aria-hidden="true">3.9.</strong> mysql日志</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="mysql/mysql日志/mysql日志及分类.html"><strong aria-hidden="true">3.9.1.</strong> mysql日志及分类</a></li><li class="chapter-item expanded "><a href="mysql/mysql日志/二进制日志.html"><strong aria-hidden="true">3.9.2.</strong> 二进制日志</a></li><li class="chapter-item expanded "><a href="mysql/mysql日志/慢sql日志.html"><strong aria-hidden="true">3.9.3.</strong> 慢sql日志</a></li><li class="chapter-item expanded "><a href="mysql/mysql日志/通用日志.html"><strong aria-hidden="true">3.9.4.</strong> 通用日志</a></li><li class="chapter-item expanded "><a href="mysql/mysql日志/错误日志.html"><strong aria-hidden="true">3.9.5.</strong> 错误日志</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/index.html"><strong aria-hidden="true">4.</strong> elasticSearch</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/BackUpACluster.html"><strong aria-hidden="true">4.1.</strong> BackUpACluster</a></li><li class="chapter-item expanded "><a href="elasticSearch/KQL.html"><strong aria-hidden="true">4.2.</strong> KQL</a></li><li class="chapter-item expanded "><a href="elasticSearch/dataStream.html"><strong aria-hidden="true">4.3.</strong> dataStream</a></li><li class="chapter-item expanded "><a href="elasticSearch/es堆内存如何分配.html"><strong aria-hidden="true">4.4.</strong> es堆内存如何分配</a></li><li class="chapter-item expanded "><a href="elasticSearch/scroll查询.html"><strong aria-hidden="true">4.5.</strong> scroll查询</a></li><li class="chapter-item expanded "><a href="elasticSearch/乐观并发控制.html"><strong aria-hidden="true">4.6.</strong> 乐观并发控制</a></li><li class="chapter-item expanded "><a href="elasticSearch/分布式检索.html"><strong aria-hidden="true">4.7.</strong> 分布式检索</a></li><li class="chapter-item expanded "><a href="elasticSearch/分片内部原理.html"><strong aria-hidden="true">4.8.</strong> 分片内部原理</a></li><li class="chapter-item expanded "><a href="elasticSearch/分词.html"><strong aria-hidden="true">4.9.</strong> 分词</a></li><li class="chapter-item expanded "><a href="elasticSearch/前缀搜索与模糊搜索.html"><strong aria-hidden="true">4.10.</strong> 前缀搜索与模糊搜索</a></li><li class="chapter-item expanded "><a href="elasticSearch/数据分片.html"><strong aria-hidden="true">4.11.</strong> 数据分片</a></li><li class="chapter-item expanded "><a href="elasticSearch/映射.html"><strong aria-hidden="true">4.12.</strong> 映射</a></li><li class="chapter-item expanded "><a href="elasticSearch/杂项学习.html"><strong aria-hidden="true">4.13.</strong> 杂项学习</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引和分片.html"><strong aria-hidden="true">4.14.</strong> 索引和分片</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引管理.html"><strong aria-hidden="true">4.15.</strong> 索引管理</a></li><li class="chapter-item expanded "><a href="elasticSearch/评分标准与相关性.html"><strong aria-hidden="true">4.16.</strong> 评分标准与相关性</a></li><li class="chapter-item expanded "><a href="elasticSearch/集群内原理.html"><strong aria-hidden="true">4.17.</strong> 集群内原理</a></li><li class="chapter-item expanded "><a href="elasticSearch/Datamanagement/index.html"><strong aria-hidden="true">4.18.</strong> Datamanagement</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/Datamanagement/DataTier.html"><strong aria-hidden="true">4.18.1.</strong> DataTier</a></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/index.html"><strong aria-hidden="true">4.19.</strong> ILM</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/ILM/1.AutomateRolloverWithILM.html"><strong aria-hidden="true">4.19.1.</strong> AutomateRolloverWithILM</a></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/FrozenIndices.html"><strong aria-hidden="true">4.19.2.</strong> FrozenIndices</a></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/ILM.html"><strong aria-hidden="true">4.19.3.</strong> ILM</a></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/RolloverAPI.html"><strong aria-hidden="true">4.19.4.</strong> RolloverAPI</a></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/ShrinkIndexAPI.html"><strong aria-hidden="true">4.19.5.</strong> ShrinkIndexAPI</a></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/ILMConcepts/index.html"><strong aria-hidden="true">4.19.6.</strong> ILMConcepts</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/ILM/ILMConcepts/1.Rollover.html"><strong aria-hidden="true">4.19.6.1.</strong> Rollover</a></li><li class="chapter-item expanded "><a href="elasticSearch/ILM/ILMConcepts/2.LifeCycleUpdates.html"><strong aria-hidden="true">4.19.6.2.</strong> LifeCycleUpdates</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/index.html"><strong aria-hidden="true">4.20.</strong> IndexModules</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/1.Analysis.html"><strong aria-hidden="true">4.20.1.</strong> Analysis</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/2.IndexShardAllocation.html"><strong aria-hidden="true">4.20.2.</strong> IndexShardAllocation</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/3.Indexblocks.html"><strong aria-hidden="true">4.20.3.</strong> Indexblocks</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/4.Mapper.html"><strong aria-hidden="true">4.20.4.</strong> Mapper</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/5.Merge.html"><strong aria-hidden="true">4.20.5.</strong> Merge</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/6.SimilarityModule.html"><strong aria-hidden="true">4.20.6.</strong> SimilarityModule</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/7.Slowlog.html"><strong aria-hidden="true">4.20.7.</strong> Slowlog</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/8.Store.html"><strong aria-hidden="true">4.20.8.</strong> Store</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/9.Translog.html"><strong aria-hidden="true">4.20.9.</strong> Translog</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/10.HistoryRetention.html"><strong aria-hidden="true">4.20.10.</strong> HistoryRetention</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/11.IndexSorting.html"><strong aria-hidden="true">4.20.11.</strong> IndexSorting</a></li><li class="chapter-item expanded "><a href="elasticSearch/IndexModules/12.IndexPressure.html"><strong aria-hidden="true">4.20.12.</strong> IndexPressure</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.21.</strong> IngestPipeline</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/IngestPipeline/1.IngestPipelines.html"><strong aria-hidden="true">4.21.1.</strong> IngestPipelines</a></li><li class="chapter-item expanded "><a href="elasticSearch/IngestPipeline/2.pipeLineExample.html"><strong aria-hidden="true">4.21.2.</strong> pipeLineExample</a></li><li class="chapter-item expanded "><a href="elasticSearch/IngestPipeline/3.EnrichYourData.html"><strong aria-hidden="true">4.21.3.</strong> EnrichYourData</a></li><li class="chapter-item expanded "><a href="elasticSearch/IngestPipeline/4.ExampleEnrichYourDataBasedOnGeolocation.html"><strong aria-hidden="true">4.21.4.</strong> ExampleEnrichYourDataBasedOnGeolocation</a></li><li class="chapter-item expanded "><a href="elasticSearch/IngestPipeline/5.IngestProcessorReference.html"><strong aria-hidden="true">4.21.5.</strong> IngestProcessorReference</a></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/index.html"><strong aria-hidden="true">4.22.</strong> queryDSL</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/1.QueryAndFilterContext.html"><strong aria-hidden="true">4.22.1.</strong> QueryAndFilterContext</a></li><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/2.CompoundQueries.html"><strong aria-hidden="true">4.22.2.</strong> CompoundQueries</a></li><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/SearchAfter.html"><strong aria-hidden="true">4.22.3.</strong> SearchAfter</a></li><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/FullTextQueries/index.html"><strong aria-hidden="true">4.22.4.</strong> FullTextQueries</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/FullTextQueries/1.IntervalsQuery.html"><strong aria-hidden="true">4.22.4.1.</strong> IntervalsQuery</a></li><li class="chapter-item expanded "><a href="elasticSearch/queryDSL/FullTextQueries/2.MatchQuery.html"><strong aria-hidden="true">4.22.4.2.</strong> MatchQuery</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/index.html"><strong aria-hidden="true">4.23.</strong> snapshotAndRestore</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/CleanUpSnapshotRepo.html"><strong aria-hidden="true">4.23.1.</strong> CleanUpSnapshotRepo</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/CloneSnapshot.html"><strong aria-hidden="true">4.23.2.</strong> CloneSnapshot</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/CreateOrUpdateSnapshotAPI.html"><strong aria-hidden="true">4.23.3.</strong> CreateOrUpdateSnapshotAPI</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/CreateSnapshot.html"><strong aria-hidden="true">4.23.4.</strong> CreateSnapshot</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/DeleteSnapShot.html"><strong aria-hidden="true">4.23.5.</strong> DeleteSnapShot</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/DeleteSnapshotRepository.html"><strong aria-hidden="true">4.23.6.</strong> DeleteSnapshotRepository</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/GetSnapshotAPI.html"><strong aria-hidden="true">4.23.7.</strong> GetSnapshotAPI</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/RegisterASnapshot.html"><strong aria-hidden="true">4.23.8.</strong> RegisterASnapshot</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/RestoreSnapshot.html"><strong aria-hidden="true">4.23.9.</strong> RestoreSnapshot</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/SLM.html"><strong aria-hidden="true">4.23.10.</strong> SLM</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/SearchableSnapshots.html"><strong aria-hidden="true">4.23.11.</strong> SearchableSnapshots</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/SearchableSnapshotsMountAPI.html"><strong aria-hidden="true">4.23.12.</strong> SearchableSnapshotsMountAPI</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/repository-s3.html"><strong aria-hidden="true">4.23.13.</strong> repository-s3</a></li><li class="chapter-item expanded "><a href="elasticSearch/snapshotAndRestore/使用OSS备份恢复集群实战.html"><strong aria-hidden="true">4.23.14.</strong> 使用OSS备份恢复集群实战</a></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/安装/index.html"><strong aria-hidden="true">4.24.</strong> 安装</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/安装/1.InstallingFromRpm.html"><strong aria-hidden="true">4.24.1.</strong> InstallingFromRpm</a></li><li class="chapter-item expanded "><a href="elasticSearch/安装/2.InstallFromDeb.html"><strong aria-hidden="true">4.24.2.</strong> InstallFromDeb</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.25.</strong> 深入搜索</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/深入搜索/全文搜索.html"><strong aria-hidden="true">4.25.1.</strong> 全文搜索</a></li><li class="chapter-item expanded "><a href="elasticSearch/深入搜索/多字段搜索.html"><strong aria-hidden="true">4.25.2.</strong> 多字段搜索</a></li><li class="chapter-item expanded "><a href="elasticSearch/深入搜索/结构化搜索.html"><strong aria-hidden="true">4.25.3.</strong> 结构化搜索</a></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/index.html"><strong aria-hidden="true">4.26.</strong> 索引分片分配</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/1.索引级别的分片分配控制.html"><strong aria-hidden="true">4.26.1.</strong> 索引级别的分片分配控制</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/2.节点离开延迟分配.html"><strong aria-hidden="true">4.26.2.</strong> 节点离开延迟分配</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/3.索引恢复优先级.html"><strong aria-hidden="true">4.26.3.</strong> 索引恢复优先级</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/4.每个节点的总分片.html"><strong aria-hidden="true">4.26.4.</strong> 每个节点的总分片</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/5.索引级别数据层分配过滤.html"><strong aria-hidden="true">4.26.5.</strong> 索引级别数据层分配过滤</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/6.手动分配分片clusterRerouteAPI.html"><strong aria-hidden="true">4.26.6.</strong> 手动分配分片clusterRerouteAPI</a></li><li class="chapter-item expanded "><a href="elasticSearch/索引分片分配/7.cluster-allocation-explain.html"><strong aria-hidden="true">4.26.7.</strong> cluster-allocation-explain</a></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/考纲解读/index.html"><strong aria-hidden="true">4.27.</strong> 考纲解读</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/考纲解读/1.ClusterManagement.html"><strong aria-hidden="true">4.27.1.</strong> ClusterManagement</a></li><li class="chapter-item expanded "><a href="elasticSearch/考纲解读/2.DataManagement.html"><strong aria-hidden="true">4.27.2.</strong> DataManagement</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.28.</strong> 聚合</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/聚合/RareTermsAggregation.html"><strong aria-hidden="true">4.28.1.</strong> RareTermsAggregation</a></li><li class="chapter-item expanded "><a href="elasticSearch/聚合/Term.html"><strong aria-hidden="true">4.28.2.</strong> Term</a></li></ol></li><li class="chapter-item expanded "><a href="elasticSearch/配置/index.html"><strong aria-hidden="true">4.29.</strong> 配置</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/配置/1.重要配置.html"><strong aria-hidden="true">4.29.1.</strong> 重要配置</a></li><li class="chapter-item expanded "><a href="elasticSearch/配置/2.elasticsearch.yml核心配置.html"><strong aria-hidden="true">4.29.2.</strong> elasticsearch.yml核心配置</a></li><li class="chapter-item expanded "><a href="elasticSearch/配置/3.es系统参数修改.html"><strong aria-hidden="true">4.29.3.</strong> es系统参数修改</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.30.</strong> 集群</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/集群/Health.html"><strong aria-hidden="true">4.30.1.</strong> Health</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.31.</strong> 高可用专题</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="elasticSearch/高可用专题/弹性设计.html"><strong aria-hidden="true">4.31.1.</strong> 弹性设计</a></li><li class="chapter-item expanded "><a href="elasticSearch/高可用专题/跨集群复制.html"><strong aria-hidden="true">4.31.2.</strong> 跨集群复制</a></li><li class="chapter-item expanded "><a href="elasticSearch/高可用专题/集群备份.html"><strong aria-hidden="true">4.31.3.</strong> 集群备份</a></li></ol></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="数据库"><a class="header" href="#数据库">数据库</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rownum"><a class="header" href="#rownum">rownum</a></h1>
<h2 id="rownum-的-产生过程"><a class="header" href="#rownum-的-产生过程">rownum 的 产生过程</a></h2>
<ol>
<li>rownum初始化为0</li>
<li>取一条记录 判断是否满足where条件</li>
<li>如果满足条件 则rownum+1</li>
<li>如果不满足条件 则rownum不变</li>
<li>继续第二步骤</li>
</ol>
<h2 id="关于where条件后-rownum-n-分析"><a class="header" href="#关于where条件后-rownum-n-分析">关于where条件后 rownum &gt;n 分析</a></h2>
<p>由于rownum 初始为 0,所以 当n为正整数时, 该条件一开始就不成立,rownum 也永远不会增加</p>
<h1 id="oracle排序函数"><a class="header" href="#oracle排序函数"><code>Oracle</code>排序函数</a></h1>
<h2 id="row_number"><a class="header" href="#row_number"><code>row_number</code></a></h2>
<p><code>row_number() over(partition by column1 order by column2) as pxh</code></p>
<h1 id="oracle分组函数"><a class="header" href="#oracle分组函数"><code>Oracle</code>分组函数</a></h1>
<h2 id="grouping-sets单排列"><a class="header" href="#grouping-sets单排列">grouping sets:单排列</a></h2>
<p><code>select a,b,sum(c) from table_d group by grouping sets(a,b)</code></p>
<p>等价于</p>
<p><code>select a,'',sum(c) from table_d  group by a</code></p>
<p><code>union all</code></p>
<p><code>select  '',b,sum(c) from table_d group by b</code></p>
<h2 id="rollup降级排列"><a class="header" href="#rollup降级排列">rollup:降级排列</a></h2>
<p><code>select a,b,sum(c) from table_d group by rollup(a,b)</code></p>
<p>等价于</p>
<p><code>select a,b,sum(c) from table_d group by a,b</code></p>
<p><code>union all</code></p>
<p><code>select a,'',sum(c) from table_d group by a</code></p>
<p><code>union all</code></p>
<p><code>select '','',sum(c) from table_d group by ''</code></p>
<h2 id="cube全排列"><a class="header" href="#cube全排列">cube:全排列</a></h2>
<p><code>select a,b,sum(c) from table_d group by cube(a,b)</code></p>
<p>等价于</p>
<p><code>select a,b,sum(c) from table_d group by a,b</code></p>
<p><code>union all</code></p>
<p><code>select a,'',sum(c) from table_d group by a</code></p>
<p><code>union all</code></p>
<p><code>select '',b,sum(c) from table_d group by b</code></p>
<p><code>union all</code></p>
<p><code>select '','',sum(c) group by ''</code></p>
<h1 id="oracle行转列"><a class="header" href="#oracle行转列">Oracle行转列</a></h1>
<h2 id="privot函数"><a class="header" href="#privot函数">privot函数</a></h2>
<pre><code class="language-sql">select * from
(select yljzhdh,substr(jyrq,5,2) jyrq, jyje from lsb)
privot
(
sum(jyje) su,
avg(jyje) av,
for(jyrq) in (
'01' as &quot;1月&quot;,
'02' as &quot;2月&quot;,
'03' as &quot;3月&quot;,
'04' as &quot;4月&quot;
)
)
</code></pre>
<h1 id="mergerinto"><a class="header" href="#mergerinto">mergerinto</a></h1>
<h2 id="语法"><a class="header" href="#语法">语法</a></h2>
<p><code>merger into table_name alias1</code></p>
<p><code>using (table |view | subquery) alias2</code></p>
<p><code>on (join condition)</code></p>
<p><code>when matched then update set</code></p>
<p><code>when not matched then insert</code></p>
<h2 id="注意事项"><a class="header" href="#注意事项">注意事项</a></h2>
<p>joinCondition必须一一对应,否则会报错</p>
<h1 id="oracle空值处理函数"><a class="header" href="#oracle空值处理函数">Oracle空值处理函数</a></h1>
<h2 id="nvlexpr1expr2"><a class="header" href="#nvlexpr1expr2"><code>nvl(expr1,expr2)</code></a></h2>
<p>如果expr1为null,返回expr2,否则返回expr1,两者类型要一致</p>
<h2 id="nvl2expr1expr2expr3"><a class="header" href="#nvl2expr1expr2expr3"><code>nvl2(expr1,expr2,expr3)</code></a></h2>
<p>expr1不为null,返回expr2,否则返回expr3,类型以expr2为准</p>
<h2 id="nullifexpr1expr2"><a class="header" href="#nullifexpr1expr2"><code>nullIf(expr1,expr2)</code></a></h2>
<p>expr1相等返回NULL,不相等返回expr1</p>
<h2 id="coalseceexpr1expr2expr3"><a class="header" href="#coalseceexpr1expr2expr3"><code>coalsece(expr1,expr2,expr3)</code></a></h2>
<p>从左到右选择不为null的一项</p>
<h2 id="case-when-else-end"><a class="header" href="#case-when-else-end"><code>case when else end</code></a></h2>
<p>......</p>
<h2 id="decodeexpr1valueresultvalueresult"><a class="header" href="#decodeexpr1valueresultvalueresult"><code>decode(expr1,value,result,value,result)</code></a></h2>
<p>根据expr1的计算值, 选择相应的result</p>
<h1 id="oracle创建序列"><a class="header" href="#oracle创建序列">Oracle创建序列</a></h1>
<p><code>create sequence seq_users seq_name</code></p>
<p><code>increment by 1</code></p>
<p><code>start with 1</code></p>
<p><code>minvalue 1</code></p>
<p><code>max value 9999</code></p>
<p><code>cycle / no cycle</code></p>
<p><code>cache 20</code></p>
<p><code>order / no order</code></p>
<h1 id="oracle权限管理"><a class="header" href="#oracle权限管理">Oracle权限管理</a></h1>
<h2 id="系统权限"><a class="header" href="#系统权限">系统权限</a></h2>
<table><thead><tr><th>权限名</th><th>权限操作</th></tr></thead><tbody>
<tr><td>索引权限</td><td>create any index<br />alter any index<br />drop any index</td></tr>
<tr><td></td><td></td></tr>
<tr><td>存储过程权限</td><td>create procedure<br />create any procedure<br />alter any procedure<br />execute any procedure<br />drop any procedure</td></tr>
<tr><td>角色权限</td><td>create role<br />alter any role<br />drop any role<br />grant any role</td></tr>
<tr><td>序列权限</td><td>create sequence<br />create any sequence<br />alter any sequence<br />select any sequence<br />drop any sequence</td></tr>
<tr><td>登录数据库权限</td><td>create session</td></tr>
<tr><td>表空间权限</td><td>create tablespace<br />alter tablespace<br />drop tablespace<br />mange tablepace<br />unlimited tablespace</td></tr>
<tr><td>类型权限</td><td>create type<br />create any type<br />alter any type<br />drop any type<br />execute any type<br />under any type</td></tr>
<tr><td>视图权限</td><td>create view<br />create any view<br />under any view<br />drop any view<br />flashback any table<br />merge any view</td></tr>
<tr><td>表权限</td><td>create table<br />create any table<br />alter any table<br />backup any table<br />delete any table<br />drop any table<br />delete any table<br />drop any table<br />insert any table<br />lock any table<br />select any table<br />flashback any table<br />update any table</td></tr>
<tr><td>触发器</td><td>create trigger<br />create any trigger<br />alter any trigger<br />drop any trigger<br />administer database trigger</td></tr>
<tr><td>备份数据库</td><td>exp_full_database<br />imp_full_database</td></tr>
</tbody></table>
<h2 id="权限赋予与撤销"><a class="header" href="#权限赋予与撤销">权限赋予与撤销</a></h2>
<h3 id="权限赋予"><a class="header" href="#权限赋予">权限赋予</a></h3>
<pre><code class="language-sql">grant select,delete,insert,update on username.table_name to username
grant execute on procedure_name to username
</code></pre>
<h3 id="限制修改的列"><a class="header" href="#限制修改的列">限制修改的列</a></h3>
<pre><code class="language-sql">grant update(column1,column2) on table_name to users
</code></pre>
<h3 id="收回权限"><a class="header" href="#收回权限">收回权限</a></h3>
<pre><code class="language-sql">revoke insert on table_name from username
</code></pre>
<h2 id="权限查询"><a class="header" href="#权限查询">权限查询</a></h2>
<h3 id="查询用户拥有的系统权限"><a class="header" href="#查询用户拥有的系统权限">查询用户拥有的系统权限</a></h3>
<pre><code class="language-sql">select grantee,privilege from dba_sys_privs where grantee = 'scott'
</code></pre>
<h3 id="查询用户拥有的对象权限"><a class="header" href="#查询用户拥有的对象权限">查询用户拥有的对象权限</a></h3>
<pre><code>select grantee,table_name,privilege from dba_tab_privs where grantee='SCOTT'

</code></pre>
<h3 id="查询用户所拥有的角色"><a class="header" href="#查询用户所拥有的角色">查询用户所拥有的角色</a></h3>
<pre><code>select grantee , granted_role from dba_role_privs where grantee = 'SCOTT'
</code></pre>
<h1 id="oracle优化器"><a class="header" href="#oracle优化器">Oracle优化器</a></h1>
<h2 id="优化器类型"><a class="header" href="#优化器类型">优化器类型</a></h2>
<h3 id="rbo"><a class="header" href="#rbo">RBO</a></h3>
<p>(rule based optimizer)</p>
<p>基于规则的优化</p>
<h3 id="cbo"><a class="header" href="#cbo">CBO</a></h3>
<p>(costed based optimizer) </p>
<p>基于成本的优化</p>
<p>通过目标sql语句所设计的表索引,列等的统计信息 选择一条执行成本最小的执行计划</p>
<h2 id="cardinaltity"><a class="header" href="#cardinaltity">cardinaltity</a></h2>
<p>结果集的势</p>
<p>指结果集所包含的列数</p>
<h2 id="selectivty"><a class="header" href="#selectivty">selectivty</a></h2>
<p>where条件筛选出来的记录数占总记录数的比率,越小越好</p>
<h2 id="可传递性"><a class="header" href="#可传递性">可传递性</a></h2>
<h3 id="简单谓语传递"><a class="header" href="#简单谓语传递">简单谓语传递</a></h3>
<p><code>t1.c1 = t2.c2 and t1.c1 = 10</code>  添加 <code>t2.c2  = 10</code></p>
<h3 id="连接谓语传递"><a class="header" href="#连接谓语传递">连接谓语传递</a></h3>
<p><code>t1.c1 = t2.c1   and  t2.c1 = t3.c1</code>  添加 <code>t3.c1 = t1.c1</code></p>
<h3 id="外连接谓语传递"><a class="header" href="#外连接谓语传递">外连接谓语传递</a></h3>
<p><code>t1.c1 = t2.c1(+) and t1.c1=10</code>  添加 <code>t2.c1(+) = 10</code></p>
<h2 id="优化器模式"><a class="header" href="#优化器模式">优化器模式</a></h2>
<h3 id="rule"><a class="header" href="#rule">RULE</a></h3>
<p>基于规则的优化RBO</p>
<h3 id="choose"><a class="header" href="#choose">CHOOSE</a></h3>
<p>如果目标含有统计信息则 使用 CBO </p>
<h3 id="first_row"><a class="header" href="#first_row">FIRST_ROW</a></h3>
<p>快速的返回记录,侧重响应时间</p>
<h3 id="all_rows"><a class="header" href="#all_rows">ALL_ROWS</a></h3>
<p>侧重最佳吞吐量,默认模式</p>
<h2 id="数据访问方式"><a class="header" href="#数据访问方式">数据访问方式</a></h2>
<h3 id="全表扫描"><a class="header" href="#全表扫描">全表扫描</a></h3>
<p>从表所占用的第一个extent 第一个块开始,一直到该表的最高水位线</p>
<h3 id="rowid扫描"><a class="header" href="#rowid扫描">ROWID扫描</a></h3>
<p>直接通过数据所在的rowID 定位到物理存储空间</p>
<h3 id="索引扫描"><a class="header" href="#索引扫描">索引扫描</a></h3>
<h4 id="索引唯一性扫描"><a class="header" href="#索引唯一性扫描">索引唯一性扫描</a></h4>
<p>index unique scan : 适用于等值查询的目标</p>
<h4 id="索引范围扫描"><a class="header" href="#索引范围扫描">索引范围扫描</a></h4>
<p>index range scan</p>
<p>​	当扫描的对象是唯一性索引时,此时where条件为范围条件</p>
<p>​    当扫描的对象是非唯一性索引,where条件不做限制</p>
<h4 id="索引全扫描"><a class="header" href="#索引全扫描">索引全扫描</a></h4>
<p>index full scan</p>
<p>​	扫描索引的所有叶子块的所有行</p>
<h4 id="索引快速扫描"><a class="header" href="#索引快速扫描">索引快速扫描</a></h4>
<p>index fast full scan</p>
<h4 id="索引跳跃扫描"><a class="header" href="#索引跳跃扫描">索引跳跃扫描</a></h4>
<p>index skip scan</p>
<h2 id="表连接的方式"><a class="header" href="#表连接的方式">表连接的方式</a></h2>
<h3 id="排序合并"><a class="header" href="#排序合并">排序合并</a></h3>
<p><code>sort meger into</code> </p>
<ol>
<li>以目标sql中的谓语条件去访问表t1,对返回结果按连接列进行排序,得到结果集 s1</li>
<li>以目标sql中的谓语条件访问 t2,对返回结果按连接列进行排序,得到结果集 s2</li>
<li>对结果s1,s2 进行合并</li>
</ol>
<h3 id="嵌套循环"><a class="header" href="#嵌套循环">嵌套循环</a></h3>
<p><code>nested loops join</code></p>
<ol>
<li>根据规则决定 t1,t2 谁是驱动表,谁是被动表,驱动表做外层循环,被驱动表做内层</li>
<li>假设t1 是 驱动表,t2是被驱动表</li>
<li>用目标sql的谓语条件 访问表 t1 得到结果集 s1</li>
<li>循环取出驱动表中的每条记录 与被驱动表关联</li>
</ol>
<h3 id="哈希连接"><a class="header" href="#哈希连接">哈希连接</a></h3>
<p><code>hash join</code></p>
<ol>
<li>依据 <code>hash_area_size</code>, <code>db_block_size</code>,<code>_hash_multiblock_io_count</code>决定<code>hash partition</code>数量</li>
<li>使用谓语条件 访问t1,t2 得到结果集 s1,s2 ,假设s1数量少与s2</li>
<li>取s1为驱动结果集, 遍历s1的每条记录, 取两个hash函数 哈希运算计算出来的结果放在不同的partition  的不同的bucket </li>
<li>同时构造s1,s2 的哈希table</li>
</ol>
<h1 id="oracle-to_char"><a class="header" href="#oracle-to_char">Oracle to_char</a></h1>
<h2 id="用于时间转换的格式字符"><a class="header" href="#用于时间转换的格式字符">用于时间转换的格式字符</a></h2>
<h2 id="用于数值格式化"><a class="header" href="#用于数值格式化">用于数值格式化</a></h2>
<p>9 带有指定位数的值</p>
<p>0 前导0的值</p>
<p>. 小数点</p>
<p>, 分组(千) 分隔符</p>
<h1 id="oracle表空间管理"><a class="header" href="#oracle表空间管理">Oracle表空间管理</a></h1>
<h2 id="简介"><a class="header" href="#简介">简介</a></h2>
<ol>
<li>Oracle的逻辑结构包括表空间 段 区 块</li>
<li>表空间是Oracle的逻辑组成部分, 一个表空间可以拥有多个数据文件</li>
<li>在逻辑上,表空间由 段segment 组成</li>
<li>段 segment 是由区构成</li>
<li>块是由 Oracle数据块构成</li>
</ol>
<h2 id="建立表空间"><a class="header" href="#建立表空间">建立表空间</a></h2>
<ol>
<li>
<p>拥有 <code>create tablespace</code> 权限</p>
</li>
<li>
<p>语法</p>
<p><code>create tablesapce tablespacename datafile'filePath' size 20M uniform size 128K</code></p>
</li>
</ol>
<h2 id="修改表空间"><a class="header" href="#修改表空间">修改表空间</a></h2>
<p><code>alter tablesapce tablename offline|online|read only| read write</code></p>
<h2 id="查询表空间所有信息"><a class="header" href="#查询表空间所有信息">查询表空间所有信息</a></h2>
<p><code>select * from all_tables where tablesapce_name = ''</code></p>
<h2 id="删除表空间"><a class="header" href="#删除表空间">删除表空间</a></h2>
<p><code>drop tablespace 'tablespacename' including contents and datafiles</code></p>
<h2 id="扩展表空间"><a class="header" href="#扩展表空间">扩展表空间</a></h2>
<h3 id="增加数据文件"><a class="header" href="#增加数据文件">增加数据文件</a></h3>
<p><code>alter tablespace tablesapcename add datafile 'filepath' size 20M</code></p>
<h2 id="增加数据文件大小"><a class="header" href="#增加数据文件大小">增加数据文件大小</a></h2>
<p><code>alter tablespace tablespacename  'filepath' resize 20M</code></p>
<h2 id="设置文件自动增长"><a class="header" href="#设置文件自动增长">设置文件自动增长</a></h2>
<p><code>alter tablesapce tablespacename 'filepath' autoextend on next 10m maxsize 500M</code></p>
<h1 id="oracle分区管理"><a class="header" href="#oracle分区管理">Oracle分区管理</a></h1>
<h2 id="概述"><a class="header" href="#概述">概述</a></h2>
<p>Oracle分区是一种处理超大型表,索引等的技术</p>
<h2 id="优缺点"><a class="header" href="#优缺点">优缺点</a></h2>
<h3 id="优点"><a class="header" href="#优点">优点</a></h3>
<p>增强可用性</p>
<p>可维护性</p>
<p>均衡IO</p>
<p>改善查询性能</p>
<h3 id="缺点"><a class="header" href="#缺点">缺点</a></h3>
<p>已经存在的表无法直接转换分区表</p>
<h2 id="分区方法"><a class="header" href="#分区方法">分区方法</a></h2>
<p>范围分区</p>
<p>Hash分区</p>
<p>列表分区</p>
<p>范围-散列分区</p>
<p>范围-列表分区</p>
<h1 id="oracle优化"><a class="header" href="#oracle优化">Oracle优化</a></h1>
<h2 id="索引优化"><a class="header" href="#索引优化">索引优化</a></h2>
<ol>
<li>
<p>Oracle不会索引空值 所以判断为空不为空 无法使用索引</p>
<p>可以使用 &lt; &gt; 替代,或者空值用特定值替代</p>
</li>
<li>
<p>不等于 &lt;&gt; 不会应用索引 可以改为 &gt; &lt; </p>
</li>
<li>
<p><code>&gt;= 3 与 &gt;2</code> 的比较</p>
<ol>
<li><code>&gt;</code>2  先找出 2的索引 然后作比较</li>
<li><code>&gt;=3</code>  直接找出3 然后作比较</li>
</ol>
</li>
<li>
<p>like操作符</p>
<p>aa like '%AAA%' 不会应用索引</p>
<p>可以改为 aa like 'BAAA%' or aa like 'CAAA%' </p>
</li>
<li>
<p>in not in 会自动转换成外连接 </p>
<p>尽量使用外连接</p>
</li>
<li>
<p>union ,union all , union 会排序去重</p>
</li>
<li>
<p>sql 语句书写, 是否大小写, 是否写用户名前缀,等等差异 会导致 识别为不同sql</p>
</li>
<li>
<p>where 条件执行顺序 是从 右到 左 , 尽量 过滤大头的 在 右边</p>
</li>
<li>
<p>表连接时, 如果有 统计信息分析, 则会自动 先小表, 后大表</p>
<p><code>generate statistics on &lt;table&gt;</code></p>
</li>
<li>
<p>采用函数处理的字段无法使用索引</p>
<ol>
<li>substr(bh,1,4) = '5400' 改成 bh like '5400'</li>
<li>trunc(rq) = trunc(sysdate)  rq &gt; = trunc(sysdate) and rq &lt; trunc(sysdate+1) </li>
</ol>
</li>
<li>
<p>进行了显示或者 隐式运算的字段不能 索引</p>
<ol>
<li>df + 20 &gt;50  不能用索引</li>
<li>rq + 5  = sysdate 不能使用索引</li>
<li>bh = 132456789 ; 不能使用索引,  bh = '132456789'</li>
</ol>
</li>
<li>
<p>条件包含了多个本表的字段运算时不能索引</p>
<p>a &gt; b 无法优化</p>
<p>a||b = '123456' 优化 成 a='123' and b = '456'</p>
</li>
</ol>
<h1 id="oracle日期函数"><a class="header" href="#oracle日期函数">Oracle日期函数</a></h1>
<h2 id="函数"><a class="header" href="#函数">函数</a></h2>
<p>to_number(char)</p>
<p>to_date(char,pattern)</p>
<p>to_char(date,pattern)</p>
<p>date'char'</p>
<p>trunct(date,pattern)</p>
<p>extract(day|month|year| from date)返回数字格式</p>
<p>date + int 给日期加天数</p>
<p>last_day(date) 最后一天</p>
<p>round(date,'pattern') 四舍五入</p>
<p>months_betwteen(date,date)  两个日期之间的月份</p>
<p>next_day(sysdate,'星期三') 下个星期三的日期</p>
<h2 id="pattern"><a class="header" href="#pattern">pattern</a></h2>
<p>y 年的最后一位</p>
<p>yy 最后两位</p>
<p>yyy 后三位</p>
<p>yyyy 四位</p>
<p>mm 两位月份</p>
<p>mon 英文简写形式 11月或者nov</p>
<p>month 全称</p>
<p>dd 当月的第几天</p>
<p>ddd 当年的第几天</p>
<p>dy 当周第几天 简写星期5,fri</p>
<p>day 全称 星期五</p>
<p>hh 小时12进制</p>
<p>hh24 24小时制</p>
<p>mi 分钟</p>
<p>ss 秒</p>
<p>q 季度</p>
<p>ｗw 当年第几周</p>
<p>w 当月第几周</p>
<h2 id="日期运算"><a class="header" href="#日期运算">日期运算</a></h2>
<p>sysdate - interval'7' year|month|minute|second 减去七年,月,分钟,秒</p>
<h1 id="oracle数据库导入导出"><a class="header" href="#oracle数据库导入导出">Oracle数据库导入导出</a></h1>
<h2 id="sqlldr加载平面文件"><a class="header" href="#sqlldr加载平面文件">sqlldr加载平面文件</a></h2>
<h2 id="数据泵二进制数据导入导出"><a class="header" href="#数据泵二进制数据导入导出">数据泵:二进制数据导入导出</a></h2>
<h2 id="外部表以上两种方案的简化工具"><a class="header" href="#外部表以上两种方案的简化工具">外部表:以上两种方案的简化工具</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="资源"><a class="header" href="#资源">资源</a></h1>
<p><a href="https://hub.docker.com/r/oracleinanutshell/oracle-xe-11g">docker镜像</a></p>
<p><a href="https://www.oracle.com/database/technologies/instant-client/downloads.html">客户端下载</a></p>
<h1 id="安装oracle服务器"><a class="header" href="#安装oracle服务器">安装Oracle服务器</a></h1>
<pre><code class="language-sh">docker pull oracleinanutshell/oracle-xe-11g
docker run -d -p 49161:1521 -e ORACLE_ALLOW_REMOTE=true oracleinanutshell/oracle-xe-11g

</code></pre>
<p><strong>默认配置</strong></p>
<pre><code>hostname: localhost
port: 49161
sid: xe
username: system
password: oracle
</code></pre>
<h1 id="客户端"><a class="header" href="#客户端">客户端</a></h1>
<ul>
<li>新建 <em>ORACLE_HOME</em> </li>
<li>新建 <em>TNS_ADMIN</em>   <em>NETWORK/ADMIN</em> 环境变量</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="更新视图"><a class="header" href="#更新视图">更新视图</a></h1>
<pre><code class="language-sql">update
(select  /*+BYPASS_UJVC*/  a.money,b.money1,b.money2 from salary a , person_money b where a.name=b.name )
a set  a.money1=a.money, a.money2=a.money+1;
</code></pre>
<p><strong>注意</strong>： 关联的 两个表必须是  唯一的</p>
<h1 id="merge关键字"><a class="header" href="#merge关键字">merge关键字</a></h1>
<pre><code class="language-sql">MERGE INTO person_money T1
USING salary T2
ON (T1.name = T2.name)
WHEN MATCHED THEN
UPDATE
SET T1.money1 = T2.money ,t1.money2 = t2.money+1
WHEN NOT MATCHED THEN NOTHING
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nosql-简介"><a class="header" href="#nosql-简介">NoSQL 简介</a></h1>
<p>NoSQL(NoSQL = Not Only SQL )，意即&quot;不仅仅是SQL&quot;。</p>
<p>在现代的计算系统上每天网络上都会产生庞大的数据量。</p>
<p>这些数据有很大一部分是由关系数据库管理系统（RDBMS）来处理。 1970年 E.F.Codd's提出的关系模型的论文 &quot;A relational model of data for large shared data banks&quot;，这使得数据建模和应用程序编程更加简单。</p>
<p>通过应用实践证明，关系模型是非常适合于客户服务器编程，远远超出预期的利益，今天它是结构化数据存储在网络和商务应用的主导技术。</p>
<h2 id="关系型数据库遵循acid规则"><a class="header" href="#关系型数据库遵循acid规则">关系型数据库遵循ACID规则</a></h2>
<p>事务在英文中是transaction，和现实世界中的交易很类似，它有如下四个特性：</p>
<p><strong>1、A (Atomicity) 原子性</strong></p>
<p><strong>2、C (Consistency) 一致性</strong></p>
<p><strong>3、I (Isolation) 独立性</strong></p>
<p><strong>4、D (Durability) 持久性</strong></p>
<h2 id="什么是nosql"><a class="header" href="#什么是nosql">什么是NoSQL?</a></h2>
<p>NoSQL，指的是非关系型的数据库。NoSQL有时也称作Not Only SQL的缩写</p>
<p>NoSQL用于超大规模数据的存储，这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。</p>
<h2 id="nosql特性"><a class="header" href="#nosql特性"><strong>NoSQL</strong>特性</a></h2>
<p>- 代表着不仅仅是SQL
- 没有声明性查询语言
- 没有预定义的模式
-键 - 值对存储，列存储，文档存储，图形数据库
- 最终一致性，而非ACID属性
- 非结构化和不可预知的数据
- CAP定理
- 高性能，高可用性和可伸缩性</p>
<h2 id="cap定理cap-theorem"><a class="header" href="#cap定理cap-theorem">CAP定理（CAP theorem）</a></h2>
<p>在计算机科学中, CAP定理（CAP theorem）, 又被称作 布鲁尔定理（Brewer's theorem）, 它指出对于一个分布式计算系统来说，不可能同时满足以下三点:</p>
<ul>
<li><strong>一致性(Consistency)</strong> (所有节点在同一时间具有相同的数据) <code>一致</code></li>
<li><strong>可用性(Availability)</strong> (保证每个请求不管成功或者失败都有响应) <code>性能好</code></li>
<li><strong>分区容错(Partition tolerance)</strong> (系统中任意信息的丢失或失败不会影响系统的继续运作) <code>可扩展</code></li>
</ul>
<p>CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。</p>
<p>满足 CP 原则和满足 AP 原则三 大类：</p>
<ul>
<li>CA - 单点集群，满足一致性，可用性的系统，通常在可扩展性上不太强大。</li>
<li>CP - 满足一致性，分区容忍性的系统，通常性能不是特别高。</li>
<li>AP - 满足可用性，分区容忍性的系统，通常可能对一致性要求低一些。</li>
</ul>
<p><img src="MongoDB//images/cap-theoram-image.png" alt="" /></p>
<h2 id="nosql的优点缺点"><a class="header" href="#nosql的优点缺点">NoSQL的优点/缺点</a></h2>
<p>优点:</p>
<ul>
<li>- 高可扩展性</li>
<li>- 分布式计算</li>
<li>- 低成本</li>
<li>- 架构的灵活性，半结构化数据</li>
<li>- 没有复杂的关系</li>
</ul>
<p>缺点:</p>
<ul>
<li>- 没有标准化</li>
<li>- 有限的查询功能（到目前为止）</li>
<li>- 最终一致是不直观的程序</li>
</ul>
<h2 id="base"><a class="header" href="#base">BASE</a></h2>
<p>BASE：Basically Available, Soft-state, Eventually Consistent。 由 Eric Brewer 定义。</p>
<p>CAP理论的核心是：一个分布式系统不可能同时很好的满足一致性，可用性和分区容错性这三个需求，最多只能同时较好的满足两个。</p>
<p>BASE是NoSQL数据库通常对可用性及一致性的弱要求原则:</p>
<ul>
<li>Basically Available --基本可用</li>
<li>Soft-state --软状态/柔性事务。 &quot;Soft state&quot; 可以理解为&quot;无连接&quot;的, 而 &quot;Hard state&quot; 是&quot;面向连接&quot;的</li>
<li>Eventually Consistency -- 最终一致性， 也是 ACID 的最终目的。</li>
</ul>
<h2 id="acid-vs-base"><a class="header" href="#acid-vs-base">ACID vs BASE</a></h2>
<table><thead><tr><th style="text-align: left">ACID</th><th style="text-align: left">BASE</th></tr></thead><tbody>
<tr><td style="text-align: left">原子性(<strong>A</strong>tomicity)</td><td style="text-align: left">基本可用(<strong>B</strong>asically <strong>A</strong>vailable)</td></tr>
<tr><td style="text-align: left">一致性(<strong>C</strong>onsistency)</td><td style="text-align: left">软状态/柔性事务(<strong>S</strong>oft state)</td></tr>
<tr><td style="text-align: left">隔离性(<strong>I</strong>solation)</td><td style="text-align: left">最终一致性 (<strong>E</strong>ventual consistency)</td></tr>
<tr><td style="text-align: left">持久性 (<strong>D</strong>urable)</td><td style="text-align: left"></td></tr>
</tbody></table>
<h2 id="nosql-数据库分类"><a class="header" href="#nosql-数据库分类">NoSQL 数据库分类</a></h2>
<table><thead><tr><th>类型</th><th>部分代表</th><th>特点</th></tr></thead><tbody>
<tr><td>列存储</td><td>HbaseCassandraHypertable</td><td>顾名思义，是按列存储数据的。最大的特点是方便存储结构化和半结构化数据，方便做数据压缩，对针对某一列或者某几列的查询有非常大的IO优势。</td></tr>
<tr><td>文档存储</td><td>MongoDBCouchDB</td><td>文档存储一般用类似json的格式存储，存储的内容是文档型的。这样也就有机会对某些字段建立索引，实现关系数据库的某些功能。</td></tr>
<tr><td>key-value存储</td><td>Tokyo Cabinet / TyrantBerkeley DBMemcacheDBRedis</td><td>可以通过key快速查询到其value。一般来说，存储不管value的格式，照单全收。（Redis包含了其他功能）</td></tr>
<tr><td>图存储</td><td>Neo4JFlockDB</td><td>图形关系的最佳存储。使用传统关系数据库来解决的话性能低下，而且设计使用不方便。</td></tr>
<tr><td>对象存储</td><td>db4oVersant</td><td>通过类似面向对象语言的语法操作数据库，通过对象的方式存取数据。</td></tr>
<tr><td>xml数据库</td><td>Berkeley DB XMLBaseX</td><td>高效的存储XML数据，并支持XML的内部查询语法，</td></tr>
</tbody></table>
<h1 id="mongodb简介"><a class="header" href="#mongodb简介">Mongodb简介</a></h1>
<ul>
<li>
<p>MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。</p>
</li>
<li>
<p>在高负载的情况下，添加更多的节点，可以保证服务器性能。</p>
</li>
<li>
<p>MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。</p>
</li>
<li>
<p>MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。</p>
</li>
</ul>
<h2 id="主要特点"><a class="header" href="#主要特点">主要特点</a></h2>
<ul>
<li>MongoDB 是一个面向文档存储的数据库，操作起来比较简单和容易。</li>
<li>你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=&quot;Sameer&quot;,Address=&quot;8 Gandhi Road&quot;)来实现更快的排序。</li>
<li>你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。</li>
<li>如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。</li>
<li>Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。</li>
<li>MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。</li>
<li>Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。</li>
<li>Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。</li>
<li>Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。</li>
<li>GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。</li>
<li>MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。</li>
</ul>
<h2 id="mongodb-下载"><a class="header" href="#mongodb-下载">MongoDB 下载</a></h2>
<p>你可以在mongodb官网下载该安装包，地址为：https://www.mongodb.com/download-center#community。MonggoDB支持以下平台:</p>
<ul>
<li>OS X 32-bit</li>
<li>OS X 64-bit</li>
<li>Linux 32-bit</li>
<li>Linux 64-bit</li>
<li>Windows 32-bit</li>
<li>Windows 64-bit</li>
<li>Solaris i86pc</li>
<li>Solaris 64</li>
</ul>
<h2 id="mongodb-工具"><a class="header" href="#mongodb-工具">MongoDB 工具</a></h2>
<p>有几种可用于MongoDB的管理工具。</p>
<h3 id="监控"><a class="header" href="#监控">监控</a></h3>
<p>MongoDB提供了网络和系统监控工具Munin，它作为一个插件应用于MongoDB中。</p>
<p>Gangila是MongoDB高性能的系统监视的工具，它作为一个插件应用于MongoDB中。</p>
<p>基于图形界面的开源工具 Cacti, 用于查看CPU负载, 网络带宽利用率,它也提供了一个应用于监控 MongoDB 的插件。</p>
<h3 id="gui"><a class="header" href="#gui">GUI</a></h3>
<ul>
<li>Fang of Mongo – 网页式,由Django和jQuery所构成。</li>
<li>Futon4Mongo – 一个CouchDB Futon web的mongodb山寨版。</li>
<li>Mongo3 – Ruby写成。</li>
<li>MongoHub – 适用于OSX的应用程序。</li>
<li>Opricot – 一个基于浏览器的MongoDB控制台, 由PHP撰写而成。</li>
<li>Database Master — Windows的mongodb管理工具</li>
<li>RockMongo — 最好的PHP语言的MongoDB管理工具，轻量级, 支持多国语言.</li>
</ul>
<h2 id="mongodb-概念解析"><a class="header" href="#mongodb-概念解析">MongoDB 概念解析</a></h2>
<table><thead><tr><th>SQL术语/概念</th><th>MongoDB术语/概念</th><th>解释/说明</th></tr></thead><tbody>
<tr><td>database</td><td>database</td><td>数据库</td></tr>
<tr><td>table</td><td>collection</td><td>数据库表/集合</td></tr>
<tr><td>row</td><td>document</td><td>数据记录行/文档</td></tr>
<tr><td>column</td><td>field</td><td>数据字段/域</td></tr>
<tr><td>index</td><td>index</td><td>索引</td></tr>
<tr><td>table joins</td><td></td><td>表连接,MongoDB不支持</td></tr>
<tr><td>primary key</td><td>primary key</td><td>主键,MongoDB自动将_id字段设置为主键</td></tr>
</tbody></table>
<h3 id="数据库-1"><a class="header" href="#数据库-1">数据库</a></h3>
<p>一个mongodb中可以建立多个数据库。</p>
<p>MongoDB的默认数据库为&quot;db&quot;，该数据库存储在data目录中。</p>
<p>MongoDB的单个实例可以容纳多个独立的数据库，每一个都有自己的集合和权限，不同的数据库也放置在不同的文件中。</p>
<table><thead><tr><th>命令</th><th>描述</th></tr></thead><tbody>
<tr><td>show dbs</td><td>显示所有数据的列表</td></tr>
<tr><td>db</td><td>显示当前数据库对象或集合</td></tr>
<tr><td>use db</td><td>连接到一个指定的数据库</td></tr>
</tbody></table>
<p><strong>数据库也通过名字来标识</strong>。数据库名可以是满足以下条件的任意UTF-8字符串。</p>
<ul>
<li>不能是空字符串（&quot;&quot;)。</li>
<li>不得含有' '（空格)、.、$、/、\和\0 (空宇符)。</li>
<li>应全部小写。</li>
<li>最多64字节。</li>
</ul>
<p><strong>有一些数据库名是保留的</strong>，可以直接访问这些有特殊作用的数据库。</p>
<p><strong>admin</strong>： 从权限的角度来看，这是&quot;root&quot;数据库。要是将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限。一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。</p>
<p>**local:**这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合</p>
<p><strong>config</strong>: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。</p>
<h3 id="文档"><a class="header" href="#文档">文档</a></h3>
<p>文档是一个键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别</p>
<pre><code class="language-json">{&quot;site&quot;:&quot;www.php.cn&quot;, &quot;name&quot;:&quot;php中文网&quot;}
</code></pre>
<table><thead><tr><th style="text-align: center">RDBMS</th><th>MongoDB</th></tr></thead><tbody>
<tr><td style="text-align: center">数据库</td><td>数据库</td></tr>
<tr><td style="text-align: center">表格</td><td>集合</td></tr>
<tr><td style="text-align: center">行</td><td>文档</td></tr>
<tr><td style="text-align: center">列</td><td>字段</td></tr>
<tr><td style="text-align: center">表联合</td><td>嵌入文档</td></tr>
<tr><td style="text-align: center">主键</td><td>主键 (MongoDB 提供了 key  为 _id )</td></tr>
<tr><td style="text-align: center">数据库服务和客户端</td><td></td></tr>
<tr><td style="text-align: center">Mysqld/Oracle</td><td>mongod</td></tr>
<tr><td style="text-align: center">mysql/sqlplus</td><td>mongo</td></tr>
</tbody></table>
<h3 id="集合"><a class="header" href="#集合">集合</a></h3>
<p>集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。</p>
<p>集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。</p>
<p>比如，我们可以将以下不同数据结构的文档插入到集合中：</p>
<pre><code class="language-json">{&quot;site&quot;:&quot;www.baidu.com&quot;}
{&quot;site&quot;:&quot;www.google.com&quot;,&quot;name&quot;:&quot;Google&quot;}
{&quot;site&quot;:&quot;www.php.cn&quot;,&quot;name&quot;:&quot;php中文网&quot;,&quot;num&quot;:5}
</code></pre>
<p>当第一个文档插入时，集合就会被创建。</p>
<h3 id="合法的集合名"><a class="header" href="#合法的集合名">合法的集合名</a></h3>
<ul>
<li>集合名不能是空字符串&quot;&quot;。</li>
<li>集合名不能含有\0字符（空字符)，这个字符表示集合名的结尾。</li>
<li>集合名不能以&quot;system.&quot;开头，这是为系统集合保留的前缀。</li>
<li>用户创建的集合名字不能含有保留字符。有些驱动程序的确支持在集合名里面包含，这是因为某些系统生成的集合中包含该字符。除非你要访问这种系统创建的集合，否则千万不要在名字里出现$。　</li>
</ul>
<h3 id="capped-collections"><a class="header" href="#capped-collections">capped collections</a></h3>
<p><strong>Capped collections 就是固定大小的collection。</strong></p>
<p>它有很高的性能以及队列过期的特性(过期按照插入的顺序). 有点和 &quot;RRD&quot; 概念类似。</p>
<p>Capped collections是高性能自动的维<strong>护对象的插入顺序</strong>。它非常适合类似记录日志的功能 和标准的collection不同，你必须要显式的创建一个capped collection， 指定一个collection的大小，单位是字节。collection的数据存储空间值提前分配的。</p>
<pre><code class="language-js">db.createCollection(&quot;mycoll&quot;, {capped:true, size:100000})
</code></pre>
<ul>
<li>在capped collection中，你能添加新的对象。</li>
<li>能进行更新，然而，对象不会增加存储空间。如果增加，更新就会失败 。</li>
<li>数据库不允许进行删除。使用drop()方法删除collection所有的行。</li>
<li>注意: 删除之后，你必须显式的重新创建这个collection。</li>
<li>在32bit机器中，capped collection最大存储为1e9( 10^9)个字节。</li>
</ul>
<h3 id="元数据"><a class="header" href="#元数据">元数据</a></h3>
<p>数据库的信息是存储在集合中。它们使用了系统的命名空间：</p>
<pre><code>dbname.system.*
</code></pre>
<p>在MongoDB数据库中名字空间 <dbname>.system.* 是包含多种系统信息的特殊集合(Collection)，如下:</p>
<table><thead><tr><th>集合命名空间</th><th>描述</th></tr></thead><tbody>
<tr><td>dbname.system.namespaces</td><td>列出所有名字空间。</td></tr>
<tr><td>dbname.system.indexes</td><td>列出所有索引。</td></tr>
<tr><td>dbname.system.profile</td><td>包含数据库概要(profile)信息。</td></tr>
<tr><td>dbname.system.users</td><td>列出所有可访问数据库的用户。</td></tr>
<tr><td>dbname.local.sources</td><td>包含复制对端（slave）的服务器信息和状态。</td></tr>
</tbody></table>
<p>对于修改系统集合中的对象有如下限制。</p>
<ul>
<li>
<p>在{{system.indexes}}插入数据，可以创建索引。但除此之外该表信息是不可变的(特殊的drop index命令将自动更新相关信息)。</p>
</li>
<li>
<p>{{system.users}}是可修改的。 {{system.profile}}是可删除的。</p>
</li>
</ul>
<h3 id="mongodb-数据类型"><a class="header" href="#mongodb-数据类型">MongoDB 数据类型</a></h3>
<p>下表为MongoDB中常用的几种数据类型。</p>
<table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody>
<tr><td>String</td><td>字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。</td></tr>
<tr><td>Integer</td><td>整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。</td></tr>
<tr><td>Boolean</td><td>布尔值。用于存储布尔值（真/假）。</td></tr>
<tr><td>Double</td><td>双精度浮点值。用于存储浮点值。</td></tr>
<tr><td>Min/Max keys</td><td>将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。</td></tr>
<tr><td>Arrays</td><td>用于将数组或列表或多个值存储为一个键。</td></tr>
<tr><td>Timestamp</td><td>时间戳。记录文档修改或添加的具体时间。</td></tr>
<tr><td>Object</td><td>用于内嵌文档。</td></tr>
<tr><td>Null</td><td>用于创建空值。</td></tr>
<tr><td>Symbol</td><td>符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。</td></tr>
<tr><td>Date</td><td>日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。</td></tr>
<tr><td>Object ID</td><td>对象 ID。用于创建文档的 ID。</td></tr>
<tr><td>Binary Data</td><td>二进制数据。用于存储二进制数据。</td></tr>
<tr><td>Code</td><td>代码类型。用于在文档中存储 JavaScript 代码。</td></tr>
<tr><td>Regular expression</td><td>正则表达式类型。用于存储正则表达式。</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mongodb-连接"><a class="header" href="#mongodb-连接">MongoDB 连接</a></h1>
<p><strong>格式</strong></p>
<pre><code class="language-js">//username:password@hostname/dbname
./mongo mongodb://admin:123456@localhost/test
</code></pre>
<h2 id="连接实例"><a class="header" href="#连接实例">连接实例</a></h2>
<p><strong>连接本地数据库服务器，端口是默认的。</strong></p>
<pre><code>mongodb://localhost
</code></pre>
<p><strong>使用用户名fred，密码foobar登录localhost的admin数据库。</strong></p>
<pre><code>mongodb://fred:foobar@localhost
</code></pre>
<p><strong>使用用户名fred，密码foobar登录localhost的baz数据库。</strong></p>
<pre><code>mongodb://fred:foobar@localhost/baz
</code></pre>
<p><strong>连接 replica pair, 服务器1为example1.com服务器2为example2。</strong></p>
<pre><code>mongodb://example1.com:27017,example2.com:27017
</code></pre>
<p><strong>连接 replica set 三台服务器 (端口 27017, 27018, 和27019):</strong></p>
<pre><code>mongodb://localhost,localhost:27018,localhost:27019
</code></pre>
<p><strong>连接 replica set 三台服务器, 写入操作应用在主服务器 并且分布查询到从服务器。</strong></p>
<pre><code>mongodb://host1,host2,host3/?slaveOk=true
</code></pre>
<p><strong>直接连接第一个服务器，无论是replica set一部分或者主服务器或者从服务器。</strong></p>
<pre><code>mongodb://host1,host2,host3/?connect=direct;slaveOk=true
</code></pre>
<p>当你的连接服务器有优先级，还需要列出所有服务器，你可以使用上述连接方式。</p>
<p><strong>安全模式连接到localhost:</strong></p>
<pre><code>mongodb://localhost/?safe=true
</code></pre>
<p><strong>以安全模式连接到replica set，并且等待至少两个复制服务器成功写入，超时时间设置为2秒。</strong></p>
<pre><code>mongodb://host1,host2,host3/?safe=true;w=2;wtimeoutMS=2000
</code></pre>
<h2 id="标准格式"><a class="header" href="#标准格式">标准格式</a></h2>
<p><code>mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]</code></p>
<h2 id="选项"><a class="header" href="#选项">选项</a></h2>
<table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody>
<tr><td>replicaSet=name</td><td>验证replica set的名称。 Impliesconnect=replicaSet.</td></tr>
<tr><td>slaveOk=true|false</td><td>true:在connect=direct模式下，驱动会连接第一台机器，即使这台服务器不是主。在connect=replicaSet模式下，驱动会发送所有的写请求到主并且把读取操作分布在其他从服务器。false: 在 connect=direct模式下，驱动会自动找寻主服务器. 在connect=replicaSet 模式下，驱动仅仅连接主服务器，并且所有的读写命令都连接到主服务器。</td></tr>
<tr><td>safe=true|false</td><td>true: 在执行更新操作之后，驱动都会发送getLastError命令来确保更新成功。(还要参考 wtimeoutMS).false: 在每次更新之后，驱动不会发送getLastError来确保更新成功。</td></tr>
<tr><td>w=n</td><td>驱动添加 { w : n } 到getLastError命令. 应用于safe=true。</td></tr>
<tr><td>wtimeoutMS=ms</td><td>驱动添加 { wtimeout : ms } 到 getlasterror 命令. 应用于 safe=true.</td></tr>
<tr><td>fsync=true|false</td><td>true: 驱动添加 { fsync : true } 到 getlasterror 命令.应用于 safe=true.false: 驱动不会添加到getLastError命令中。</td></tr>
<tr><td>journal=true|false</td><td>如果设置为 true, 同步到 journal (在提交到数据库前写入到实体中). 应用于 safe=true</td></tr>
<tr><td>connectTimeoutMS=ms</td><td>可以打开连接的时间。</td></tr>
<tr><td>socketTimeoutMS=ms</td><td>发送和接受sockets的时间。</td></tr>
</tbody></table>
<h1 id="mongodb数据库创建与删除"><a class="header" href="#mongodb数据库创建与删除">MongoDB数据库创建与删除</a></h1>
<h2 id="创建数据库"><a class="header" href="#创建数据库">创建数据库</a></h2>
<p>MongoDB 创建数据库的语法格式如下：</p>
<pre><code>use DATABASE_NAME
</code></pre>
<p>如果数据库不存在，则创建数据库，否则切换到指定数据库。</p>
<h3 id="实例"><a class="header" href="#实例">实例</a></h3>
<p>以下实例我们创建了数据库 php:</p>
<pre><code>&gt; use php
switched to db php
&gt; db
php
&gt;
</code></pre>
<p>如果你想查看所有数据库，可以使用 <strong>show dbs</strong> 命令：</p>
<pre><code>&gt; show dbs
local  0.078GB
test   0.078GB
&gt;
</code></pre>
<p>可以看到，我们刚创建的数据库 php 并不在数据库的列表中， 要显示它，我们需要向 php 数据库插入一些数据。</p>
<pre><code>&gt; db.php.insert({&quot;name&quot;:&quot;php中文网&quot;})
WriteResult({ &quot;nInserted&quot; : 1 })
&gt; show dbs
local   0.078GB
php  0.078GB
test    0.078GB
&gt;
</code></pre>
<h2 id="删除数据库"><a class="header" href="#删除数据库">删除数据库</a></h2>
<h3 id="语法-1"><a class="header" href="#语法-1">语法</a></h3>
<p>MongoDB 删除数据库的语法格式如下：</p>
<pre><code>db.dropDatabase()
</code></pre>
<p>删除当前数据库，默认为 test，你可以使用 db 命令查看当前数据库名。</p>
<h1 id="文档管理"><a class="header" href="#文档管理">文档管理</a></h1>
<p>本章节中我们将向大家介绍如何将数据插入到MongoDB的集合中。</p>
<p>文档的数据结构和JSON基本一样。</p>
<p>所有存储在集合中的数据都是BSON格式。</p>
<p>BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。</p>
<h2 id="插入文档"><a class="header" href="#插入文档">插入文档</a></h2>
<p>MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下：</p>
<pre><code>db.COLLECTION_NAME.insert(document)
</code></pre>
<h3 id="实例-1"><a class="header" href="#实例-1">实例</a></h3>
<p>以下文档可以存储在 MongoDB 的 php 数据库 的 col集合中：</p>
<pre><code>&gt;db.col.insert({title: 'MongoDB 教程', 
    description: 'MongoDB 是一个 Nosql 数据库',
    by: 'php中文网',
    url: 'http://www.php.cn',
    tags: ['mongodb', 'database', 'NoSQL'],
    likes: 100
})
</code></pre>
<h2 id="更新文档"><a class="header" href="#更新文档">更新文档</a></h2>
<p>MongoDB 使用 <strong>update()</strong> 和 <strong>save()</strong> 方法来更新集合中的文档。接下来让我们详细来看下两个函数的应用及其区别。</p>
<h3 id="update-方法"><a class="header" href="#update-方法">update() 方法</a></h3>
<p>update() 方法用于更新已存在的文档。语法格式如下：</p>
<pre><code>db.collection.update(
   &lt;query&gt;,
   &lt;update&gt;,
   {
     upsert: &lt;boolean&gt;,
     multi: &lt;boolean&gt;,
     writeConcern: &lt;document&gt;
   }
)
</code></pre>
<p><strong>参数说明：</strong></p>
<ul>
<li><strong>query</strong> : update的查询条件，类似sql update查询内where后面的。</li>
<li><strong>update</strong> : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的</li>
<li><strong>upsert</strong> : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。</li>
<li><strong>multi</strong> : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。</li>
<li><strong>writeConcern</strong> :可选，抛出异常的级别。</li>
</ul>
<p><strong>实例</strong></p>
<pre><code class="language-js">db.col.update(
	//query
	{'title':'MongoDB 教程'},
	//updates
	{$set:{'title':'MongoDB'}}
	//options
	{multi:true}
)
</code></pre>
<h3 id="save-方法"><a class="header" href="#save-方法">save() 方法</a></h3>
<p>save() 方法通过传入的文档来替换已有文档。语法格式如下：</p>
<pre><code class="language-js">db.collection.save(
   &lt;document&gt;,
   {
     writeConcern: &lt;document&gt;
   }
)
</code></pre>
<p><strong>参数说明：</strong></p>
<ul>
<li><strong>document</strong> : 文档数据。</li>
<li><strong>writeConcern</strong> :可选，抛出异常的级别。</li>
</ul>
<p>以下实例中我们替换了 _id  为 56064f89ade2f21f36b03136 的文档数据：</p>
<pre><code class="language-js">&gt;db.col.save({
	&quot;_id&quot; : ObjectId(&quot;56064f89ade2f21f36b03136&quot;),
    &quot;title&quot; : &quot;MongoDB&quot;,
    &quot;description&quot; : &quot;MongoDB 是一个 Nosql 数据库&quot;,
    &quot;by&quot; : &quot;php&quot;,
    &quot;url&quot; : &quot;http://www.php.cn&quot;,
    &quot;tags&quot; : [
            &quot;mongodb&quot;,
            &quot;NoSQL&quot;
    ],
    &quot;likes&quot; : 110
})
</code></pre>
<h3 id="更多实例"><a class="header" href="#更多实例">更多实例</a></h3>
<pre><code>只更新第一条记录：
db.col.update( { &quot;count&quot; : { $gt : 1 } } , { $set : { &quot;test2&quot; : &quot;OK&quot;} } );
全部更新：

db.col.update( { &quot;count&quot; : { $gt : 3 } } , { $set : { &quot;test2&quot; : &quot;OK&quot;} },false,true );
只添加第一条：

db.col.update( { &quot;count&quot; : { $gt : 4 } } , { $set : { &quot;test5&quot; : &quot;OK&quot;} },true,false );
全部添加加进去:

db.col.update( { &quot;count&quot; : { $gt : 5 } } , { $set : { &quot;test5&quot; : &quot;OK&quot;} },true,true );
全部更新：

db.col.update( { &quot;count&quot; : { $gt : 15 } } , { $inc : { &quot;count&quot; : 1} },false,true );
只更新第一条记录：

db.col.update( { &quot;count&quot; : { $gt : 10 } } , { $inc : { &quot;count&quot; : 1} },false,false );
</code></pre>
<h2 id="删除文档"><a class="header" href="#删除文档">删除文档</a></h2>
<p>remove() 方法的基本语法格式如下所示：</p>
<pre><code class="language-js">db.collection.remove(
   &lt;query&gt;,
   &lt;justOne&gt;
)
</code></pre>
<p>如果你的 MongoDB 是 2.6 版本以后的，语法格式如下：</p>
<pre><code class="language-js">db.collection.remove(
   &lt;query&gt;,
   {
     justOne: &lt;boolean&gt;,
     writeConcern: &lt;document&gt;
   }
)
</code></pre>
<p><strong>参数说明：</strong></p>
<ul>
<li><strong>query</strong> :（可选）删除的文档的条件。</li>
<li><strong>justOne</strong> : （可选）如果设为 true 或 1，则只删除一个文档。</li>
<li><strong>writeConcern</strong> :（可选）抛出异常的级别。</li>
</ul>
<h2 id="查询文档"><a class="header" href="#查询文档">查询文档</a></h2>
<h3 id="mongodb-与-rdbms-where-语句比较"><a class="header" href="#mongodb-与-rdbms-where-语句比较">MongoDB 与 RDBMS Where 语句比较</a></h3>
<p>如果你熟悉常规的 SQL 数据，通过下表可以更好的理解 MongoDB 的条件语句查询：</p>
<table><thead><tr><th>操作</th><th>格式</th><th>范例</th><th>RDBMS中的类似语句</th></tr></thead><tbody>
<tr><td>等于</td><td><code>{&lt;key&gt;:&lt;value&gt;</code>}</td><td><code>db.col.find({&quot;by&quot;:&quot;php中文网&quot;}).pretty()</code></td><td><code>where by = 'php中文网'</code></td></tr>
<tr><td>小于</td><td><code>{&lt;key&gt;:{$lt:&lt;value&gt;}}</code></td><td><code>db.col.find({&quot;likes&quot;:{$lt:50}}).pretty()</code></td><td><code>where likes &lt; 50</code></td></tr>
<tr><td>小于或等于</td><td><code>{&lt;key&gt;:{$lte:&lt;value&gt;}}</code></td><td><code>db.col.find({&quot;likes&quot;:{$lte:50}}).pretty()</code></td><td><code>where likes &lt;= 50</code></td></tr>
<tr><td>大于</td><td><code>{&lt;key&gt;:{$gt:&lt;value&gt;}}</code></td><td><code>db.col.find({&quot;likes&quot;:{$gt:50}}).pretty()</code></td><td><code>where likes &gt; 50</code></td></tr>
<tr><td>大于或等于</td><td><code>{&lt;key&gt;:{$gte:&lt;value&gt;}}</code></td><td><code>db.col.find({&quot;likes&quot;:{$gte:50}}).pretty()</code></td><td><code>where likes &gt;= 50</code></td></tr>
<tr><td>不等于</td><td><code>{&lt;key&gt;:{$ne:&lt;value&gt;}}</code></td><td><code>db.col.find({&quot;likes&quot;:{$ne:50}}).pretty()</code></td><td><code>where likes != 50</code></td></tr>
</tbody></table>
<h3 id="find"><a class="header" href="#find">find</a></h3>
<p><strong>findOne用于查一个</strong></p>
<p>MongoDB 查询数据的语法格式如下：</p>
<pre><code>&gt;db.COLLECTION_NAME.find({query})
</code></pre>
<p><strong>find() 方法以非结构化的方式来显示所有文档。</strong></p>
<p>如果你需要以易读的方式来读取数据，可以使用 pretty() 方法，语法格式如下：</p>
<pre><code>&gt;db.col.find().pretty()
</code></pre>
<h3 id="mongodb-and-条件"><a class="header" href="#mongodb-and-条件">MongoDB AND 条件</a></h3>
<pre><code>&gt;db.col.find({key1:value1, key2:value2}).pretty()
</code></pre>
<h3 id="mongodb-or-条件"><a class="header" href="#mongodb-or-条件">MongoDB OR 条件</a></h3>
<pre><code class="language-js">&gt;db.col.find(
   {
      $or: [
	     {key1: value1}, {key2:value2}
      ]
   }
).pretty()

</code></pre>
<h3 id="and-和-or-联合使用"><a class="header" href="#and-和-or-联合使用">AND 和 OR 联合使用</a></h3>
<p><strong>'where likes&gt;50 AND (by = 'php中文网' OR title = 'MongoDB 教程')'</strong></p>
<pre><code>db.col.find({&quot;likes&quot;: {$gt:50}, $or: [{&quot;by&quot;: &quot;php中文网&quot;},{&quot;title&quot;: &quot;MongoDB 教程&quot;}]}).pretty()
</code></pre>
<p>查看已插入文档：</p>
<pre><code class="language-js">&gt; db.col.find()
{ &quot;_id&quot; : ObjectId(&quot;56064886ade2f21f36b03134&quot;), &quot;title&quot; : &quot;MongoDB 教程&quot;, &quot;description&quot; : &quot;MongoDB 是一个 Nosql 数据库&quot;, &quot;by&quot; : &quot;php中文网&quot;, &quot;url&quot; : &quot;http://www.php.cn&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;database&quot;, &quot;NoSQL&quot; ], &quot;likes&quot; : 100 }
&gt;
</code></pre>
<p><strong>定义变量</strong></p>
<p>我们也可以将数据定义为一个变量，如下所示：</p>
<pre><code class="language-js">document=({title: 'MongoDB 教程', 
    description: 'MongoDB 是一个 Nosql 数据库',
    by: 'php中文网',
    url: 'http://www.php.cn',
    tags: ['mongodb', 'database', 'NoSQL'],
    likes: 100
});

db.col.insert(document)
</code></pre>
<p>插入文档你也可以使用 db.col.save(document) 命令。如果不指定 _id 字段 save() 方法类似于 insert() 方法。如果指定 _id 字段，则会更新该 _id 的数据。</p>
<h1 id="其他关键字"><a class="header" href="#其他关键字">其他关键字</a></h1>
<h2 id="type-操作符"><a class="header" href="#type-操作符">$type 操作符</a></h2>
<p>MongoDB 中可以使用的类型如下表所示：</p>
<table><thead><tr><th><strong>类型</strong></th><th><strong>数字</strong></th><th><strong>备注</strong></th></tr></thead><tbody>
<tr><td>Double</td><td>1</td><td></td></tr>
<tr><td>String</td><td>2</td><td></td></tr>
<tr><td>Object</td><td>3</td><td></td></tr>
<tr><td>Array</td><td>4</td><td></td></tr>
<tr><td>Binary data</td><td>5</td><td></td></tr>
<tr><td>Undefined</td><td>6</td><td>已废弃。</td></tr>
<tr><td>Object id</td><td>7</td><td></td></tr>
<tr><td>Boolean</td><td>8</td><td></td></tr>
<tr><td>Date</td><td>9</td><td></td></tr>
<tr><td>Null</td><td>10</td><td></td></tr>
<tr><td>Regular Expression</td><td>11</td><td></td></tr>
<tr><td>JavaScript</td><td>13</td><td></td></tr>
<tr><td>Symbol</td><td>14</td><td></td></tr>
<tr><td>JavaScript (with scope)</td><td>15</td><td></td></tr>
<tr><td>32-bit integer</td><td>16</td><td></td></tr>
<tr><td>Timestamp</td><td>17</td><td></td></tr>
<tr><td>64-bit integer</td><td>18</td><td></td></tr>
<tr><td>Min key</td><td>255</td><td>Query with <code>-1</code>.</td></tr>
<tr><td>Max key</td><td>127</td><td></td></tr>
</tbody></table>
<p>$type操作符是基于BSON类型来检索集合中匹配的数据类型，并返回结果。</p>
<p>如果想获取 &quot;col&quot; 集合中 title 为 String 的数据，你可以使用以下命令：</p>
<pre><code>db.col.find({&quot;title&quot; : {$type : 2}})
</code></pre>
<h2 id="limit"><a class="header" href="#limit">Limit()</a></h2>
<p>如果你需要在MongoDB中读取指定数量的数据记录，可以使用MongoDB的Limit方法，limit()方法接受一个数字参数，该参数指定从MongoDB中读取的记录条数。</p>
<pre><code>&gt;db.COLLECTION_NAME.find().limit(NUMBER)
</code></pre>
<h2 id="skip"><a class="header" href="#skip">Skip()</a></h2>
<p>我们除了可以使用limit()方法来读取指定数量的数据外，还可以使用skip()方法来跳过指定数量的数据，skip方法同样接受一个数字参数作为跳过的记录条数。</p>
<pre><code>&gt;db.COLLECTION_NAME.find().limit(NUMBER).skip(NUMBER)
</code></pre>
<h2 id="sort"><a class="header" href="#sort">sort()</a></h2>
<p>sort()方法可以通过参数指定排序的字段，并使用 1 和 -1 来指定排序的方式，其中 1 为升序排列，而-1是用于降序排列。</p>
<pre><code>db.COLLECTION_NAME.find().sort({KEY:1})
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="索引"><a class="header" href="#索引">索引</a></h1>
<p>索引通常能够极大的提高查询的效率，如果没有索引，MongoDB在读取数据时必须扫描集合中的每个文件并选取那些符合查询条件的记录。</p>
<p>索引存储在一个易于遍历读取的数据集合中，索引是对数据库表中一列或多列的值进行排序的一种结构</p>
<h2 id="ensureindex-方法"><a class="header" href="#ensureindex-方法">ensureIndex() 方法</a></h2>
<p>MongoDB使用 ensureIndex() 方法来创建索引。</p>
<h3 id="语法-2"><a class="header" href="#语法-2">语法</a></h3>
<p>ensureIndex()方法基本语法格式如下所示：</p>
<pre><code>&gt;db.COLLECTION_NAME.ensureIndex({KEY:1})
</code></pre>
<p>ensureIndex() 方法中你也可以设置使用多个字段创建索引（关系型数据库中称作复合索引）。</p>
<pre><code>&gt;db.col.ensureIndex({&quot;title&quot;:1,&quot;description&quot;:-1})
&gt;
</code></pre>
<p>ensureIndex() 接收可选参数，可选参数列表如下：</p>
<table><thead><tr><th>Parameter</th><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td>background</td><td>Boolean</td><td>建索引过程会阻塞其它数据库操作，background可指定以后台方式创建索引，即增加 &quot;background&quot;  可选参数。  &quot;background&quot; 默认值为<strong>false</strong>。</td></tr>
<tr><td>unique</td><td>Boolean</td><td>建立的索引是否唯一。指定为true创建唯一索引。默认值为<strong>false</strong>.</td></tr>
<tr><td>name</td><td>string</td><td>索引的名称。如果未指定，MongoDB的通过连接索引的字段名和排序顺序生成一个索引名称。</td></tr>
<tr><td>dropDups</td><td>Boolean</td><td>在建立唯一索引时是否删除重复记录,指定 true 创建唯一索引。默认值为 <strong>false</strong>.</td></tr>
<tr><td>sparse</td><td>Boolean</td><td>对文档中不存在的字段数据不启用索引；这个参数需要特别注意，如果设置为true的话，在索引字段中不会查询出不包含对应字段的文档.。默认值为 <strong>false</strong>.</td></tr>
<tr><td>expireAfterSeconds</td><td>integer</td><td>指定一个以秒为单位的数值，完成 TTL设定，设定集合的生存时间。</td></tr>
<tr><td>v</td><td>index version</td><td>索引的版本号。默认的索引版本取决于mongod创建索引时运行的版本。</td></tr>
<tr><td>weights</td><td>document</td><td>索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重。</td></tr>
<tr><td>default_language</td><td>string</td><td>对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语</td></tr>
<tr><td>language_override</td><td>string</td><td>对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language.</td></tr>
</tbody></table>
<h3 id="实例-2"><a class="header" href="#实例-2">实例</a></h3>
<p>在后台创建索引：</p>
<pre><code>db.values.ensureIndex({open: 1, close: 1}, {background: true})
</code></pre>
<h1 id="聚合"><a class="header" href="#聚合">聚合</a></h1>
<p>MongoDB中聚合(aggregate)主要用于处理数据(诸如统计平均值,求和等)，并返回计算后的数据结果。有点类似sql语句中的 count(*)。</p>
<h2 id="aggregate-方法"><a class="header" href="#aggregate-方法">aggregate() 方法</a></h2>
<p>MongoDB中聚合的方法使用aggregate()。</p>
<h3 id="语法-3"><a class="header" href="#语法-3">语法</a></h3>
<p>aggregate() 方法的基本语法格式如下所示：</p>
<pre><code>&gt;db.COLLECTION_NAME.aggregate(AGGREGATE_OPERATION)
</code></pre>
<p>现在我们通过以上集合计算每个作者所写的文章数，使用aggregate()计算结果如下：</p>
<p>以上实例类似sql语句： <em>select by_user, count(*) from mycol group by by_user</em></p>
<pre><code>&gt; db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$sum : 1}}}])
{
   &quot;result&quot; : [
      {
         &quot;_id&quot; : &quot;w3cschool.cc&quot;,
         &quot;num_tutorial&quot; : 2
      },
      {
         &quot;_id&quot; : &quot;Neo4j&quot;,
         &quot;num_tutorial&quot; : 1
      }
   ],
   &quot;ok&quot; : 1
}
&gt;
</code></pre>
<p>下表展示了一些聚合的表达式:</p>
<table><thead><tr><th>表达式</th><th>描述</th><th>实例</th></tr></thead><tbody>
<tr><td>$sum</td><td>计算总和。</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$sum : &quot;$likes&quot;}}}])</td></tr>
<tr><td>$avg</td><td>计算平均值</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$avg : &quot;$likes&quot;}}}])</td></tr>
<tr><td>$min</td><td>获取集合中所有文档对应值得最小值。</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$min : &quot;$likes&quot;}}}])</td></tr>
<tr><td>$max</td><td>获取集合中所有文档对应值得最大值。</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, num_tutorial : {$max : &quot;$likes&quot;}}}])</td></tr>
<tr><td>$push</td><td>在结果文档中插入值到一个数组中。</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, url : {$push: &quot;$url&quot;}}}])</td></tr>
<tr><td>$addToSet</td><td>在结果文档中插入值到一个数组中，但不创建副本。</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, url : {$addToSet : &quot;$url&quot;}}}])</td></tr>
<tr><td>$first</td><td>根据资源文档的排序获取第一个文档数据。</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, first_url : {$first : &quot;$url&quot;}}}])</td></tr>
<tr><td>$last</td><td>根据资源文档的排序获取最后一个文档数据</td><td>db.mycol.aggregate([{$group : {_id : &quot;$by_user&quot;, last_url : {$last : &quot;$url&quot;}}}])</td></tr>
</tbody></table>
<h2 id="管道的概念"><a class="header" href="#管道的概念">管道的概念</a></h2>
<p>管道在Unix和Linux中一般用于将当前命令的输出结果作为下一个命令的参数。</p>
<p>MongoDB的聚合管道将MongoDB文档在一个管道处理完毕后将结果传递给下一个管道处理。管道操作是可以重复的。</p>
<p>表达式：处理输入文档并输出。表达式是无状态的，只能用于计算当前聚合管道的文档，不能处理其它的文档。</p>
<p>这里我们介绍一下聚合框架中常用的几个操作：</p>
<ul>
<li>$project：修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。</li>
<li>$match：用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。</li>
<li>$limit：用来限制MongoDB聚合管道返回的文档数。</li>
<li>$skip：在聚合管道中跳过指定数量的文档，并返回余下的文档。</li>
<li>$unwind：将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。</li>
<li>$group：将集合中的文档分组，可用于统计结果。</li>
<li>$sort：将输入文档排序后输出。</li>
<li>$geoNear：输出接近某一地理位置的有序文档。</li>
</ul>
<pre><code class="language-js">db.article.aggregate(
    { $project : {
        title : 1 ,
        author : 1 ,
    }}
 );
</code></pre>
<p>这样的话结果中就只还有_id,tilte和author三个字段了，默认情况下_id字段是被包含的，如果要想不包含_id话可以这样:</p>
<pre><code>db.article.aggregate(
    { $project : {
        _id : 0 ,
        title : 1 ,
        author : 1
    }});
</code></pre>
<p>$match实例</p>
<pre><code>db.articles.aggregate( [
                        { $match : { score : { $gt : 70, $lte : 90 } } },
                        { $group: { _id: null, count: { $sum: 1 } } }
                       ] );
</code></pre>
<p>$match用于获取分数大于70小于或等于90记录，然后将符合条件的记录送到下一阶段$group管道操作符进行处理。</p>
<p>3.$skip实例</p>
<pre><code>db.article.aggregate(
    { $skip : 5 });
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mongodb-security简介"><a class="header" href="#mongodb-security简介">Mongodb Security简介</a></h1>
<p>MongoDB提供了各种功能，例如身份验证，访问控制，加密，以保护您的MongoDB部署安全。
一些关键的安全功能包括：</p>
<table><thead><tr><th style="text-align: left">Authentication</th><th style="text-align: left">Authorization</th><th style="text-align: left">TLS/SSL</th></tr></thead><tbody>
<tr><td style="text-align: left"><a href="https://docs.mongodb.com/manual/core/authentication/">Authentication</a></td><td style="text-align: left"><a href="https://docs.mongodb.com/manual/core/authorization/">Role-Based Access Control</a></td><td style="text-align: left"><a href="https://docs.mongodb.com/manual/core/security-transport-encryption/">TLS/SSL (Transport Encryption)</a></td></tr>
<tr><td style="text-align: left"><a href="https://docs.mongodb.com/manual/core/security-x.509/">x.509</a></td><td style="text-align: left"><a href="https://docs.mongodb.com/manual/tutorial/enable-authentication/">Enable Access Control</a></td><td style="text-align: left"><a href="https://docs.mongodb.com/manual/tutorial/configure-ssl/">Configure <code>mongod</code> and <code>mongos</code> for TLS/SSL</a></td></tr>
<tr><td style="text-align: left"><a href="https://docs.mongodb.com/manual/core/security-scram/">SCRAM</a></td><td style="text-align: left"><a href="https://docs.mongodb.com/manual/tutorial/manage-users-and-roles/">Manage Users and Roles</a></td><td style="text-align: left"><a href="https://docs.mongodb.com/manual/tutorial/configure-ssl-clients/">TLS/SSL Configuration for Clients</a></td></tr>
</tbody></table>
<h1 id="security-checklist"><a class="header" href="#security-checklist">Security Checklist</a></h1>
<blockquote>
<p>安全自查手册</p>
</blockquote>
<p>本文档提供了应实施的安全措施列表，以保护您的MongoDB安装。该列表并非详尽无遗。</p>
<h2 id="部署前的注意事项"><a class="header" href="#部署前的注意事项">部署前的注意事项</a></h2>
<h3 id="enable-access-control-and-enforce-authentication"><a class="header" href="#enable-access-control-and-enforce-authentication">Enable Access Control and Enforce Authentication</a></h3>
<p>启用访问控制并指定身份验证机制。
您可以使用MongoDB的SCRAM或x.509身份验证机制，也可以与现有的Kerberos / LDAP基础结构集成。
身份验证要求所有客户端和服务器都必须提供有效的凭据，然后才能连接到系统。</p>
<ul>
<li>See also:
<ul>
<li><a href="https://docs.mongodb.com/manual/core/authentication/">Authentication</a></li>
<li><a href="https://docs.mongodb.com/manual/tutorial/enable-authentication/">Enable Access Control</a></li>
</ul>
</li>
</ul>
<h3 id="configure-role-based-access-control"><a class="header" href="#configure-role-based-access-control">Configure Role-Based Access Control</a></h3>
<ul>
<li>首先创建一个用户管理员，然后创建其他用户。为访问系统的每个人/应用程序创建一个唯一的MongoDB用户。</li>
<li>遵循最小特权原则。创建角色，以定义一组用户所需的确切访问权限。然后创建用户，并仅为其分配执行操作所需的角色。用户可以是个人或客户应用程序。</li>
<li>用户可以在不同的数据库之间拥有特权。如果用户需要对多个数据库的特权，请创建一个具有授予适用数据库特权的角色的单个用户，而不要在不同的数据库中多次创建该用户。</li>
</ul>
<ul>
<li>
<p>See also:</p>
<ul>
<li><a href="https://docs.mongodb.com/manual/core/authorization/">Role-Based Access Control</a></li>
<li><a href="https://docs.mongodb.com/manual/tutorial/manage-users-and-roles/">Manage Users and Roles</a></li>
</ul>
</li>
</ul>
<h3 id="encrypt-communication-tlsssl"><a class="header" href="#encrypt-communication-tlsssl">Encrypt Communication (TLS/SSL)</a></h3>
<p>配置MongoDB以对所有传入和传出连接使用TLS / SSL。
使用TLS / SSL加密MongoDB部署的mongod和mongos组件之间以及所有应用程序和MongoDB之间的通信。
从版本4.0开始，MongoDB使用本机TLS / SSL OS库：</p>
<table><thead><tr><th style="text-align: left">Platform</th><th style="text-align: left">TLS/SSL Library</th></tr></thead><tbody>
<tr><td style="text-align: left">Windows</td><td style="text-align: left">Secure Channel (Schannel)</td></tr>
<tr><td style="text-align: left">Linux/BSD</td><td style="text-align: left">OpenSSL</td></tr>
<tr><td style="text-align: left">macOS</td><td style="text-align: left">Secure Transport</td></tr>
</tbody></table>
<ul>
<li>See also: <a href="https://docs.mongodb.com/manual/tutorial/configure-ssl/">Configure <code>mongod</code> and <code>mongos</code> for TLS/SSL</a>.</li>
</ul>
<p>and so on...</p>
<h1 id="认证"><a class="header" href="#认证">认证</a></h1>
<h2 id="认证方法"><a class="header" href="#认证方法">认证方法</a></h2>
<p><strong>命令行认证</strong></p>
<pre><code>mongo --username &quot;myTestDBUser&quot; --password --authenticationDatabase test --authenticationMechanism SCRAM-SHA-256
或者 mongodb://userName:pwd@hostname
</code></pre>
<p><strong>登录后认证</strong></p>
<pre><code class="language-js">db.auth()
//Use passwordPrompt() to prompt the user to enter a password:
db.auth( &lt;username&gt;, passwordPrompt() )

//Specify a cleartext password:
db.auth( &lt;username&gt;, &lt;password&gt; )
</code></pre>
<pre><code class="language-js">db.auth( {
   user: &lt;username&gt;,
   pwd: passwordPrompt(),   // Or &quot;&lt;cleartext password&gt;&quot;
   mechanism: &lt;authentication mechanism&gt;,
   digestPassword: &lt;boolean&gt;
} )
</code></pre>
<table><thead><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>user</code></td><td style="text-align: left">string</td><td style="text-align: left">用户名</td></tr>
<tr><td style="text-align: left"><code>pwd</code></td><td style="text-align: left">string</td><td style="text-align: left">值可以是：明文字符串中的用户密码 或者提示用户输入密码</td></tr>
<tr><td style="text-align: left"><code>mechanism</code></td><td style="text-align: left">string</td><td style="text-align: left">可选，see <a href="https://docs.mongodb.com/manual/reference/program/mongo/#std-label-mongo-shell-authentication-mechanisms">authentication mechanisms</a>.If unspecified, uses the <a href="https://docs.mongodb.com/manual/reference/command/isMaster/#mongodb-dbcommand-dbcmd.isMaster"><code>isMaster</code></a> to determine the SASL mechanism or mechanisms for the specified user. See <a href="https://docs.mongodb.com/manual/reference/command/isMaster/#mongodb-data-isMaster.saslSupportedMechs"><code>saslSupportedMechs</code></a>.</td></tr>
<tr><td style="text-align: left"><code>digestPassword</code></td><td style="text-align: left">boolean</td><td style="text-align: left">可选，是否对密码加密.For <a href="https://docs.mongodb.com/manual/core/security-scram/#std-label-scram-mechanisms">SCRAM-SHA-1</a>, although you may specify <code>true</code>, setting this value to <code>true</code> does not improve security and may interfere with credentials using other mechanisms.For all other methods, this value must be set to <code>false</code> (default value). Any other value will result in authentication failure since those methods do not understand MongoDB pre-hashing.The default value is <code>false</code>.</td></tr>
</tbody></table>
<h2 id="改密码"><a class="header" href="#改密码">改密码</a></h2>
<pre><code>db.changeUserPassword(username, password)

</code></pre>
<table><thead><tr><th style="text-align: left">Parameter</th><th style="text-align: left">Type</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>username</code></td><td style="text-align: left">string</td><td style="text-align: left">The name of the user whose password you wish to change.</td></tr>
<tr><td style="text-align: left"><code>password</code></td><td style="text-align: left">string</td><td style="text-align: left">The user's password. The value can be either:the user's password in cleartext string, or<a href="https://docs.mongodb.com/manual/reference/method/passwordPrompt/#mongodb-method-passwordPrompt"><code>passwordPrompt()</code></a> to prompt for the user's password.TIPStarting in version 4.2 of the <a href="https://docs.mongodb.com/manual/reference/program/mongo/#mongodb-binary-bin.mongo"><code>mongo</code></a> shell, you can use the <a href="https://docs.mongodb.com/manual/reference/method/passwordPrompt/#mongodb-method-passwordPrompt"><code>passwordPrompt()</code></a> method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the <a href="https://docs.mongodb.com/manual/reference/program/mongo/#mongodb-binary-bin.mongo"><code>mongo</code></a> shell.</td></tr>
<tr><td style="text-align: left"><code>writeConcern</code></td><td style="text-align: left">document</td><td style="text-align: left">Optional. The level of <a href="https://docs.mongodb.com/manual/reference/write-concern/">write concern</a> for the creation operation. The <code>writeConcern</code> document takes the same fields as the <a href="https://docs.mongodb.com/manual/reference/command/getLastError/#mongodb-dbcommand-dbcmd.getLastError"><code>getLastError</code></a> command.</td></tr>
</tbody></table>
<h2 id="创建用户"><a class="header" href="#创建用户">创建用户</a></h2>
<pre><code>db.createUser(user, writeConcern)
</code></pre>
<p>The <a href="https://docs.mongodb.com/manual/reference/method/db.createUser/#mongodb-method-db.createUser"><code>db.createUser()</code></a> method has the following syntax:</p>
<table><thead><tr><th style="text-align: left">Field</th><th style="text-align: left">Type</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>user</code></td><td style="text-align: left">document</td><td style="text-align: left">The document with authentication and access information about the user to create.</td></tr>
<tr><td style="text-align: left"><code>writeConcern</code></td><td style="text-align: left">document</td><td style="text-align: left">Optional. The level of <a href="https://docs.mongodb.com/manual/reference/write-concern/">write concern</a> for the creation operation. The <code>writeConcern</code> document takes the same fields as the <a href="https://docs.mongodb.com/manual/reference/command/getLastError/#mongodb-dbcommand-dbcmd.getLastError"><code>getLastError</code></a> command.</td></tr>
</tbody></table>
<pre><code class="language-json">{
  user: &quot;&lt;name&gt;&quot;,
  pwd: passwordPrompt(),      // Or  &quot;&lt;cleartext password&gt;&quot;
  customData: { &lt;any information&gt; },
  roles: [
    { role: &quot;&lt;role&gt;&quot;, db: &quot;&lt;database&gt;&quot; } | &quot;&lt;role&gt;&quot;,
    ...
  ],
  authenticationRestrictions: [
     {
       clientSource: [&quot;&lt;IP&gt;&quot; | &quot;&lt;CIDR range&gt;&quot;, ...],
       serverAddress: [&quot;&lt;IP&gt;&quot; | &quot;&lt;CIDR range&gt;&quot;, ...]
     },
     ...
  ],
  mechanisms: [ &quot;&lt;SCRAM-SHA-1|SCRAM-SHA-256&gt;&quot;, ... ],
  passwordDigestor: &quot;&lt;server|client&gt;&quot;
}
</code></pre>
<p>The <code>user</code> document has the following fields:</p>
<table><thead><tr><th style="text-align: left">Field</th><th style="text-align: left">Type</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>user</code></td><td style="text-align: left">string</td><td style="text-align: left">The name of the new user.</td></tr>
<tr><td style="text-align: left"><code>pwd</code></td><td style="text-align: left">string</td><td style="text-align: left">The user's password. The <code>pwd</code> field is not required if you run <a href="https://docs.mongodb.com/manual/reference/method/db.createUser/#mongodb-method-db.createUser"><code>db.createUser()</code></a> on the <code>$external</code> database to create users who have credentials stored externally to MongoDB.The value can be either:the user's password in cleartext string, or<a href="https://docs.mongodb.com/manual/reference/method/passwordPrompt/#mongodb-method-passwordPrompt"><code>passwordPrompt()</code></a> to prompt for the user's password.TIPStarting in version 4.2 of the <a href="https://docs.mongodb.com/manual/reference/program/mongo/#mongodb-binary-bin.mongo"><code>mongo</code></a> shell, you can use the <a href="https://docs.mongodb.com/manual/reference/method/passwordPrompt/#mongodb-method-passwordPrompt"><code>passwordPrompt()</code></a> method in conjunction with various user authentication/management methods/commands to prompt for the password instead of specifying the password directly in the method/command call. However, you can still specify the password directly as you would with earlier versions of the <a href="https://docs.mongodb.com/manual/reference/program/mongo/#mongodb-binary-bin.mongo"><code>mongo</code></a> shell.</td></tr>
<tr><td style="text-align: left"><code>customData</code></td><td style="text-align: left">document</td><td style="text-align: left">Optional. Any arbitrary information. This field can be used to store any data an admin wishes to associate with this particular user. For example, this could be the user's full name or employee id.</td></tr>
<tr><td style="text-align: left"><code>roles</code></td><td style="text-align: left">array</td><td style="text-align: left">The roles granted to the user. Can specify an empty array <code>[]</code> to create users without roles.</td></tr>
<tr><td style="text-align: left"><a href="https://docs.mongodb.com/manual/reference/method/db.createUser/#std-label-db-createUser-authenticationRestrictions">authenticationRestrictions</a></td><td style="text-align: left">array</td><td style="text-align: left">Optional. The authentication restrictions the server enforces on the created user. Specifies a list of IP addresses and CIDR ranges from which the user is allowed to connect to the server or from which the server can accept users.<em>New in version 3.6</em>.</td></tr>
<tr><td style="text-align: left"><code>mechanisms</code></td><td style="text-align: left">array</td><td style="text-align: left">Optional. Specify the specific SCRAM mechanism or mechanisms for creating SCRAM user credentials. If <a href="https://docs.mongodb.com/manual/reference/parameters/#mongodb-parameter-param.authenticationMechanisms"><code>authenticationMechanisms</code></a> is specified, you can only specify a subset of the <a href="https://docs.mongodb.com/manual/reference/parameters/#mongodb-parameter-param.authenticationMechanisms"><code>authenticationMechanisms</code></a>.Valid values are:<code>&quot;SCRAM-SHA-1&quot;</code>Uses the <code>SHA-1</code> hashing function.<code>&quot;SCRAM-SHA-256&quot;</code>Uses the <code>SHA-256</code> hashing function.Requires featureCompatibilityVersion set to <code>4.0</code>.Requires passwordDigestor to be <code>server</code>.The default for featureCompatibilityVersion is <code>4.0</code> is both <code>SCRAM-SHA-1</code> and <code>SCRAM-SHA-256</code>.The default for featureCompatibilityVersion is <code>3.6</code> is <code>SCRAM-SHA-1</code>.<em>New in version 4.0</em>.</td></tr>
<tr><td style="text-align: left"><code>passwordDigestor</code></td><td style="text-align: left">string</td><td style="text-align: left">Optional. Indicates whether the server or the client digests the password.Available values are:<code>&quot;server&quot;</code> (Default)The server receives undigested password from the client and digests the password.<code>&quot;client&quot;</code> (Not compatible with <code>SCRAM-SHA-256</code>)The client digests the password and passes the digested password to the server.<em>Changed in version 4.0</em>: The default value is <code>&quot;server&quot;</code>. In earlier versions, the default value is <code>&quot;client&quot;</code>.</td></tr>
</tbody></table>
<h3 id="roles"><a class="header" href="#roles">Roles</a></h3>
<p>在角色字段中，您可以指定内置角色和用户定义的角色。</p>
<p>要指定存在于其他数据库中的角色，请与文档一起指定该角色。</p>
<pre><code>&quot;readWrite&quot;
or
{ role: &quot;&lt;role&gt;&quot;, db: &quot;&lt;database&gt;&quot; }
</code></pre>
<h3 id="authentication-restrictions"><a class="header" href="#authentication-restrictions">Authentication Restrictions</a></h3>
<table><thead><tr><th style="text-align: left">Field Name</th><th style="text-align: left">Value</th><th style="text-align: left">Description</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>clientSource</code></td><td style="text-align: left">Array of IP addresses and/or CIDR ranges</td><td style="text-align: left">如果存在，则在验证用户身份时，服务器将验证客户端的IP地址在给定列表中还是属于列表中的CIDR范围。<br/>如果客户端的IP地址不存在，则服务器不会对用户进行身份验证。</td></tr>
<tr><td style="text-align: left"><code>serverAddress</code></td><td style="text-align: left">Array of IP addresses and/or CIDR ranges</td><td style="text-align: left">客户端可以连接到的IP地址或CIDR范围的列表。<br/>如果存在，服务器将通过给定列表中的IP地址验证是否接受了客户端的连接。<br/>如果通过无法识别的IP地址接受了连接，则服务器不会对用户进行身份验证。</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><p>find_in_set
group_concat</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mysql-57-mysqlpump-备份工具说明"><a class="header" href="#mysql-57-mysqlpump-备份工具说明">MySQL 5.7 mysqlpump 备份工具说明</a></h1>
<p>mysqlpump和mysqldump一样，属于逻辑备份，备份以SQL形式的文本保存。逻辑备份相对物理备份的好处是不关心undo log的大小，直接备份数据即可。它最主要的特点是：</p>
<ul>
<li><strong>并行备份</strong>数据库和数据库中的对象的，加快备份过程。</li>
<li><strong>更好的控制数据库和数据库对象</strong>（表，存储过程，用户帐户）的备份。</li>
<li>备份用户账号作为帐户管理语句（CREATE USER，GRANT），而不是直接插入到MySQL的系统数据库。</li>
<li><strong>备份出来直接生成压缩后的备份文件。</strong></li>
<li>备份进度指示（估计值）。</li>
<li>重新加载（还原）备份文件，先建表后插入数据最后建立索引，减少了索引维护开销，加快了还原速度。</li>
<li>备份可以排除或则指定数据库。</li>
</ul>
<h1 id="选项说明"><a class="header" href="#选项说明">选项说明</a></h1>
<p>**参数：**绝大部分参数和mysqldump一致，顺便复习一下。对于mysqlpump参数会标记出来。</p>
<h2 id="mysqldump与mysqlpump共同拥有"><a class="header" href="#mysqldump与mysqlpump共同拥有">mysqldump与mysqlpump共同拥有</a></h2>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_add-drop-database">--add-drop-database</a>：在建立库之前先执行删库操作。</p>
<pre><code>DROP DATABASE IF EXISTS `...`;
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_add-drop-table">--add-drop-table</a>：在建表之前先执行删表操作。</p>
<pre><code>DROP TABLE IF EXISTS `...`.`...`;
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_add-locks">--add-locks</a>：备份表时，使用LOCK TABLES和UNLOCK TABLES。**注意：**这个参数不支持并行备份，需要关闭并行备份功能：<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_default-parallelism">--default-parallelism</a>=0 </p>
<pre><code>LOCK TABLES `...`.`...` WRITE;
...
UNLOCK TABLES;
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_all-databases">--all-databases</a>：备份所有库，-A。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_bind-address">--bind-address</a>：指定通过哪个网络接口来连接Mysql服务器（一台服务器可能有多个IP），防止同一个网卡出去影响业务。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_complete-insert">--complete-insert</a>：dump出包含所有列的完整insert语句。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_compress">--compress</a>： 压缩客户端和服务器传输的所有的数据，-C。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_databases">--databases</a>：手动指定要备份的库，支持多个数据库，用空格分隔，-B。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_default-character-set">--default-character-set</a>：指定备份的字符集。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_events">--events</a>：备份数据库的事件，默认开启，关闭使用--skip-events参数。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_insert-ignore">--insert-ignore</a>：备份用insert ignore语句代替insert语句。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_log-error-file">--log-error-file</a>：备份出现的warnings和erros信息输出到一个指定的文件。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_max-allowed-packet">--max-allowed-packet</a>：备份时用于client/server直接通信的最大buffer包的大小。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_net-buffer-length">--net-buffer-length</a>：备份时用于client/server通信的初始buffer大小，当创建多行插入语句的时候，mysqlpump 创建行到N个字节长。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_no-create-db">--no-create-db</a>：备份不写CREATE DATABASE语句。要是备份多个库，需要使用参数-B，而使用-B的时候会出现create database语句，该参数可以屏蔽create database 语句。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_no-create-info">--no-create-info</a>：备份不写建表语句，即不备份表结构，只备份数据，-t。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_hex-blob">--hex-blob</a>： 备份binary字段的时候使用十六进制计数法，受影响的字段类型有BINARY、VARBINARY、BLOB、BIT。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_host">--host</a> ：备份指定的数据库地址，-h。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_password">--password</a>：备份需要的密码。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_port">--port</a> ：备份数据库的端口。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_protocol">--protocol={TCP|SOCKET|PIPE|MEMORY}</a>：指定连接服务器的协议。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_replace">--replace</a>：备份出来replace into语句。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_routines">--routines</a>：备份出来包含存储过程和函数，默认开启，需要对 <code>mysql.proc表有查看权限。生成的文件中会包含CREATE PROCEDURE 和 CREATE FUNCTION语句以用于恢复，关闭则需要用--skip-routines参数。</code></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_triggers">--triggers</a>：备份出来包含触发器，默认开启，使用<code>--skip-triggers来关闭。</code></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_set-charset">--set-charset</a>：备份文件里写SET NAMES default_character_set 到输出，此参默认开启。 -- skip-set-charset禁用此参数，不会在备份文件里面写出set names...</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_single-transaction">--single-transaction</a>：该参数在事务隔离级别设置成Repeatable Read，并在dump之前发送start transaction 语句给服务端。这在使用innodb时很有用，因为在发出start transaction时，保证了在不阻塞任何应用下的一致性状态。对myisam和memory等非事务表，还是会改变状态的，当使用此参的时候要确保没有其他连接在使用ALTER TABLE、CREATE TABLE、DROP TABLE、RENAME TABLE、TRUNCATE TABLE等语句，否则会出现不正确的内容或则失败。--add-locks和此参互斥，在mysql5.7.11之前，--default-parallelism大于1的时候和此参也互斥，必须使用--default-parallelism=0。5.7.11之后解决了--single-transaction和--default-parallelism的互斥问题。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_skip-definer">--skip-definer</a>：忽略那些创建视图和存储过程用到的 DEFINER 和 SQL SECURITY 语句，恢复的时候，会使用默认值，否则会在还原的时候看到没有DEFINER定义时的账号而报错。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_socket">--socket</a>：对于连接到localhost，Unix使用套接字文件，在Windows上是命名管道的名称使用，-S。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_ssl">--ssl</a>：--ssl参数将要被去除，用<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_ssl">--ssl-mode</a>取代。关于ssl相关的备份，请看<a href="https://dev.mysql.com/doc/refman/5.7/en/secure-connection-options.html#option_general_ssl-mode">官方文档</a>。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_tz-utc">--tz-utc</a>：备份时会在备份文件的最前几行添加SET TIME_ZONE='+00:00'。**注意：**如果还原的服务器不在同一个时区并且还原表中的列有timestamp字段，会导致还原出来的结果不一致。默认开启该参数，用 <code>--skip-tz-utc来关闭参数。</code></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_user">--user</a>：备份时候的用户名，-u。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_users">--users</a>：备份数据库用户，备份的形式是CREATE USER...，GRANT...，只备份数据库账号可以通过如下命令：</p>
<pre><code>mysqlpump --exclude-databases=% --users    #过滤掉所有数据库
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_watch-progress">--watch-progress</a>：定期显示进度的完成，包括总数表、行和其他对象。该参数默认开启，用<code>--skip-watch-progress来关闭。</code></p>
<h2 id="mysqlpump新增功能"><a class="header" href="#mysqlpump新增功能">mysqlpump新增功能</a></h2>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_add-drop-user">--add-drop-user</a>：在CREATE USER语句之前增加DROP USER，**注意：**这个参数需要和<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_users">--users</a>一起使用，否者不生效。</p>
<pre><code>DROP USER 'backup'@'192.168.123.%';
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_compress-output">--compress-output</a>：默认不压缩输出，目前可以使用的压缩算法有LZ4和ZLIB。</p>
<pre><code>shell&gt; mysqlpump --compress-output=LZ4 &gt; dump.lz4
shell&gt; lz4_decompress dump.lz4 dump.txt

shell&gt; mysqlpump --compress-output=ZLIB &gt; dump.zlib
shell&gt; zlib_decompress dump.zlib dump.txt
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_skip-dump-rows">--skip-dump-rows</a>：</p>
<p>只备份表结构，不备份数据，-d。**注意：**mysqldump支持--no-data，mysqlpump不支持--no-data</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_default-parallelism">--default-parallelism</a>：指定并行线程数，默认是2，如果设置成0，表示不使用并行备份。**注意：**每个线程的备份步骤是：先create table但不建立二级索引（主键会在create table时候建立），再写入数据，最后建立二级索引。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_defer-table-indexes">--defer-table-indexes</a>：</p>
<p>延迟创建索引，直到所有数据都加载完之后，再创建索引，默认开启。若关闭则会和mysqldump一样：先创建一个表和所有索引，再导入数据，因为在加载还原数据的时候要维护二级索引的开销，导致效率比较低。关闭使用参数：<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_defer-table-indexes">--skip--defer-table-indexes</a>。</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_exclude-databases">--exclude-databases</a>：备份排除该参数指定的数据库，多个用逗号分隔。类似的还有<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_exclude-events">--exclude-events</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_exclude-routines">--exclude-routines</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_exclude-tables">--exclude-tables</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_exclude-triggers">--exclude-triggers</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_exclude-users">--exclude-users</a>。</p>
<pre><code>mysqlpump --exclude-databases=mysql,sys    #备份过滤mysql和sys数据库

mysqlpump --exclude-tables=rr,tt   #备份过滤所有数据库中rr、tt表

mysqlpump -B test --exclude-tables=tmp_ifulltext,tt #备份过滤test库中的rr、tt表
</code></pre>
<p>要是只备份数据库的账号，需要添加参数<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_users">--users</a>，并且需要过滤掉所有的数据库，如：</p>
<pre><code>mysqlpump --users --exclude-databases=sys,mysql,db1,db2 --exclude-users=dba,backup  #备份除dba和backup的所有账号。
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_include-databases">--include-databases</a>：指定备份数据库，多个用逗号分隔，类似的还有<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_include-events">--include-events</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_include-routines">--include-routines</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_include-tables">--include-tables</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_include-triggers">--include-triggers</a>、<a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html#option_mysqlpump_include-users">--include-users</a>，大致方法使用同</p>
<p><code>--parallel-schemas=[N:]db_list：</code></p>
<p>指定并行备份的库，多个库用逗号分隔，如果指定了N，将使用N个线程的地队列，如果N不指定，将由 --default-parallelism才确认N的值，可以设置多个<code>--parallel-schemas。</code></p>
<pre><code>mysqlpump --parallel-schemas=4:vs,aa --parallel-schemas=3:pt   #4个线程备份vs和aa，3个线程备份pt。通过show processlist 可以看到有7个线程。

mysqlpump --parallel-schemas=vs,abc --parallel-schemas=pt  #默认2个线程，即2个线程备份vs和abc，2个线程备份pt

####当然要是硬盘IO不允许的话，可以少开几个线程和数据库进行并行备份
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html">官方文档</a></p>
<p><a href="https://www.cnblogs.com/zhoujinyi/p/5684903.html">参考文档</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>配置环境变量</p>
<p>在mysql安装路径下 建 mysql.ini,与 data目录</p>
<pre><code class="language-dart">[mysql]

# 设置mysql客户端默认字符集
default-character-set=utf8 

[mysqld]

#设置3306端口
port = 3306 

# 设置mysql的安装目录
basedir=F:\mysql\mysql-5.7.24-winx64\mysql-5.7.24-winx64

# 设置mysql数据库的数据的存放目录
datadir=F:\mysql\mysql-5.7.24-winx64\mysql-5.7.24-winx64\data

# 允许最大连接数
max_connections=200

# 服务端使用的字符集默认为8比特编码的latin1字符集
character-set-server=utf8

# 创建新表时将使用的默认存储引擎
default-storage-engine=INNODB
</code></pre>
<p>初始化mysql服务</p>
<pre><code>mysqld --initialize-insecure --user=mysql
mysqld install
net start mysql
</code></pre>
<p>登录mysql修改密码</p>
<pre><code>mysql -u root -p
mysqladmin -u root password
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="什么是公用表表达式cte"><a class="header" href="#什么是公用表表达式cte">什么是公用表表达式(CTE)?</a></h1>
<p><em>common table expression</em></p>
<ol>
<li>
<p>公用表表达式是一个<strong>命名的临时结果集</strong></p>
</li>
<li>
<p>仅在单个SQL语句  SELECT，INSERTUPDATE或DELETE的执行范围内存在</p>
</li>
<li>
<p><em>CTE</em>不作为对象存储，仅在查询执行期间持续。 与派生表不同，<em>CTE</em>可以是自引用(<a href="http://www.yiibai.com/mysql/recursive-cte.html">递归CTE</a>)，也可以在同一查询中多次引用。 此外，与派生表相比，<em>CTE</em>提供了更好的可读性和性能。</p>
</li>
</ol>
<h1 id="mysql-cte语法"><a class="header" href="#mysql-cte语法">MySQL CTE语法</a></h1>
<p><strong>组成</strong></p>
<pre><code class="language-mysql">with cte_name (column_list) as (
	query
)
SELECT * FROM cte_name
</code></pre>
<p>请注意，查询中的列数必须与<code>column_list</code>中的列数相同。 如果省略<code>column_list</code>，<em>CTE</em>将使用定义<em>CTE</em>的查询的列列表</p>
<h1 id="多个cte"><a class="header" href="#多个cte">多个CTE</a></h1>
<pre><code class="language-mysql">WITH salesrep AS (
    SELECT 
        employeeNumber,
        CONCAT(firstName, ' ', lastName) AS salesrepName
    FROM
        employees
    WHERE
        jobTitle = 'Sales Rep'
),
customer_salesrep AS (
    SELECT 
        customerName, salesrepName
    FROM
        customers
            INNER JOIN
        salesrep ON employeeNumber = salesrepEmployeeNumber
)
SELECT 
    *
FROM
    customer_salesrep
ORDER BY customerName;
</code></pre>
<h1 id="递归查询"><a class="header" href="#递归查询">递归查询</a></h1>
<pre><code class="language-mysql">
WITH RECURSIVE test(id, name, path)
AS
(
    // 初始值
SELECT id, name, CAST(id AS CHAR(200))
FROM emp WHERE manager_id IS NULL
UNION ALL
    //递归查询
SELECT e.id, e.name, CONCAT(ep.path, ',', e.id)
FROM test AS ep JOIN emp AS e  ON ep.id = e.manager_id
)SELECT * FROM test ORDER BY path;
</code></pre>
<h1 id="使用cte的好处"><a class="header" href="#使用cte的好处">使用CTE的好处</a></h1>
<p>○ 可读性：CTE提高了可读性。而不是将所有查询逻辑都集中到一个大型查询中，而是创建几个CTE，它们将在语句的后面组合。这使您可以获得所需的数据块，并将它们组合在最终的SELECT中。</p>
<p>○ 替代视图：您可以用CTE替换视图。如果您没有创建视图对象的权限，或者您不想创建一个视图对象，因为它仅在此一个查询中使用，这很方便。</p>
<p>○ 递归：使用CTE会创建递归查询，即可以调用自身的查询。当您需要处理组织结构图等分层数据时，这很方便。</p>
<p>○ 限制：克服SELECT语句限制，例如引用自身（递归）或使用非确定性函数执行GROUP BY。</p>
<p>○ 排名：每当你想使用排名函数，如ROW_NUMBER()，RANK()，NTILE()等。</p>
<h1 id="子查询派生表临时表"><a class="header" href="#子查询派生表临时表">子查询,派生表,临时表</a></h1>
<h2 id="子查询"><a class="header" href="#子查询">子查询</a></h2>
<p>子查询是嵌套在另一个查询(如select、insert、update和delete)中的查询。子查询又称为内部查询，而包含子查询的查询称为外部查询。 子查询可以在使用表达式的任何地方使用，并且必须在括号中关闭。</p>
<h1 id="派生表"><a class="header" href="#派生表">派生表</a></h1>
<p>派生表和子查询通常可以互换使用，但是与子查询不同的是，派生表必须具有别名</p>
<pre><code class="language-sql">
SELECT column_list  FROM
    ( SELECT column_list  FROM table_1) derived_table_name   --派生表
WHERE derived_table_name.c1 &gt; 0;
</code></pre>
<p><strong><code>派生表之间不可以相互引用。例如：</code>SELECT ... FROM (SELECT ... FROM ...) AS d1, (SELECT ... FROM d1 ...) AS d2，第一个查询标记为d1，在第二个查询语句中使用d1是不允许的。</strong></p>
<h2 id="临时表"><a class="header" href="#临时表">临时表</a></h2>
<p>临时表是一种特殊类型的表，它允许您存储一个临时结果集，可以在单个会话中多次重用。</p>
<ul>
<li>
<p>使用<code>CREATE TEMPORARY TABLE</code>语句创建临时表。请注意，在<code>CREATE</code>和<code>TABLE</code>关键字之间添加<code>TEMPORARY</code>关键字。</p>
</li>
<li>
<p>当会话结束或连接终止时，MySQL会自动删除临时表。当您不再使用临时表时，也可以使用<a href="http://www.yiibai.com/mysql/drop-table.html">DROP TABLE</a>语句来显式删除临时表。</p>
</li>
<li>
<p>一个临时表只能由创建它的客户机访问。不同的客户端可以创建具有相同名称的临时表，而不会导致错误，因为只有创建临时表的客户端才能看到它。 但是，在同一个会话中，两个临时表不能共享相同的名称。</p>
</li>
<li>
<p>临时表可以与数据库中的普通表具有相同的名称。 不推荐使用相同名称。例如，如果在<a href="http://www.yiibai.com/mysql/sample-database.html">示例数据库(yiibaidb)</a>中创建一个名为<code>employees</code>的临时表，则现有的<code>employees</code>表将变得无法访问。 对<code>employees</code>表发出的每个查询现在都是指<code>employees</code>临时表。 当删除<code>您</code>临时表时，永久<code>employees</code>表可以再次访问。</p>
</li>
</ul>
<pre><code class="language-SQL">
CREATE TEMPORARY TABLE table_name (
    name VARCHAR(10) NOT NULL,
    value INTEGER NOT NULL
  )
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mysql配置文件解析"><a class="header" href="#mysql配置文件解析">mysql配置文件解析</a></h1>
<p>查看版本 version()</p>
<h1 id="mysql语法规范"><a class="header" href="#mysql语法规范">mysql语法规范</a></h1>
<p>不区分大小写</p>
<p>建议 关键字大写 表明列名 小写</p>
<p>sql格式可以根据需要缩进 换行</p>
<h1 id="语法分类"><a class="header" href="#语法分类">语法分类</a></h1>
<p>DQL</p>
<p>DML</p>
<p>DDL</p>
<p>TCL</p>
<p>DCL</p>
<h1 id="函数-1"><a class="header" href="#函数-1">函数</a></h1>
<h2 id="单行函数"><a class="header" href="#单行函数">单行函数</a></h2>
<h3 id="字符函数"><a class="header" href="#字符函数">字符函数</a></h3>
<p>查找</p>
<p>替换</p>
<p>填充</p>
<p>去除</p>
<h3 id="数学函数"><a class="header" href="#数学函数">数学函数</a></h3>
<p>取整</p>
<p>取余</p>
<p>小数取值</p>
<h3 id="日期函数"><a class="header" href="#日期函数">日期函数</a></h3>
<p>时间:now()</p>
<p>日期:curdate()</p>
<p>时间不包含日期:curtime()</p>
<p>year()获取日期类型或者字符串类型的年</p>
<p>month()/monthname():获取月名</p>
<p>str_to_date(str,date) </p>
<p>date_format</p>
<p>datediff</p>
<h3 id="其他函数"><a class="header" href="#其他函数">其他函数</a></h3>
<p>version()</p>
<p>database()</p>
<p>user()</p>
<h3 id="流程控制函数"><a class="header" href="#流程控制函数">流程控制函数</a></h3>
<p><em>select  if(10&lt;5,'大','小')</em></p>
<p><em>case expr when value_1 then statement; else statement end</em></p>
<p><em>case when expr1 then statement; when expr2 then statement else statement end</em></p>
<h2 id="聚合函数"><a class="header" href="#聚合函数">聚合函数</a></h2>
<p><em>sum,avg,max,min,count</em></p>
<h1 id="连接查询"><a class="header" href="#连接查询">连接查询</a></h1>
<p>内连接</p>
<ul>
<li>等值连接</li>
<li>非等值连接</li>
<li>自连接</li>
</ul>
<p>外连接</p>
<ul>
<li>左外</li>
<li>右外</li>
<li>全外</li>
</ul>
<p>交叉连接</p>
<h1 id="子查询-1"><a class="header" href="#子查询-1">子查询</a></h1>
<h2 id="按结果集的行列数不同分类"><a class="header" href="#按结果集的行列数不同分类">按结果集的行列数不同分类</a></h2>
<p>标量子查询:结果集只有一行一列 </p>
<p>列子查询 结果集只有一列 多行, 搭配多行操作符使用 in,any/some, all</p>
<p>行子查询:结果集 一行多列</p>
<p>表子查询:多行多列</p>
<h2 id="按出现的位置"><a class="header" href="#按出现的位置">按出现的位置</a></h2>
<p>select后面,只支持标量子查询</p>
<p>from 后面:表子查询</p>
<p>where后面:标量子查询,列子查询,行子查询</p>
<p>having后面</p>
<p>exists后面:表子查询</p>
<h1 id="数据类型"><a class="header" href="#数据类型">数据类型</a></h1>
<p>数值类型</p>
<p><em>age int unsigned</em> 无符号int类型</p>
<p><em>t1 int(7) zerofill unisnged</em> 七位格式控制,不足七位0填充</p>
<p>枚举类型</p>
<p><em>c1 enum('a','b','c')</em></p>
<p><em>set</em> 类型</p>
<p>c1 set('a','b','c','d')</p>
<p>日期类型</p>
<p>date</p>
<p>time</p>
<p>datetime:</p>
<p>timestamp:不建议使用,2038年截止</p>
<table><thead><tr><th>日期和时间类型</th><th>字节数</th><th>最小值</th><th>最大值</th></tr></thead><tbody>
<tr><td>date</td><td>4</td><td>1000-01-01</td><td>9999-12-31</td></tr>
<tr><td>datetime</td><td>8</td><td>1000-01-01 00:00:00</td><td>9999-12-31 23:59:59</td></tr>
<tr><td>timestamp</td><td>4</td><td>19700101080001</td><td>2038年</td></tr>
<tr><td>time</td><td>3</td><td></td><td></td></tr>
<tr><td>year</td><td>1</td><td>1901</td><td>2155</td></tr>
</tbody></table>
<p>timestamp 取决于时区的影响</p>
<h1 id="约束"><a class="header" href="#约束">约束</a></h1>
<p>常见约束</p>
<ul>
<li>
<p>not null</p>
</li>
<li>
<p>default</p>
</li>
<li>
<p>primary key</p>
</li>
<li>
<p>unique 可以为空</p>
</li>
<li>
<p>check 约束检查</p>
</li>
<li>
<p>foreign key 外键约束</p>
</li>
<li>
<p>表级约束</p>
<p>constraint pk primary key(id)</p>
<p>constraint pk check(gender='1'or gender='0') </p>
<p>constraint fk_stuinfo_major foreign key(majorid) references major(id)</p>
<p>联合主键</p>
</li>
<li>
<p>约束管理</p>
<ul>
<li>alter table 表名 modify column 字段名 字段类型  新约束</li>
<li>alter table 表明 add [contraint 约束名] 约束类型(字段名) [外键引用]</li>
<li>alter table stuinfo drop primary key</li>
<li>列级约束 不支持起名字</li>
</ul>
</li>
</ul>
<h1 id="标识列"><a class="header" href="#标识列">标识列</a></h1>
<p>系统提供的默认的序列值</p>
<ul>
<li><em>auto_increment</em>:步长与初始值,可以在mysql环境变量查看</li>
<li>自增长列必须时唯一的</li>
<li>一个表只能有一个</li>
<li>只能是数值类型</li>
<li>可以通过 set auto_increment_increment  =3 设置步长</li>
<li>手动给定初始值设置 初始值</li>
</ul>
<h1 id="tcl"><a class="header" href="#tcl">TCL</a></h1>
<ul>
<li>
<p>开启事务</p>
<p><em>set autocommit = 0</em></p>
<p><em>start transaction</em></p>
<p><em>commit;</em></p>
<p><em>rollback</em></p>
</li>
<li>
<p>隔离级别</p>
<ul>
<li>脏读</li>
<li>不可重复读:针对更新,同一个事务的多次查询 数据不一致</li>
<li>幻读:针对插入,同一个事务中 准备更新数据时,其他事务提交了新的行</li>
</ul>
</li>
<li>
<p>mysql隔离级别设置</p>
<ul>
<li><em>select @@tx_isolation</em></li>
<li><em>set session transaction isolation level read uncommited</em></li>
<li>set names gbk</li>
</ul>
</li>
<li>
<p>mysql隔离级别</p>
<ul>
<li>read uncommited
<ul>
<li>可以读到别人没有提交的数据</li>
</ul>
</li>
<li><em>read commited</em>
<ul>
<li>读已提交</li>
<li>可避免脏读</li>
</ul>
</li>
<li><em>repeatable read</em>
<ul>
<li>可以重复读,但无法避免幻读</li>
</ul>
</li>
<li>serializable 串行化
<ul>
<li>一次只执行一个事务</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="视图"><a class="header" href="#视图">视图</a></h1>
<p>create view as select </p>
<h1 id="变量"><a class="header" href="#变量">变量</a></h1>
<h2 id="系统变量"><a class="header" href="#系统变量">系统变量</a></h2>
<h3 id="全局变量会话变量"><a class="header" href="#全局变量会话变量">全局变量/会话变量</a></h3>
<pre><code>查看所有某部分变量
show [session|global] variables  [like] &quot;variable_name&quot;
查看系统变量的某个值
select @@[global|session].系统变量 ;
系统变量赋值
set [global|session] 系统容变量名 = 值
set @@[global|session].系统变量名 = 值
global,session 不写的话 默认为 session

</code></pre>
<h2 id="自定义变量"><a class="header" href="#自定义变量">自定义变量</a></h2>
<h3 id="用户变量"><a class="header" href="#用户变量">用户变量</a></h3>
<p>用户自定义变量,作用于当前会话</p>
<pre><code>声明
set @用户变量名=值
set @用户变量名 := 值
select @用户变量名:= 值;
赋值
select 字段名 into @变量名 from 表
使用
select @变量名
</code></pre>
<h3 id="局部变量"><a class="header" href="#局部变量">局部变量</a></h3>
<p>在 <em>begin end</em> 块之内的 变量,必须在第一行</p>
<p>需要限定类型</p>
<pre><code>声明
declare 变量名 类型 default 值;
赋值
set 局部变量名 [:]= 值
select 局部变量名
</code></pre>
<h1 id="存储过程"><a class="header" href="#存储过程">存储过程</a></h1>
<p>一组编译好的sql语句集合</p>
<pre><code>create procedure 存储过程名(参数列表)
beign
	方法体
end
参数列表
模式 参数名 参数类型
in stuname varchar(20)
in|out|inout
</code></pre>
<ul>
<li>
<p>存储过程一句话 可以省略 <em>begin end</em></p>
</li>
<li>
<p>每条语句必须加分号 结束符, 可以通过 <em>delimter</em> 重新设置</p>
</li>
<li>
<p>调用: <em>call</em> 存储过程名(实参列表);</p>
</li>
<li>
<p>out模式的参数 可以当作 用户自定义变量一样访问</p>
</li>
<li>
<p><em>drop procedure p1</em></p>
</li>
<li>
<p><em>desc p1</em></p>
</li>
<li>
<p><em>show create procedure p2</em></p>
</li>
</ul>
<h1 id="函数-2"><a class="header" href="#函数-2">函数</a></h1>
<p>只能有一个返回</p>
<pre><code>create function 函数名 (参数列表) returns 返回类型
beign
	函数体
end

select 函数名(参数列表)

</code></pre>
<p>`` 区分 关键字与字段名表名</p>
<p>mysql中的 +  号  只起到 加法作用,不是数值型变量会试图转换 成数值型</p>
<p>对 null值 进行混合 运算时 也为null</p>
<p>ifnull(expr1,expr2) 判断是否为空</p>
<p><em>where last_name like '$_' escape '$'</em>  申明转义字符</p>
<p>count(*) myisam 引擎 count(*) 效率高,建议使用 <em>count(*)</em></p>
<p>order by 后面 支持 <em>select</em> 中引用的别名</p>
<p>not 关键字 可以 与 <em>between and</em> 配合</p>
<p>sql语言 索引都是从 1开始的</p>
<p><em>trim('aa' from 'str')</em> 去头尾的某些字符串</p>
<p>所有分组函数都忽略null值,且null元素不参与计数</p>
<p>a &gt; any(select ....) 表示 a &gt; 集合任何一个就可以, 即 a&gt;最小值即可</p>
<p>多表删除</p>
<p>delete  a1,a2 from table1 a1 join table2 a2 on 连接条件 where筛选条件</p>
<p>truncate会重置 表的 自增长序列, 而delete不会</p>
<p>create database if not exists books</p>
<p>alter database books character set gbk</p>
<p>create table 表名(</p>
<p>​	列名 列的类型(长度) 约束,</p>
<p>​	列名 列的类型(长度) 约束,</p>
<p>)</p>
<p>表的修改</p>
<p>列名</p>
<p><em>alter table book change column publishdata pubDate datetime</em></p>
<p>列的类型与约束</p>
<p><em>alter table book modify column pubdate</em> </p>
<p>添加列</p>
<p><em>add column</em></p>
<p>删除列</p>
<p><em>drop column</em></p>
<p>修改表名</p>
<p><em>rename to</em> </p>
<p>复制表结构 create table copy like author</p>
<p>复制表结构与数据  create table copy2 select * from author</p>
<h1 id="流程控制"><a class="header" href="#流程控制">流程控制</a></h1>
<h2 id="分支结构"><a class="header" href="#分支结构">分支结构</a></h2>
<p>case</p>
<pre><code>delimeter $
create procedure test_case(in score int)
begin
	case 
	when score &gt;= 90 and score &lt;=100 then select '1';
	when score &gt;= 80 then select '2';
	when score &gt;= 70 then selecet '3';
	else select '4';
	end case ;
end $
</code></pre>
<p>if</p>
<pre><code>if 条件1 then 语句1;
elseif 条件2 then 语句2;
else 语句;
end if;
</code></pre>
<p>循环结构</p>
<pre><code>while,loop,repeat
循环控制
iterate:类似continue
leave:break类似

标签:while 进入循环的条件 do
	循环体;
end while 标签;

标签:loop
	循环体;
end loop 标签

标签:repeat
	循环体;
until 结束循环的条件
end repeat 标签
</code></pre>
<p>179集</p>
<h1 id="mysql文件组织"><a class="header" href="#mysql文件组织">mysql文件组织</a></h1>
<table><thead><tr><th>文件名</th><th>作用</th></tr></thead><tbody>
<tr><td>log-bin</td><td>用于主从复制</td></tr>
<tr><td>log-error</td><td>用于记录mysql启停日志</td></tr>
<tr><td>log</td><td>查询日志,默认不开启,记录每条查询sql的日志</td></tr>
<tr><td>data目录</td><td>数据库数据存放的位置</td></tr>
<tr><td>frm文件</td><td>存放数据库定数据</td></tr>
<tr><td>myd文件</td><td>数据文件</td></tr>
<tr><td>myi</td><td>索引文件</td></tr>
</tbody></table>
<p>数据文件存放路径</p>
<p>配置文件目录:my.cnf</p>
<p>命令目录</p>
<p>启停脚本目录</p>
<h1 id="mysql逻辑架构"><a class="header" href="#mysql逻辑架构">mysql逻辑架构</a></h1>
<p><img src="https://i.loli.net/2020/06/07/Me6goXFNGzmYuhK.png" alt="逻辑架构图" /></p>
<p>连接接入层:<em>connection pool</em> </p>
<ul>
<li>负责认证, 线程重用,连接限制,内存检查,缓存</li>
</ul>
<p>sql 接口层</p>
<ul>
<li>DML DDL,存储过程, 视图,触发器</li>
</ul>
<p>查询sql解析器</p>
<p>优化器</p>
<p>缓存池</p>
<p>管理工具</p>
<ul>
<li>备份恢复</li>
<li>安全副本</li>
<li>集群</li>
<li>配置</li>
<li>迁移</li>
<li>元数据</li>
</ul>
<p>可插拔的存储引擎</p>
<ul>
<li>myisam</li>
<li>innoDB</li>
<li>Memory</li>
</ul>
<p>大致分为四层</p>
<p>第一层是 连接层, 解决谁可以连,怎么通信的问题</p>
<p>第二层是 sql处理层,包括解析,优化等,解决要查什么的问题</p>
<p>第三层是 存储存 , 解决 数据底层 是如何存</p>
<p>第四层是 管理层, 负责 备份,恢复,集群等</p>
<h1 id="存储引擎"><a class="header" href="#存储引擎">存储引擎</a></h1>
<p>查看</p>
<pre><code>show engine
show variables like '%storgeengine%'
</code></pre>
<table><thead><tr><th>对比项</th><th>MyISAM</th><th>InnoDB</th></tr></thead><tbody>
<tr><td>主外键</td><td>不支持</td><td>支持</td></tr>
<tr><td>事务</td><td>不支持</td><td>支持</td></tr>
<tr><td>行表锁</td><td>表锁</td><td>行锁</td></tr>
<tr><td>缓存</td><td>只缓存索引</td><td>可以缓存真实数据,对内存有要求</td></tr>
<tr><td>表空间</td><td>小</td><td>大</td></tr>
<tr><td>关注点</td><td>性能</td><td>事务</td></tr>
</tbody></table>
<p><em>XtraDB</em> 存储引擎</p>
<h1 id="mysql优化"><a class="header" href="#mysql优化">mysql优化</a></h1>
<p>sql性能下降的原因</p>
<ul>
<li>sql本身写得烂</li>
<li>索引失效</li>
<li>关联查询太多(设计缺陷)</li>
<li>服务器调优 不给力</li>
</ul>
<p>sql执行顺序</p>
<pre><code>from
join on
where
group by
having
select
distinct
order by
limit
</code></pre>
<p>七种连接理论</p>
<ul>
<li>join</li>
<li>left join</li>
<li>right join</li>
<li>fuller join</li>
<li>left join and b.key is null</li>
<li>right join  and a.key is null</li>
<li>fuller join  and a.key is null or b.key is null</li>
</ul>
<p>索引</p>
<ul>
<li>索引是一种数据结构,排好序的快速查找数据结构</li>
<li>在数据之外,数据库系统还维护者满足特定 查找算法的数据结构,这些数据结构以某种方式引用数据</li>
<li>索引的每个结点包含索引键值(即对应数据库索引字段),和指向对应数据记录的物理地址</li>
</ul>
<p>索引分类</p>
<ul>
<li>单值索引: 索引的key只有一个 字段</li>
<li>唯一索引</li>
<li>复合索引</li>
</ul>
<p>建立索引的建议</p>
<ul>
<li>主键自动建立索引</li>
<li>频繁条件查询字段作为索引</li>
<li>join的字段建立索引</li>
<li>复合索引 一般好于 单值索引</li>
<li>排序的字段 可以建立索引</li>
<li>分组的字段 也可以建立索引</li>
<li>字段分布均匀, 经常更新的字段 ,记录少的表 不需要建立索引</li>
</ul>
<h1 id="执行计划"><a class="header" href="#执行计划">执行计划</a></h1>
<p><a href="mysql/mysql%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92.html">执行计划</a></p>
<h1 id="索引优化案例"><a class="header" href="#索引优化案例">索引优化案例</a></h1>
<h2 id="单表查询优化"><a class="header" href="#单表查询优化">单表查询优化</a></h2>
<p><code>select * from table_a where a=1 and b&gt;2 group by c;</code></p>
<p>可以 建立 a,c 的联合索引,不能建立 a,b,c的联合索引</p>
<h2 id="两表连接"><a class="header" href="#两表连接">两表连接</a></h2>
<p>左连接 索引加右表, 因为左表无论如何都会有</p>
<p>右连接 索引加左表</p>
<h2 id="三表优化"><a class="header" href="#三表优化">三表优化</a></h2>
<p>在join字段 设置索引</p>
<h2 id="索引使用准则"><a class="header" href="#索引使用准则">索引使用准则</a></h2>
<ul>
<li>使用联合索引的过程中:最左前缀法则,且不能跳过索引的中间列</li>
<li>不要在索引列上做任何操作,包括显示转换,隐式转换</li>
<li>在联合索引中,使用范围之后的索引列全失效</li>
<li>按需取字段,尽量使用覆盖索引,可以避免索引失效的问题</li>
<li>避免判断空值</li>
<li>groupBy order by 后的字段一般要遵守 联合索引字段顺序,除非 其中的其中为空值</li>
<li>组合索引中,字段过滤性越好 越要 靠前</li>
</ul>
<h1 id="查询截取分析"><a class="header" href="#查询截取分析">查询截取分析</a></h1>
<ol>
<li>开启慢查询日志,设置阀值,超过多少s的是慢sql</li>
<li>explain 分析该sql</li>
<li>show profile</li>
<li>sql服务器的参数调优</li>
</ol>
<h1 id="关键字优化"><a class="header" href="#关键字优化">关键字优化</a></h1>
<h2 id="小表驱动大表"><a class="header" href="#小表驱动大表">小表驱动大表</a></h2>
<ul>
<li>
<p>in 与exists的分析</p>
<ul>
<li>
<p>可以把子查询过滤表等价于双层for循环,</p>
<pre><code class="language-mysql">select *from a where id in (select id from b)
等价于 
for (select id from b) 
	for select * from b where b.id= a.id

select * from a where exist (select 1 from b where exists b.id=a.id)
等价于
for(select * from a)
	for(select * from b where b.id=a.id
	)
</code></pre>
</li>
<li>
<p>总结: 当外层表数据&gt;内部表数据,使用 in,当外层表数据&lt;内部表数据  使用exists</p>
</li>
</ul>
</li>
</ul>
<h2 id="order-by关键字"><a class="header" href="#order-by关键字">ORDER BY关键字</a></h2>
<ul>
<li>排序的字段 尽量使用索引字段</li>
<li>不要使用select *</li>
<li>排序算法
<ul>
<li>单路排序	
<ul>
<li>后出, 取出所有数据,在缓冲区中排序,</li>
<li>如果数据超过 buffer缓冲区 ,会一部分一部分的排序,然后合并,产生多次IO</li>
</ul>
</li>
<li>双路排序
<ul>
<li>早先,取出排序字段排序,排完序后 在读 其余字段</li>
</ul>
</li>
<li>增大 sort_buffer_size 缓冲区</li>
</ul>
</li>
</ul>
<h1 id="慢查询日志"><a class="header" href="#慢查询日志">慢查询日志</a></h1>
<p><em>long_time_query</em> 默认10s</p>
<p><em>show variables like 'slow_query_log'</em></p>
<p><em>set global slow_query_log=1</em></p>
<p>需要重开<em>session</em></p>
<p>测试</p>
<p><em>select sleep(4)</em></p>
<p><em>mysqldumpslow</em> mysql慢查询日志分析工具</p>
<ul>
<li>s:何种方式排序
<ul>
<li>c:访问次数</li>
<li>l:锁定时间</li>
<li>r:返回记录</li>
<li>t:查询时间</li>
<li>al:平均锁定时间</li>
<li>ar:平均返回记录</li>
<li>at:平均返回时间</li>
</ul>
</li>
<li>t:返回记录的个数 </li>
<li>g:正则</li>
</ul>
<h1 id="showprofile-性能分析"><a class="header" href="#showprofile-性能分析"><em>showprofile</em> 性能分析</a></h1>
<ul>
<li>
<p>默认情况下关闭</p>
</li>
<li>
<p>保存最近15次运行的结果</p>
</li>
<li>
<p>命令</p>
<ul>
<li>show variables like 'profiling%'</li>
<li><em>show profiles</em> 查询历史sql</li>
<li>show profile cpu,block,io for query ${id},看到sql的生命周期</li>
<li>以下四个现象 是比较糟糕的
<ul>
<li>converting heap to myisam 查询结果太大,内存不够用了往磁盘上搬运</li>
<li>creating tmp table </li>
<li>copying to tmp table on disk 把内存中临时表 复制到磁盘</li>
<li>locked</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="全局查询日志"><a class="header" href="#全局查询日志">全局查询日志</a></h1>
<p>不要在生产环境启用这个选项</p>
<p>set global general_log =1</p>
<p>set global log_output ='TABLE'</p>
<p>所编写的sql语句,将会记录到mysql库里的general_log表中</p>
<h1 id="mysql锁机制"><a class="header" href="#mysql锁机制">Mysql锁机制</a></h1>
<h2 id="表级锁"><a class="header" href="#表级锁">表级锁</a></h2>
<h3 id="手动表锁"><a class="header" href="#手动表锁">手动表锁</a></h3>
<pre><code>lock table (table_name read|write[,])
show open tables

unlock talbes
其中想要更新该表的 会话会阻塞
某会话加读锁之后,只能先对该表进行读操作
myisam在查询时会自动给涉及的所有表加读锁,
在修改时会自动加写锁
</code></pre>
<h3 id="表锁定分析"><a class="header" href="#表锁定分析">表锁定分析</a></h3>
<ul>
<li><em>show 	status like 'table%'</em></li>
<li>记录mysql内部表级锁定情况
<ul>
<li><em>table_locks_immediate</em> 表示可以立即获取锁的次数</li>
<li><em>table_locks_waited</em> 表示 不能立即获取锁的次数</li>
</ul>
</li>
<li>isam存储引擎偏向 读写</li>
</ul>
<h2 id="行级锁"><a class="header" href="#行级锁">行级锁</a></h2>
<ul>
<li>
<p>行级锁升级案例</p>
<p>索引失效,导致表锁升级成表锁</p>
</li>
<li>
<p>间隙锁</p>
<p>在区间更新时,mysql会 把 某个范围内的所有记录加锁,即使这个记录不存在</p>
</li>
<li>
<p>手动加锁</p>
<p><em>select * from table_name where ... for update</em></p>
</li>
<li>
<p>行锁分析</p>
<pre><code>show inodb_row_lock%
innodb_row_lock_current_waits:当前正在等待锁定的数量
innodb_row_lock_time:从系统启动到现在总锁定时间
innodb_row_lock_time_avg:每次的等待平均时间
innodb_row_lock_time_max:从系统启动到现在等待最长的一次
innodb_row_lock_waits:系统启动总共等待的次数
</code></pre>
</li>
</ul>
<h1 id="主从复制"><a class="header" href="#主从复制">主从复制</a></h1>
<ul>
<li>
<p>slaver 会从 master读取 binlog数据 进行同步</p>
<ul>
<li>master改变记录到二进制日志</li>
<li>slaver将master 的binary log events 拷贝到中继日志</li>
<li>slaver重做 中继日志的 事件</li>
<li>mysql复制是异步串行化的</li>
</ul>
</li>
<li>
<p>主从复制的基本原则</p>
<ul>
<li>每个slaver只能有一个唯一的服务器ID</li>
<li>每个<em>master</em>可以有多个<em>slaver</em></li>
<li>复制的最大问题:延时</li>
</ul>
</li>
<li>
<p>等待实验</p>
</li>
</ul>
<p>\G 表示kv键值对显示</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="优化策略"><a class="header" href="#优化策略">优化策略</a></h1>
<ul>
<li>出现了 Using temporary；</li>
<li>rows 过多，或者几乎是全表的记录数；</li>
<li>filtered 太低</li>
<li>key 是 (NULL)；</li>
<li>possible_keys 出现过多（待选）索引。</li>
</ul>
<h1 id="explain"><a class="header" href="#explain">explain</a></h1>
<h2 id="id"><a class="header" href="#id"><strong>ID</strong></a></h2>
<blockquote>
<p>表示表的读取顺序</p>
</blockquote>
<ul>
<li>
<p>id 相同的情况下:执行顺序由上至下</p>
</li>
<li>
<p>id 不同的情况下: id 越大,越先被执行( 例如最里面的越先被执行)</p>
</li>
</ul>
<h2 id="select_type"><a class="header" href="#select_type">select_type</a></h2>
<blockquote>
<p>数据是以何种方式读取的</p>
</blockquote>
<p><strong>simple</strong></p>
<p>简单的子查询,不包含子查询或者<em>union</em></p>
<p><strong>primary</strong></p>
<p>查询中若包含多层子查询,最外层的查询为<em>primary</em>查询</p>
<p><strong>subquery</strong></p>
<p>子查询</p>
<p><strong>derived</strong></p>
<ul>
<li>子查询衍生的虚表的表名格式为 derived${id},</li>
<li>其中 id 为 explan 表 的 id,表示由这个 id 代表的某步骤产生的临时表</li>
<li>用于 from 子句里有子查询的情况。MySQL 会递归执行这些子查询，把结果放在临时表里</li>
</ul>
<p><strong>union</strong></p>
<ul>
<li>若第二个 select 出现在 union 之后,则被标记为 union</li>
<li>若 union 包含在 from 子句的 子查询中,则外层的 select 将被标记为 drived</li>
</ul>
<p><strong>union result</strong></p>
<ul>
<li>从 union 表获取结果的 select</li>
</ul>
<h2 id="table"><a class="header" href="#table">table</a></h2>
<blockquote>
<p>表名</p>
</blockquote>
<h2 id="type"><a class="header" href="#type">type</a></h2>
<blockquote>
<p>实际索引使用方式,性能升序增加</p>
</blockquote>
<p><strong>ALL</strong></p>
<blockquote>
<p>全表扫描</p>
</blockquote>
<p><strong>index</strong> （带索引的全表扫描）</p>
<p>这种连接类型只是另外一种形式的全表扫描，只不过它的扫描顺序是按照索引的顺序。这种扫描根据索引然后回表取数据，和 all 相比，他们都是取得了全表的数据，而且 index 要先读索引而且要回表随机取数据，<strong>因此 index 不可能会比 all 快</strong>（取同一个表数据），但为什么官方的手册将它的效率说的比 all 好，唯一可能的场景在于，按照索引扫描全表的数据是有序的。这样一来使用该索引排序 时 比 ALL 要好</p>
<pre><code class="language-m">mysql&gt; explain select * from employee order by `no` ;
+----+-------------+----------+------+---------------+------+---------+------+------+----------------+
| id | select_type | table    | type | possible_keys | key  | key_len | ref  | rows | Extra          |
+----+-------------+----------+------+---------------+------+---------+------+------+----------------+
|  1 | SIMPLE      | employee | ALL  | NULL          | NULL | NULL    | NULL |    5 | Using filesort |
+----+-------------+----------+------+---------------+------+---------+------+------+----------------+
mysql&gt; explain select * from employee order by rec_id ;
+----+-------------+----------+-------+---------------+---------+---------+------+------+-------+
| id | select_type | table    | type  | possible_keys | key     | key_len | ref  | rows | Extra |
+----+-------------+----------+-------+---------------+---------+---------+------+------+-------+
|  1 | SIMPLE      | employee | index | NULL          | PRIMARY | 4       | NULL |    5 | NULL  |
+----+-------------+----------+-------+---------------+---------+---------+------+------+-------+
</code></pre>
<p><strong>range</strong></p>
<ul>
<li>只检索给定范围的行,使用一个索引来选择行,索引开始于某一点,结束于某一点</li>
<li>例如 between in,&gt; &lt; , in, or 也是索引扫描</li>
</ul>
<p><strong>ref</strong></p>
<ul>
<li>非唯一性索引扫描,返回匹配某个单独值得所有行</li>
</ul>
<p><strong>eq_ref</strong></p>
<ul>
<li>唯一性索引扫描,对于每个索引键 只有一条记录与之匹配,常见于主键或唯一扫描</li>
</ul>
<p><strong>const</strong></p>
<ul>
<li>只有一条记录匹配,通常常见于 主键和唯一性索引</li>
</ul>
<p><strong>system</strong></p>
<p>表只有一行记录,等于系统表,是 const 类型的特例</p>
<pre><code>system&gt;const&gt;eq_ref&gt;ref&gt;fulltext&gt;ref_or_null&gt;index_merge&gt;unique_subquery&gt;index_subquery&gt;rang&gt;index&gt;all
保证达到 range级别 即可,最好能达到ref
</code></pre>
<h2 id="possiblekeys"><a class="header" href="#possiblekeys">possibleKeys</a></h2>
<blockquote>
<p>可能用到的索引,因为一张表索引只能使用一个</p>
</blockquote>
<h2 id="keys"><a class="header" href="#keys">keys</a></h2>
<blockquote>
<p>实际使用到的索引</p>
</blockquote>
<ul>
<li>
<p>覆盖索引</p>
<p>查询的字段 刚好建立了索引,且查询顺序一致</p>
</li>
</ul>
<h2 id="key_len"><a class="header" href="#key_len">key_len</a></h2>
<ul>
<li>表示索引中使用的字节数,可通过该计算查询中使用的索引的长度,长度越短越好</li>
<li>根据表定义计算得出得 索引字节数</li>
</ul>
<h2 id="ref"><a class="header" href="#ref">ref</a></h2>
<blockquote>
<p>显示该次查询 使用的索引的值 是 引用得哪个地方的, 一般有两种引用</p>
</blockquote>
<ul>
<li>const 引用的常量</li>
<li>test.t1.id 引用 某个库的某个表的某个字段 作为索引值的来源</li>
</ul>
<h2 id="rows"><a class="header" href="#rows">rows</a></h2>
<ul>
<li>找到记录大致要读取的行数</li>
</ul>
<h2 id="extra"><a class="header" href="#extra">extra</a></h2>
<blockquote>
<p>表示不适合在其他列中显示,但十分重要的额外信息</p>
</blockquote>
<ul>
<li>
<p><em>using filesort</em> 没有用到索引的排序,因为联合索引 字段顺序问题</p>
<pre><code>example
index(col1,col2,col3)
col1='ac' order by col2,col3
这种情况是会用到索引
如何建索引 , 就 如何按照索引走
</code></pre>
</li>
<li>
<p><em>using temporary</em>:使用了临时表,保存中间结果,常见于 order by,group by</p>
<ul>
<li>对于联合索引 请按照建立的顺序使用</li>
</ul>
</li>
<li>
<p><em>using index</em></p>
<ul>
<li>select 操作中使用了覆盖索引,效率不错</li>
<li>如果同时出现了<em>usingwhere</em> 表明索引被用来 执行索引键值的查找</li>
<li>如果没有出现 <em>using where</em> 表明索引用来读取数据而非执行查找</li>
</ul>
</li>
<li>
<p><em>using where</em></p>
<ul>
<li>使用<em>where</em>过滤</li>
</ul>
</li>
<li>
<p><em>using join buffer</em></p>
<ul>
<li>使用了连接缓存</li>
</ul>
</li>
<li>
<p><em>impossiable where</em></p>
<ul>
<li>不可能的 where 过滤条件</li>
</ul>
</li>
<li>
<p><em>select table optimized away</em></p>
<ul>
<li>在没有 groupby 子句的情况下,基于索引优化 MIN/MAX 操作</li>
<li>或 对于 MyISAM 存储引擎 优化 count(*) 操作,不必等到执行阶段计算</li>
</ul>
</li>
<li>
<p><em>distinct</em></p>
</li>
</ul>
<h1 id="using-temporaryusing-filesort"><a class="header" href="#using-temporaryusing-filesort">Using temporary/Using filesort</a></h1>
<h2 id="using-temporary"><a class="header" href="#using-temporary"><strong>Using temporary</strong></a></h2>
<blockquote>
<p>表示由于排序没有走索引、使用<code>union</code>、子查询连接查询、使用某些视图等原因（详见<a href="https://dev.mysql.com/doc/refman/5.6/en/internal-temporary-tables.html">internal-temporary-tables</a>），因此创建了一个内部临时表。</p>
<p>注意这里的临时表<strong>可能是内存上的临时表</strong>，也有可能是<strong>硬盘上的临时表</strong></p>
</blockquote>
<p><strong>内存临时表 or 硬盘临时表</strong></p>
<p>查看 sql 执行时使用的是内存临时表还是硬盘临时表，需要使用如下命令：</p>
<pre><code>mysql&gt; show global status like '%tmp%';
+-------------------------+-------+
| Variable_name           | Value |
+-------------------------+-------+
| Created_tmp_disk_tables | 0     |
| Created_tmp_files       | 5     |
| Created_tmp_tables      | 11    |
+-------------------------+-------+
3 rows in set
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.6/en/server-status-variables.html#statvar_Created_tmp_tables">Created_tmp_tables</a> 表示 mysql 创建的内部临时表的总数（包括内存临时表和硬盘临时表）；</p>
<p><a href="https://dev.mysql.com/doc/refman/5.6/en/server-status-variables.html#statvar_Created_tmp_disk_tables">Created_tmp_disk_tables</a> 表示 mysql 创建的硬盘临时表的总数。</p>
<p><strong>与临时表有关的参数</strong></p>
<p>当 mysql 需要创建临时表时，选择内存临时表还是硬盘临时表取决于参数<a href="https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_tmp_table_size">tmp_table_size</a>和<a href="https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_max_heap_table_size">max_heap_table_size</a>，</p>
<p>当<code>临时表的容量 &gt; Min(tmp_table_size ,max_heap_table_size )</code> , mysql 就会使用硬盘临时表存放数据。</p>
<p>用户可以在 mysql 的配置文件里修改该两个参数的值，两者的默认值均为 16M。</p>
<pre><code>tmp_table_size = 16M
max_heap_table_size = 16M
12
</code></pre>
<p>查看<code>tmp_table_size</code>和<code>max_heap_table_size</code>值：</p>
<pre><code class="language-sql">mysql&gt; show global variables like 'max_heap_table_size' or 'tmp_table_size';
</code></pre>
<h2 id="using-filesort"><a class="header" href="#using-filesort">Using filesort</a></h2>
<blockquote>
<p><code>Using filesort</code>仅仅表示没有使用索引的排序,<code>filesort</code>与文件无关。消除<code>Using filesort</code>的方法就是让查询 sql 的排序走索引</p>
</blockquote>
<p><strong>简介</strong></p>
<p><code>filesort</code>使用的算法是<code>QuickSort</code>，即对需要排序的记录生成元数据进行<strong>分块排序</strong>，然后再使用 mergesort 方法<strong>合并块</strong>。其中<code>filesort</code>可以使用的内存空间大小为参数<a href="https://dev.mysql.com/doc/refman/5.5/en/server-system-variables.html#sysvar_sort_buffer_size">sort_buffer_size</a>的值，默认为 2M。当排序记录太多<code>sort_buffer_size</code>不够用时，mysql 会<strong>使用临时文件来存放各个分块</strong>，然后各个分块排序后再多次合并分块最终全局完成排序。</p>
<pre><code>mysql&gt; show global variables like 'sort_buffer_size';
+------------------+--------+
| Variable_name    | Value  |
+------------------+--------+
| sort_buffer_size | 262144 |
+------------------+--------+
1 row in set
</code></pre>
<p><a href="https://dev.mysql.com/doc/refman/5.6/en/server-status-variables.html#statvar_Sort_merge_passes">Sort_merge_passes</a>表示<code>filesort</code>执行过的文件分块合并次数的总和，如果该值比较大，建议增大<code>sort_buffer_size</code>的值。</p>
<p><strong>使用算法</strong></p>
<p><code>filesort</code>使用的排序方法有两种：</p>
<p><strong>rowid 回表排序</strong></p>
<p>第一种方法是对需要排序的记录生成<code>&lt;sort_key,rowid&gt;</code>的元数据进行排序，该元数据仅包含排序字段和 rowid。排序完成后只有按字段排序的 rowid，因此还需要通过 rowid 进行回表操作获取所需要的列的值，可能会导致大量的随机 IO 读消耗；</p>
<p><strong>带元数据排序</strong></p>
<p>第二种方法是是对需要排序的记录生成<code>&lt;sort_key,additional_fields&gt;</code>的元数据，该元数据包含排序字段和需要返回的所有列。排序完后不需要回表，但是元数据要比第一种方法长得多，需要更多的空间用于排序。</p>
<p>参数<a href="https://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_max_length_for_sort_data">max_length_for_sort_data</a>字段用于控制<code>filesort</code>使用的排序方法，当所有需要排序记录的字段数量总和小于<code>max_length_for_sort_data</code>时使用第二种算法，否则会用第一种算法。该值的默认值为 1024</p>
<h1 id="小表驱动大表-1"><a class="header" href="#小表驱动大表-1">小表驱动大表</a></h1>
<h2 id="重要知识点"><a class="header" href="#重要知识点">重要知识点</a></h2>
<ul>
<li><strong>EXPLAIN 结果中，第一行出现的表就是驱动表</strong></li>
<li><strong>对驱动表可以直接排序</strong>，<strong>对非驱动表（的字段排序）需要对循环查询的合并结果（临时表）进行排序**</strong>（Important!）**</li>
<li><strong>永远用小结果集驱动大结果集(mysql 中)</strong></li>
</ul>
<h2 id="nested-loop-join"><a class="header" href="#nested-loop-join">Nested Loop Join</a></h2>
<p>以驱动表的结果集作为循环的基础数据，然后将结果集中的数据作为过滤条件一条条地到下一个表中查询数据，最后合并结果；此时还有第三个表，则将前两个表的 Join 结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此反复。</p>
<h2 id="驱动表的定义"><a class="header" href="#驱动表的定义">驱动表的定义</a></h2>
<p>进行多表连接查询时， <strong>[驱动表]</strong> 的定义为：
1）指定了联接条件时，<strong>满足查询条件的记录行数少</strong>的表为[驱动表]；</p>
<p>2）未指定联接条件时，<strong>行数少</strong>的表为[驱动表]</p>
<ol start="3">
<li>left join 一定程度上会 将 左表设置为 [驱动表] (除非 右表的查询足够小)</li>
</ol>
<h1 id="keylen-的计算"><a class="header" href="#keylen-的计算">KeyLen 的计算</a></h1>
<p>key_len 表示索引使用的字节数，根据这个值可以判断索引的使用情况,<strong>特别是在组合索引的时候</strong>,判断该索引有多少部分被使用到
在计算 key_len 时，下面是一些需要考虑的点:</p>
<ul>
<li>**索引字段的附加信息:**可以分为变长和定长数据类型讨论
<ul>
<li>当索引字段为定长数据类型时,如 char，int，datetime,需要有是否为空的标记,这个标记占用 1 个字节(对于 not null 的字段来说,则不需要这 1 字节);</li>
<li>对于变长数据类型,比如 varchar,除了是否为空的标记外,还需要有长度信息,需要占用两个字节。</li>
</ul>
</li>
<li>对于,char、varchar、blob、text 等字符集来说，key len 的长度还和字符集有关
<ul>
<li>latin1 一个字符占用 1 个字节</li>
<li>gbk 一个字符占用 2 个字节</li>
<li>utf8 一个字符占用 3 个字节。</li>
</ul>
</li>
</ul>
<p>综上，下面来看一些例子:</p>
<table><thead><tr><th>列类型</th><th>KEY_LEN</th><th>备注</th></tr></thead><tbody>
<tr><td>id int</td><td>key_len = 4+1</td><td>int 为 4bytes,允许为 NULL,加 1byte</td></tr>
<tr><td>id bigint not null</td><td>key_len=8</td><td>bigint 为 8bytes</td></tr>
<tr><td>user char(30) utf8</td><td>key_len=30*3+1</td><td>utf8 每个字符为 3bytes,允许为 NULL,加 1byte</td></tr>
<tr><td>user varchar(30) not null utf8</td><td>key_len=30*3+2</td><td>utf8 每个字符为 3bytes,变长数据类型,加 2bytes</td></tr>
<tr><td>user varchar(30) utf8</td><td>key_len=30*3+2+1</td><td>utf8 每个字符为 3bytes,允许为 NULL,加 1byte,变长数据类型,加 2bytes</td></tr>
<tr><td>detail text(10) utf8</td><td>key_len=30*3+2+1</td><td>TEXT 截取部分,被视为动态列类型。</td></tr>
</tbody></table>
<p>key_len 只指示了<strong>where 中用于条件过滤时被选中的索引列</strong>，是不包含 order by/group by 这一部分被选中的索引列的
例如,有个联合索引 idx(c1,c2,c3),3 列均是 int not null,那么下面的 SQL 执行计划中</p>
<pre><code class="language-sql">//key_len的值是8而不是12:
select ... from tb where c1=? and c2=? order by c1;
</code></pre>
<h1 id="indexmerge"><a class="header" href="#indexmerge">indexMerge</a></h1>
<blockquote>
<p><strong>对多个索引分别进行条件扫描，然后将它们各自的结果进行合并(intersect/union)</strong> 同一个表的多个索引的范围扫描可以对结果进行合并</p>
</blockquote>
<h2 id="示例"><a class="header" href="#示例"><strong>示例</strong></a></h2>
<pre><code class="language-sql">SELECT * FROM tbl_name WHERE key1 = 10 OR key2 = 20;
SELECT * FROM tbl_name WHERE (key1 = 10 OR key2 = 20) AND non_key=30;
SELECT * FROM t1, t2 WHERE (t1.key1 IN (1,2) OR t1.key2 LIKE 'value%') AND t2.key1=t1.some_col;
SELECT * FROM t1, t2 WHERE t1.key1=1 AND (t2.key1=t1.some_col OR t2.key2=t1.some_col2);
</code></pre>
<pre><code>1,SIMPLE,a,,index_merge,&quot;index_shift_results_worker_id,index_shift_results_org_code&quot;,&quot;index_shift_results_worker_id,index_shift_results_org_code&quot;,&quot;51,81&quot;,,61,100,&quot;Using union(index_shift_results_worker_id,index_shift_results_org_code); Using where
</code></pre>
<h2 id="using-intersectindex_1index_2"><a class="header" href="#using-intersectindex_1index_2">Using intersect(index_1,index_2...)</a></h2>
<pre><code>//取交集
SELECT * FROM tbl_name WHERE key1 = 10 and key2 = 20;
</code></pre>
<h2 id="using-unionindex_1index_2"><a class="header" href="#using-unionindex_1index_2"><strong>Using union(index_1,index_2)</strong></a></h2>
<pre><code>//取并集
SELECT * FROM tbl_name WHERE key1 = 10 or key2 = 20;
</code></pre>
<p>以及它们的组合(先内部 intersect 然后在外面 union)。</p>
<h2 id="using-sort_unionindex_1index_2"><a class="header" href="#using-sort_unionindex_1index_2">Using sort_union(index_1,index_2)</a></h2>
<pre><code class="language-sql">select id,worker_id,org_code from shift_results a where worker_id between '10197' and '102000' or org_code between  '100101100' and '100201100'
</code></pre>
<p>两个结果集进行并集 运算 需要排序去重</p>
<h1 id="ref_or_null"><a class="header" href="#ref_or_null">ref_or_null</a></h1>
<p>https://dev.mysql.com/doc/refman/8.0/en/is-null-optimization.html</p>
<pre><code class="language-sql">select id,worker_id,org_code from shift_results a where
worker_id = '10197' or worker_id is null
</code></pre>
<h1 id="mysql-子查询"><a class="header" href="#mysql-子查询">mysql 子查询</a></h1>
<h2 id="子查询定义"><a class="header" href="#子查询定义">子查询定义</a></h2>
<p><strong>SUBQUERY</strong></p>
<p>子查询中的第一个 SELECT</p>
<p><strong>DEPENDENT SUBQUERY</strong></p>
<p>子查询中的第一个 SELECT，<strong>取决于外面的查询</strong> 。</p>
<pre><code>换句话说，就是 子查询对 g2 的查询方式依赖于外层 g1 的查询。


第一步，MySQL 根据 select gid,count(id) from shop_goods where status=0 group by gid; 得到一个大结果集 t1，其数据量就是上图中的 rows=850672 了。

第二步，上面的大结果集 t1 中的每一条记录，都将与子查询 SQL 组成新的查询语句：select gid from shop_goods where sid in (15...blabla..29) and gid=%t1.gid%。等于说，子查询要执行85万次……即使这两步查询都用到了索引，但不慢才怪。

如此一来，子查询的执行效率居然受制于外层查询的记录数，那还不如拆成两个独立查询顺序执行呢。
</code></pre>
<p>**优化策略 **</p>
<p>子查询转临时表 做 join<strong>关联</strong></p>
<h1 id="where条件分析"><a class="header" href="#where条件分析">where条件分析</a></h1>
<p>所有SQL的where条件，均可归纳为3大类</p>
<p><em>Index Key (First Key &amp; Last Key)</em></p>
<p><em>Index Filter</em></p>
<p><em>Table Filter</em></p>
<p>对 where 中过滤条件的处理 ,根据索引使用情况分成了三种：index key, index filter, table filter</p>
<p><strong>index key</strong></p>
<p>用于确定SQL查询在索引中的连续范围(起始范围+结束范围)的查询条件，被称之为Index Key。由于一个范围，至少包含一个起始与一个终止，因此Index Key也被拆分为Index First Key和Index Last Key，分别用于定位索引查找的起始，以及索引查询的终止条件。也就是说根据索引来确定扫描的范围。</p>
<p><strong>index filter</strong></p>
<p>在使用 index key 确定了起始范围和介绍范围之后，在此范围之内，还有一些记录不符合where 条件，如果这些条件可以使用索引进行过滤，那么就是 index filter。也就是说用索引来进行where条件过滤。</p>
<p><strong>table filter</strong></p>
<p>where 中的条件不能使用索引进行处理的，只能访问table，进行条件过滤了。</p>
<h3 id="什么是icp"><a class="header" href="#什么是icp">什么是ICP？</a></h3>
<p>即所索引条件下推（index condition pushdown）</p>
<p>它能减少在使用 二级索引 过滤where条件时的回表次数 和 减少MySQL server层和引擎层的交互次数。在索引组织表中，使用二级索引进行回表的代价相比堆表中是要高一些的。</p>
<p>也就是说各种各样的 where 条件，在进行处理时，分成了上面三种情况，一种条件会使用索引确定扫描的范围；一种条件可以在索引中进行过滤；一种必须回表进行过滤；</p>
<p>在 MySQL5.6 之前，并不区分Index Filter与Table Filter，统统将Index First Key与Index Last Key范围内的索引记录，回表读取完整记录，然后返回给MySQL Server层进行过滤。</p>
<p>而在MySQL 5.6之后，Index Filter与Table Filter分离，Index Filter下降到InnoDB的索引层面进行过滤，减少了回表与返回MySQL Server层的记录交互开销，提高了SQL的执行效率。</p>
<p>所以所谓的 ICP 技术，其实就是 index filter 技术而已。只不过因为MySQL的架构原因，分成了server层和引擎层，才有所谓的“下推”的说法。所以ICP其实就是实现了index filter技术，将原来的在server层进行的table filter中可以进行index filter的部分，在引擎层面使用index filter进行处理，不再需要回表进行table filter。</p>
<p>using index</p>
<p>using index;using where</p>
<p>using where</p>
<p>using index condition</p>
<h2 id="示例-1"><a class="header" href="#示例-1">示例</a></h2>
<pre><code class="language-sql">// 索引`INDEX(zipcode, lastname, firstname)`

SELECT * FROM people
WHERE zipcode='95054'
AND lastname LIKE '%etrunia%'
AND address LIKE '%Main Street%';
</code></pre>
<p>MySQL可以使用索引去定位那些<code>zipcode='95054'</code>的信息，但是第二个条件<code>lastname LIKE '%etrunia%'</code>却没法用于减少必须要扫描的表行，所以如果没有ICP优化，在执行此查询时必须读取所有<code>zipcode='95054'</code>的表行数据。</p>
<p>如果启用了ICP优化，因为MySQL使用了索引<code>INDEX(zipcode, lastname, firstname)</code>，并且WHERE语句中的第二部分<code>lastname LIKE '%etrunia%'</code>仅仅使用了索引中的列<code>lastname</code>，所以在读取完整表行数据前可以基于此过滤那些索引中<code>lastname</code>不符合条件的索引数据，这样就能避免对那些满足<code>zipcode='95054'</code>但是不满足条件<code>lastname LIKE '%etrunia%'</code>表行数据的访问。</p>
<p>ICP可以通过系统变量<code>optimizer_switch</code>中的<code>index_condition_pushdown</code>进行启用和关闭：</p>
<pre><code class="language-sql">SET optimizer_switch = 'index_condition_pushdown=off';
SET optimizer_switch = 'index_condition_pushdown=on';
</code></pre>
<h1 id="物化表"><a class="header" href="#物化表">物化表</a></h1>
<blockquote>
<p>MySQL 引入了<code>Materialization</code>（物化）这一关键特性用于子查询（比如在 IN/NOT IN 子查询以及 FROM 子查询）优化。</p>
</blockquote>
<p><strong>具体方式是</strong></p>
<ul>
<li>
<p>在 SQL 执行过程中，第一次需要子查询结果时执行子查询并将子查询的结果保存为临时表 ，后续对子查询结果集的访问将直接通过临时表获得。</p>
</li>
<li>
<p>与此同时，优化器还具有延迟物化子查询的能力，先通过其它条件判断子查询是否真的需要执行。</p>
</li>
<li>
<p>物化子查询优化 SQL 执行的关键点在于对子查询只需要执行一次。 与之相对的执行方式是对外表的每一行都对子查询进行调用，其执行计划中的查询类型为“DEPENDENT SUBQUERY”。</p>
</li>
</ul>
<h1 id="查询优化开关"><a class="header" href="#查询优化开关">查询优化开关</a></h1>
<pre><code>show variables  like 'optimizer_switch'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody>
<tr><td>IF(expr1,expr2,expr3)</td><td>expr1为true 则返回 expr2 否则expr3</td></tr>
<tr><td>IFNULL(expr1,expr2)</td><td>判断第一个表达式是否为 NULL，如果为 NULL 则返回第二个参数的值，如果不为 NULL 则返回第一个参数的值。</td></tr>
<tr><td>NULLIF(expr1,expr2)</td><td>如果两个参数相等则返回NULL，否则返回第一个参数的值expr1</td></tr>
<tr><td>coalesce(expr1,expr2....)</td><td>返回第一个不为null的值</td></tr>
<tr><td>ISNULL(expr)</td><td>判断是否为null</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mysql-server-启停脚本"><a class="header" href="#mysql-server-启停脚本">mysql server 启停脚本</a></h1>
<h2 id="mysqld"><a class="header" href="#mysqld"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld.html"><strong>mysqld</strong></a></a></h2>
<p>服务程序,主要进程</p>
<h2 id="mysqld_safe"><a class="header" href="#mysqld_safe"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html"><strong>mysqld_safe</strong></a></a></h2>
<p>启动脚本</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html"><strong>mysqld_safe</strong></a> attempts to start <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld.html"><strong>mysqld</strong></a></p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html">Section 4.3.2, “<strong>mysqld_safe</strong> — MySQL Server Startup Script”</a>.</p>
<h2 id="mysqlserver"><a class="header" href="#mysqlserver"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-server.html"><strong>mysql.server</strong></a></a></h2>
<p>启动脚本</p>
<p>使用在 system-V 风格的  通过目录脚本划分 系统特定服务等级</p>
<p>它调用了 <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html"><strong>mysqld_safe</strong></a> to start the MySQL server. </p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-server.html">Section 4.3.3, “<strong>mysql.server</strong> — MySQL Server Startup Script”</a>.</p>
<h2 id="mysqld_multi"><a class="header" href="#mysqld_multi"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html"><strong>mysqld_multi</strong></a></a></h2>
<p>多服务启停</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html">Section 4.3.4, “<strong>mysqld_multi</strong> — Manage Multiple MySQL Servers”</a>.</p>
<h1 id="安装升级脚本"><a class="header" href="#安装升级脚本">安装升级脚本</a></h1>
<h2 id="comp_err"><a class="header" href="#comp_err"><a href="https://dev.mysql.com/doc/refman/5.7/en/comp-err.html"><strong>comp_err</strong></a></a></h2>
<p>在编译构建过程中, 从错误文件中 解析出错误消息</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/comp-err.html">Section 4.4.1, “<strong>comp_err</strong> — Compile MySQL Error Message File”</a>.</p>
<h2 id="mysql_install_db"><a class="header" href="#mysql_install_db"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-install-db.html"><strong>mysql_install_db</strong></a></a></h2>
<p>初始化 mysql data目录</p>
<p>创建mysql数据库</p>
<p>初始化默认表权限</p>
<p>初始化innodb 表空间</p>
<p>when first installing MySQL on a system. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-install-db.html">Section 4.4.2, “<strong>mysql_install_db</strong> — Initialize MySQL Data Directory”</a>, and <a href="https://dev.mysql.com/doc/refman/5.7/en/postinstallation.html">Section 2.10, “Postinstallation Setup and Testing”</a>.</p>
<h2 id="mysql_plugin"><a class="header" href="#mysql_plugin"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-plugin.html"><strong>mysql_plugin</strong></a></a></h2>
<p>This program configures MySQL server plugins. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-plugin.html">Section 4.4.3, “<strong>mysql_plugin</strong> — Configure MySQL Server Plugins”</a>.</p>
<h2 id="mysql_secure_installation"><a class="header" href="#mysql_secure_installation"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-secure-installation.html"><strong>mysql_secure_installation</strong></a></a></h2>
<p>This program enables you to improve the security of your MySQL installation. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-secure-installation.html">Section 4.4.4, “<strong>mysql_secure_installation</strong> — Improve MySQL Installation Security”</a>.</p>
<h2 id="mysql_ssl_rsa_setup"><a class="header" href="#mysql_ssl_rsa_setup"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-rsa-setup.html"><strong>mysql_ssl_rsa_setup</strong></a></a></h2>
<p>This program creates the SSL certificate and key files and RSA key-pair files required to support secure connections, if those files are missing. Files created by <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-rsa-setup.html"><strong>mysql_ssl_rsa_setup</strong></a> can be used for secure connections using SSL or RSA. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-rsa-setup.html">Section 4.4.5, “<strong>mysql_ssl_rsa_setup</strong> — Create SSL/RSA Files”</a>.</p>
<h2 id="mysql_tzinfo_to_sql"><a class="header" href="#mysql_tzinfo_to_sql"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-tzinfo-to-sql.html"><strong>mysql_tzinfo_to_sql</strong></a></a></h2>
<p>This program loads the time zone tables in the <code>mysql</code> database using the contents of the host system zoneinfo database (the set of files describing time zones). See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-tzinfo-to-sql.html">Section 4.4.6, “<strong>mysql_tzinfo_to_sql</strong> — Load the Time Zone Tables”</a></p>
<h2 id="mysql_upgrade"><a class="header" href="#mysql_upgrade"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-upgrade.html"><strong>mysql_upgrade</strong></a></a></h2>
<p>This program is used after a MySQL upgrade operation. It updates the grant tables with any changes that have been made in newer versions of MySQL, and checks tables for incompatibilities and repairs them if necessary. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-upgrade.html">Section 4.4.7, “<strong>mysql_upgrade</strong> — Check and Upgrade MySQL Tables”</a>.</p>
<h1 id="mysql客户端程序"><a class="header" href="#mysql客户端程序">mysql客户端程序</a></h1>
<h2 id="mysql"><a class="header" href="#mysql"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html"><strong>mysql</strong></a></a></h2>
<p>mysql命令行工具,从文件中执行批量sql</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html">Section 4.5.1, “<strong>mysql</strong> — The MySQL Command-Line Client”</a>.</p>
<h2 id="mysqladmin"><a class="header" href="#mysqladmin"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqladmin.html"><strong>mysqladmin</strong></a></a></h2>
<p>执行管理员操作</p>
<p>数据库创建</p>
<p>重新加载授权表</p>
<p>重开日志文件</p>
<p>查询版本,进程,状态信息,</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqladmin.html">Section 4.5.2, “<strong>mysqladmin</strong> — A MySQL Server Administration Program”</a>.</p>
<h2 id="mysqlcheck"><a class="header" href="#mysqlcheck"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlcheck.html"><strong>mysqlcheck</strong></a></a></h2>
<p>检查维护分析 优化表,</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlcheck.html">Section 4.5.3, “<strong>mysqlcheck</strong> — A Table Maintenance Program”</a>.</p>
<h2 id="mysqldump"><a class="header" href="#mysqldump"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html"><strong>mysqldump</strong></a></a></h2>
<p>导出数据库为文件</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html">Section 4.5.4, “<strong>mysqldump</strong> — A Database Backup Program”</a>.</p>
<h2 id="mysqlimport"><a class="header" href="#mysqlimport"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlimport.html"><strong>mysqlimport</strong></a></a></h2>
<p>导入程序</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlimport.html">Section 4.5.5, “<strong>mysqlimport</strong> — A Data Import Program”</a>.</p>
<h2 id="mysqlpump"><a class="header" href="#mysqlpump"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html"><strong>mysqlpump</strong></a></a></h2>
<p>A client that dumps a MySQL database into a file as SQL. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html">Section 4.5.6, “<strong>mysqlpump</strong> — A Database Backup Program”</a>.</p>
<h2 id="mysqlsh"><a class="header" href="#mysqlsh"><strong>mysqlsh</strong></a></h2>
<p>MySQL Shell is an advanced client and code editor for MySQL Server. See <a href="https://dev.mysql.com/doc/mysql-shell/8.0/en/">MySQL Shell 8.0 (part of MySQL 8.0)</a>. In addition to the provided SQL functionality, similar to <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html"><strong>mysql</strong></a>,</p>
<p>see <a href="https://dev.mysql.com/doc/refman/5.7/en/document-store.html">Chapter 19, <em>Using MySQL as a Document Store</em></a>.</p>
<p>see <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-innodb-cluster-userguide.html">Chapter 20, <em>InnoDB Cluster</em></a>.</p>
<h2 id="mysqlshow"><a class="header" href="#mysqlshow"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlshow.html"><strong>mysqlshow</strong></a></a></h2>
<p>显示数据库信息 </p>
<p>数据库</p>
<p>表</p>
<p>列</p>
<p>索引</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlshow.html">Section 4.5.7, “<strong>mysqlshow</strong> — Display Database, Table, and Column Information”</a>.</p>
<h2 id="mysqlslap"><a class="header" href="#mysqlslap"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlslap.html"><strong>mysqlslap</strong></a></a></h2>
<p>负载模拟客户端</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlslap.html">Section 4.5.8, “<strong>mysqlslap</strong> — A Load Emulation Client”</a>.</p>
<h1 id="管理工具"><a class="header" href="#管理工具">管理工具</a></h1>
<h2 id="innochecksum"><a class="header" href="#innochecksum"><a href="https://dev.mysql.com/doc/refman/5.7/en/innochecksum.html"><strong>innochecksum</strong></a></a></h2>
<p>离线Innodb 文件校验和工具</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/innochecksum.html">Section 4.6.1, “<strong>innochecksum</strong> — Offline InnoDB File Checksum Utility”</a>.</p>
<h2 id="myisam_ftdump"><a class="header" href="#myisam_ftdump"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisam-ftdump.html"><strong>myisam_ftdump</strong></a></a></h2>
<p>查看 myisam 全文索引 信息的工具</p>
<h2 id="myisamchk"><a class="header" href="#myisamchk"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisamchk.html"><strong>myisamchk</strong></a></a></h2>
<p>描述,检查,优化 修复MyISAM 表</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/myisamchk.html">Section 4.6.3, “<strong>myisamchk</strong> — MyISAM Table-Maintenance Utility”</a>.</p>
<h2 id="myisamlog"><a class="header" href="#myisamlog"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisamlog.html"><strong>myisamlog</strong></a></a></h2>
<p>处理myisam 日志文件内容</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/myisamlog.html">Section 4.6.4, “<strong>myisamlog</strong> — Display MyISAM Log File Contents”</a>.</p>
<h2 id="myisampack"><a class="header" href="#myisampack"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisampack.html"><strong>myisampack</strong></a></a></h2>
<p>A utility that compresses <code>MyISAM</code> tables to produce smaller read-only tables. See <a href="https://dev.mysql.com/doc/refman/5.7/en/myisampack.html">Section 4.6.5, “<strong>myisampack</strong> — Generate Compressed, Read-Only MyISAM Tables”</a>.</p>
<h2 id="mysql_config_editor"><a class="header" href="#mysql_config_editor"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html"><strong>mysql_config_editor</strong></a></a></h2>
<p>A utility that enables you to store authentication credentials in a secure, encrypted login path </p>
<p>file named <code>.mylogin.cnf</code>. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html">Section 4.6.6, “<strong>mysql_config_editor</strong> — MySQL Configuration Utility”</a>.</p>
<h2 id="mysqlbinlog"><a class="header" href="#mysqlbinlog"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlbinlog.html"><strong>mysqlbinlog</strong></a></a></h2>
<p>A utility for reading statements from a binary log. The log of executed statements contained in the binary log files can be used to help recover from a crash. </p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlbinlog.html">Section 4.6.7, “<strong>mysqlbinlog</strong> — Utility for Processing Binary Log Files”</a>.</p>
<h2 id="mysqldumpslow"><a class="header" href="#mysqldumpslow"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldumpslow.html"><strong>mysqldumpslow</strong></a></a></h2>
<p>慢sql日志</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldumpslow.html">Section 4.6.8, “<strong>mysqldumpslow</strong> — Summarize Slow Query Log Files”</a>.</p>
<p>mysql客户端 服务器通信时 使用如下环境变量</p>
<table><thead><tr><th style="text-align: left">Environment Variable</th><th style="text-align: left">Meaning</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>MYSQL_UNIX_PORT</code></td><td style="text-align: left">The default Unix socket file; used for connections to <code>localhost</code></td></tr>
<tr><td style="text-align: left"><code>MYSQL_TCP_PORT</code></td><td style="text-align: left">The default port number; used for TCP/IP connections</td></tr>
<tr><td style="text-align: left"><code>MYSQL_PWD</code></td><td style="text-align: left">The default password</td></tr>
<tr><td style="text-align: left"><code>MYSQL_DEBUG</code></td><td style="text-align: left">Debug trace options when debugging</td></tr>
<tr><td style="text-align: left"><code>TMPDIR</code></td><td style="text-align: left">The directory where temporary tables and files are created</td></tr>
</tbody></table>
<p>For a full list of environment variables used by MySQL programs, see <a href="https://dev.mysql.com/doc/refman/5.7/en/environment-variables.html">Section 4.9, “Environment Variables”</a>.</p>
<p>Use of <code>MYSQL_PWD</code> is insecure. See <a href="https://dev.mysql.com/doc/refman/5.7/en/password-security-user.html">Section 6.1.2.1, “End-User Guidelines for Password Security”</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="连接"><a class="header" href="#连接">连接</a></h1>
<pre><code class="language-shell">mysql -h host -u user -p
mysql -u user -p
</code></pre>
<h1 id="查询输入"><a class="header" href="#查询输入">查询输入</a></h1>
<table><thead><tr><th style="text-align: left">Prompt</th><th style="text-align: left">Meaning</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>mysql&gt;</code></td><td style="text-align: left">Ready for new query</td></tr>
<tr><td style="text-align: left"><code>-&gt;</code></td><td style="text-align: left">Waiting for next line of multiple-line query</td></tr>
<tr><td style="text-align: left"><code>'&gt;</code></td><td style="text-align: left">Waiting for next line, waiting for completion of a string that began with a single quote (<code>'</code>)</td></tr>
<tr><td style="text-align: left"><code>&quot;&gt;</code></td><td style="text-align: left">Waiting for next line, waiting for completion of a string that began with a double quote (<code>&quot;</code>)</td></tr>
<tr><td style="text-align: left">``&gt;`</td><td style="text-align: left">Waiting for next line, waiting for completion of an identifier that began with a backtick (```)</td></tr>
<tr><td style="text-align: left"><code>/*&gt;</code></td><td style="text-align: left">Waiting for next line, waiting for completion of a comment that began with <code>/*</code></td></tr>
</tbody></table>
<h1 id="数据库查看操作"><a class="header" href="#数据库查看操作">数据库查看操作</a></h1>
<pre><code class="language-sql">SHOW DATABASES;
USE test
show tables
LOAD DATA LOCAL INFILE '/path/pet.txt' INTO TABLE pet;
LOAD DATA LOCAL INFILE '/path/pet.txt' INTO TABLE pet
       LINES TERMINATED BY '\r\n';
</code></pre>
<h1 id="杂项"><a class="header" href="#杂项">杂项</a></h1>
<p><strong>赋权</strong></p>
<pre><code>GRANT ALL ON menagerie.* TO 'your_mysql_name'@'your_client_host';
</code></pre>
<p><strong>空值处理</strong></p>
<pre><code>IS NULL and IS NOT NULL
任何与空值作比较 都为null
0或者null为 false
null在分组中会被当做一个组
</code></pre>
<p><strong>模式匹配</strong></p>
<p>使用  <a href="https://dev.mysql.com/doc/refman/5.7/en/regexp.html#operator_regexp"><code>REGEXP</code></a> and <a href="https://dev.mysql.com/doc/refman/5.7/en/regexp.html#operator_not-regexp"><code>NOT REGEXP</code></a>  操作符, 模式匹配</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/regexp.html#operator_regexp"><code>RLIKE</code></a> and <a href="https://dev.mysql.com/doc/refman/5.7/en/regexp.html#operator_not-regexp"><code>NOT RLIKE</code></a> 同义词</p>
<p><strong>匹配规则</strong></p>
<ul>
<li><code>.</code>  匹配单字符</li>
</ul>
<ul>
<li><code>[]</code>  匹配任意字符</li>
</ul>
<ul>
<li><code>*</code> 匹配0个或者多个字符</li>
<li>正则匹配只要部分匹配成功就 满足, 而 like是 整体匹配</li>
<li>匹配整个字符 串 使用 <code>^$</code></li>
<li>大小写敏感</li>
</ul>
<pre><code class="language-sql">SELECT * FROM pet WHERE name REGEXP BINARY '^b';
</code></pre>
<p><code>SELECT DATABASE();</code></p>
<p><strong>mysql 批处理模式</strong></p>
<pre><code>mysql &lt; batch-file
mysql -e &quot;source batch-file&quot;
mysql -h host -u user -p &lt; batch-file
mysql &lt; batch-file | more
mysql &lt; batch-file &gt; mysql.out
source filename;
</code></pre>
<p><strong>使用自定义变量</strong></p>
<pre><code>mysql&gt; SELECT @min_price:=MIN(price),@max_price:=MAX(price) FROM shop;
mysql&gt; SELECT * FROM shop WHERE price=@min_price OR price=@max_price;
+---------+--------+-------+
| article | dealer | price |
+---------+--------+-------+
|    0003 | D      |  1.25 |
|    0004 | D      | 19.95 |
+---------+--------+-------+
</code></pre>
<p><strong>外键的使用</strong></p>
<p>mysql innnodb 引擎  将外键 当作一种注释,</p>
<p>不会进行 外键约束检查</p>
<p>不会级联删除</p>
<p>不会索引</p>
<p><strong>多键值搜索</strong></p>
<p>使用union all 代替</p>
<pre><code class="language-sql">SELECT field1_index, field2_index FROM test_table
WHERE field1_index = '1' OR  field2_index = '1'


SELECT field1_index, field2_index
    FROM test_table WHERE field1_index = '1'
UNION
SELECT field1_index, field2_index
    FROM test_table WHERE field2_index = '1';
</code></pre>
<p><strong>BIT_COUNT/BIT_OR</strong></p>
<pre><code>CREATE TABLE t1 (year YEAR, month INT UNSIGNED,
             day INT UNSIGNED);
INSERT INTO t1 VALUES(2000,1,1),(2000,1,20),(2000,1,30),(2000,2,2),
            (2000,2,23),(2000,2,23);
            
            
SELECT year,month,BIT_COUNT(BIT_OR(1&lt;&lt;day)) AS days FROM t1
       GROUP BY year,month;

</code></pre>
<p><strong>AUTO_INCREMENT</strong></p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/sql-mode.html#sqlmode_no_auto_value_on_zero"><code>NO_AUTO_VALUE_ON_ZERO</code></a> </p>
<p>如果已启用</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mysql-server-启停脚本-1"><a class="header" href="#mysql-server-启停脚本-1">mysql server 启停脚本</a></h1>
<h2 id="mysqld-1"><a class="header" href="#mysqld-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld.html"><strong>mysqld</strong></a></a></h2>
<p>服务程序,主要进程</p>
<h2 id="mysqld_safe-1"><a class="header" href="#mysqld_safe-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html"><strong>mysqld_safe</strong></a></a></h2>
<p>启动脚本</p>
<p><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html"><strong>mysqld_safe</strong></a> attempts to start <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld.html"><strong>mysqld</strong></a></p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html">Section 4.3.2, “<strong>mysqld_safe</strong> — MySQL Server Startup Script”</a>.</p>
<h2 id="mysqlserver-1"><a class="header" href="#mysqlserver-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-server.html"><strong>mysql.server</strong></a></a></h2>
<p>启动脚本</p>
<p>使用在 system-V 风格的 通过目录脚本划分 系统特定服务等级</p>
<p>它调用了 <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-safe.html"><strong>mysqld_safe</strong></a> to start the MySQL server.</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-server.html">Section 4.3.3, “<strong>mysql.server</strong> — MySQL Server Startup Script”</a>.</p>
<h2 id="mysqld_multi-1"><a class="header" href="#mysqld_multi-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html"><strong>mysqld_multi</strong></a></a></h2>
<p>多服务启停</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld-multi.html">Section 4.3.4, “<strong>mysqld_multi</strong> — Manage Multiple MySQL Servers”</a>.</p>
<h1 id="安装升级脚本-1"><a class="header" href="#安装升级脚本-1">安装升级脚本</a></h1>
<h2 id="comp_err-1"><a class="header" href="#comp_err-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/comp-err.html"><strong>comp_err</strong></a></a></h2>
<p>在编译构建过程中, 从错误文件中 解析出错误消息</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/comp-err.html">Section 4.4.1, “<strong>comp_err</strong> — Compile MySQL Error Message File”</a>.</p>
<h2 id="mysql_install_db-1"><a class="header" href="#mysql_install_db-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-install-db.html"><strong>mysql_install_db</strong></a></a></h2>
<p>初始化 mysql data 目录</p>
<p>创建 mysql 数据库</p>
<p>初始化默认表权限</p>
<p>初始化 innodb 表空间</p>
<p>when first installing MySQL on a system. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-install-db.html">Section 4.4.2, “<strong>mysql_install_db</strong> — Initialize MySQL Data Directory”</a>, and <a href="https://dev.mysql.com/doc/refman/5.7/en/postinstallation.html">Section 2.10, “Postinstallation Setup and Testing”</a>.</p>
<h2 id="mysql_plugin-1"><a class="header" href="#mysql_plugin-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-plugin.html"><strong>mysql_plugin</strong></a></a></h2>
<p>This program configures MySQL server plugins. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-plugin.html">Section 4.4.3, “<strong>mysql_plugin</strong> — Configure MySQL Server Plugins”</a>.</p>
<h2 id="mysql_secure_installation-1"><a class="header" href="#mysql_secure_installation-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-secure-installation.html"><strong>mysql_secure_installation</strong></a></a></h2>
<p>This program enables you to improve the security of your MySQL installation. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-secure-installation.html">Section 4.4.4, “<strong>mysql_secure_installation</strong> — Improve MySQL Installation Security”</a>.</p>
<h2 id="mysql_ssl_rsa_setup-1"><a class="header" href="#mysql_ssl_rsa_setup-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-rsa-setup.html"><strong>mysql_ssl_rsa_setup</strong></a></a></h2>
<p>This program creates the SSL certificate and key files and RSA key-pair files required to support secure connections, if those files are missing. Files created by <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-rsa-setup.html"><strong>mysql_ssl_rsa_setup</strong></a> can be used for secure connections using SSL or RSA. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-ssl-rsa-setup.html">Section 4.4.5, “<strong>mysql_ssl_rsa_setup</strong> — Create SSL/RSA Files”</a>.</p>
<h2 id="mysql_tzinfo_to_sql-1"><a class="header" href="#mysql_tzinfo_to_sql-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-tzinfo-to-sql.html"><strong>mysql_tzinfo_to_sql</strong></a></a></h2>
<p>This program loads the time zone tables in the <code>mysql</code> database using the contents of the host system zoneinfo database (the set of files describing time zones). See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-tzinfo-to-sql.html">Section 4.4.6, “<strong>mysql_tzinfo_to_sql</strong> — Load the Time Zone Tables”</a></p>
<h2 id="mysql_upgrade-1"><a class="header" href="#mysql_upgrade-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-upgrade.html"><strong>mysql_upgrade</strong></a></a></h2>
<p>This program is used after a MySQL upgrade operation. It updates the grant tables with any changes that have been made in newer versions of MySQL, and checks tables for incompatibilities and repairs them if necessary. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-upgrade.html">Section 4.4.7, “<strong>mysql_upgrade</strong> — Check and Upgrade MySQL Tables”</a>.</p>
<h1 id="mysql-客户端程序"><a class="header" href="#mysql-客户端程序">mysql 客户端程序</a></h1>
<h2 id="mysql-1"><a class="header" href="#mysql-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html"><strong>mysql</strong></a></a></h2>
<p>mysql 命令行工具,从文件中执行批量 sql</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html">Section 4.5.1, “<strong>mysql</strong> — The MySQL Command-Line Client”</a>.</p>
<h2 id="mysqladmin-1"><a class="header" href="#mysqladmin-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqladmin.html"><strong>mysqladmin</strong></a></a></h2>
<p>执行管理员操作</p>
<p>数据库创建</p>
<p>重新加载授权表</p>
<p>重开日志文件</p>
<p>查询版本,进程,状态信息,</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqladmin.html">Section 4.5.2, “<strong>mysqladmin</strong> — A MySQL Server Administration Program”</a>.</p>
<h2 id="mysqlcheck-1"><a class="header" href="#mysqlcheck-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlcheck.html"><strong>mysqlcheck</strong></a></a></h2>
<p>检查维护分析 优化表,</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlcheck.html">Section 4.5.3, “<strong>mysqlcheck</strong> — A Table Maintenance Program”</a>.</p>
<h2 id="mysqldump-1"><a class="header" href="#mysqldump-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html"><strong>mysqldump</strong></a></a></h2>
<p>导出数据库为文件</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html">Section 4.5.4, “<strong>mysqldump</strong> — A Database Backup Program”</a>.</p>
<h2 id="mysqlimport-1"><a class="header" href="#mysqlimport-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlimport.html"><strong>mysqlimport</strong></a></a></h2>
<p>导入程序</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlimport.html">Section 4.5.5, “<strong>mysqlimport</strong> — A Data Import Program”</a>.</p>
<h2 id="mysqlpump-1"><a class="header" href="#mysqlpump-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html"><strong>mysqlpump</strong></a></a></h2>
<p>A client that dumps a MySQL database into a file as SQL. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlpump.html">Section 4.5.6, “<strong>mysqlpump</strong> — A Database Backup Program”</a>.</p>
<h2 id="mysqlsh-1"><a class="header" href="#mysqlsh-1"><strong>mysqlsh</strong></a></h2>
<p>MySQL Shell is an advanced client and code editor for MySQL Server. See <a href="https://dev.mysql.com/doc/mysql-shell/8.0/en/">MySQL Shell 8.0 (part of MySQL 8.0)</a>. In addition to the provided SQL functionality, similar to <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html"><strong>mysql</strong></a>,</p>
<p>see <a href="https://dev.mysql.com/doc/refman/5.7/en/document-store.html">Chapter 19, <em>Using MySQL as a Document Store</em></a>.</p>
<p>see <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-innodb-cluster-userguide.html">Chapter 20, <em>InnoDB Cluster</em></a>.</p>
<h2 id="mysqlshow-1"><a class="header" href="#mysqlshow-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlshow.html"><strong>mysqlshow</strong></a></a></h2>
<p>显示数据库信息</p>
<p>数据库</p>
<p>表</p>
<p>列</p>
<p>索引</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlshow.html">Section 4.5.7, “<strong>mysqlshow</strong> — Display Database, Table, and Column Information”</a>.</p>
<h2 id="mysqlslap-1"><a class="header" href="#mysqlslap-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlslap.html"><strong>mysqlslap</strong></a></a></h2>
<p>负载模拟客户端</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlslap.html">Section 4.5.8, “<strong>mysqlslap</strong> — A Load Emulation Client”</a>.</p>
<h1 id="管理工具-1"><a class="header" href="#管理工具-1">管理工具</a></h1>
<h2 id="innochecksum-1"><a class="header" href="#innochecksum-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/innochecksum.html"><strong>innochecksum</strong></a></a></h2>
<p>离线 Innodb 文件校验和工具</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/innochecksum.html">Section 4.6.1, “<strong>innochecksum</strong> — Offline InnoDB File Checksum Utility”</a>.</p>
<h2 id="myisam_ftdump-1"><a class="header" href="#myisam_ftdump-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisam-ftdump.html"><strong>myisam_ftdump</strong></a></a></h2>
<p>查看 myisam 全文索引 信息的工具</p>
<h2 id="myisamchk-1"><a class="header" href="#myisamchk-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisamchk.html"><strong>myisamchk</strong></a></a></h2>
<p>描述,检查,优化 修复 MyISAM 表</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/myisamchk.html">Section 4.6.3, “<strong>myisamchk</strong> — MyISAM Table-Maintenance Utility”</a>.</p>
<h2 id="myisamlog-1"><a class="header" href="#myisamlog-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisamlog.html"><strong>myisamlog</strong></a></a></h2>
<p>处理 myisam 日志文件内容</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/myisamlog.html">Section 4.6.4, “<strong>myisamlog</strong> — Display MyISAM Log File Contents”</a>.</p>
<h2 id="myisampack-1"><a class="header" href="#myisampack-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/myisampack.html"><strong>myisampack</strong></a></a></h2>
<p>A utility that compresses <code>MyISAM</code> tables to produce smaller read-only tables. See <a href="https://dev.mysql.com/doc/refman/5.7/en/myisampack.html">Section 4.6.5, “<strong>myisampack</strong> — Generate Compressed, Read-Only MyISAM Tables”</a>.</p>
<h2 id="mysql_config_editor-1"><a class="header" href="#mysql_config_editor-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html"><strong>mysql_config_editor</strong></a></a></h2>
<p>A utility that enables you to store authentication credentials in a secure, encrypted login path</p>
<p>file named <code>.mylogin.cnf</code>. See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html">Section 4.6.6, “<strong>mysql_config_editor</strong> — MySQL Configuration Utility”</a>.</p>
<h2 id="mysqlbinlog-1"><a class="header" href="#mysqlbinlog-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlbinlog.html"><strong>mysqlbinlog</strong></a></a></h2>
<p>A utility for reading statements from a binary log. The log of executed statements contained in the binary log files can be used to help recover from a crash.</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqlbinlog.html">Section 4.6.7, “<strong>mysqlbinlog</strong> — Utility for Processing Binary Log Files”</a>.</p>
<h2 id="mysqldumpslow-1"><a class="header" href="#mysqldumpslow-1"><a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldumpslow.html"><strong>mysqldumpslow</strong></a></a></h2>
<p>慢 sql 日志</p>
<p>See <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldumpslow.html">Section 4.6.8, “<strong>mysqldumpslow</strong> — Summarize Slow Query Log Files”</a>.</p>
<p>mysql 客户端 服务器通信时 使用如下环境变量</p>
<table><thead><tr><th style="text-align: left">Environment Variable</th><th style="text-align: left">Meaning</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>MYSQL_UNIX_PORT</code></td><td style="text-align: left">The default Unix socket file; used for connections to <code>localhost</code></td></tr>
<tr><td style="text-align: left"><code>MYSQL_TCP_PORT</code></td><td style="text-align: left">The default port number; used for TCP/IP connections</td></tr>
<tr><td style="text-align: left"><code>MYSQL_PWD</code></td><td style="text-align: left">The default password</td></tr>
<tr><td style="text-align: left"><code>MYSQL_DEBUG</code></td><td style="text-align: left">Debug trace options when debugging</td></tr>
<tr><td style="text-align: left"><code>TMPDIR</code></td><td style="text-align: left">The directory where temporary tables and files are created</td></tr>
</tbody></table>
<p>For a full list of environment variables used by MySQL programs, see <a href="https://dev.mysql.com/doc/refman/5.7/en/environment-variables.html">Section 4.9, “Environment Variables”</a>.</p>
<p>Use of <code>MYSQL_PWD</code> is insecure. See <a href="https://dev.mysql.com/doc/refman/5.7/en/password-security-user.html">Section 6.1.2.1, “End-User Guidelines for Password Security”</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="选项参数与非选项参数"><a class="header" href="#选项参数与非选项参数">选项参数与非选项参数</a></h2>
<p><strong>选项参数</strong>以 <code>- 或者 --</code>   开始的命令选项</p>
<p><strong>非选项参数</strong> 提供额外信息给命令行程序</p>
<p>例如 mysql命令 的第一个非选项 参数 为数据库名</p>
<p>使用最多的选项 <strong>连接参数</strong></p>
<p><code>--host -h</code></p>
<p><code>--user -u</code></p>
<p><code>--password -p</code></p>
<p><code>--port -P</code></p>
<p><code>--socket -S</code> 指定 UnixSocketFile 或者windows上的具名管道</p>
<h2 id="指定选项参数的-几种方式"><a class="header" href="#指定选项参数的-几种方式">指定选项参数的 几种方式</a></h2>
<h3 id="命令名称后面-跟随"><a class="header" href="#命令名称后面-跟随">命令名称后面 跟随</a></h3>
<ul>
<li><strong>后面的覆盖前面的</strong></li>
</ul>
<pre><code>mysql --column-names --skip-column-names
</code></pre>
<ul>
<li>
<p><code>--</code> 为选项全拼 <code>-</code> 为选项缩写</p>
</li>
<li>
<p>缩写没有 等于号,选项名 与选项值 直接可以有 空格</p>
<pre><code>-h localhost or --host=localhost
</code></pre>
</li>
<li>
<p>大小写敏感</p>
</li>
<li>
<p><code>- _</code> 一致</p>
<pre><code>--skip-grant-tables and --skip_grant_tables是一样的
</code></pre>
</li>
<li>
<p>数值类型可以 有 <code>K M G</code></p>
</li>
</ul>
<pre><code>mysqladmin --count=1K --sleep=10 ping
ping服务器 1024次 每次等10s
</code></pre>
<ul>
<li>带有空格的 选项值 带引号</li>
</ul>
<pre><code>shell&gt; mysql -u root -p -e &quot;SELECT VERSION();SELECT NOW()&quot;
Enter password: ******
+------------+
| VERSION()  |
+------------+
| 5.7.29     |
+------------+
+---------------------+
| NOW()               |
+---------------------+
| 2019-09-03 10:36:28 |
+---------------------+
shell&gt;
</code></pre>
<h3 id="选项文件"><a class="header" href="#选项文件">选项文件</a></h3>
<p>忽略空行</p>
<p>头尾空格自动去除</p>
<p>非空行有以下几种形式</p>
<ul>
<li>
<p>注释 #<em><code>comment</code></em><code>, </code>;<em><code>comment</code></em></p>
<p>Comment lines start with <code>#</code> or <code>;</code>. A <code>#</code> comment can start in the middle of a line as well.</p>
</li>
<li>
<p>[<em><code>group</code></em>]</p>
<p>组号,或程序的 名称, 后面跟着的是 应用与该程序的 选项</p>
<ul>
<li>
<p>the <code>[mysqld]</code> and <code>[mysql]</code> groups apply to the <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld.html"><strong>mysqld</strong></a> server and the <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql.html"><strong>mysql</strong></a> client program, respectively.</p>
</li>
<li>
<p>The <code>[client]</code> option group is read by all client programs provided in MySQL distributions (but <em>not</em> by <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqld.html"><strong>mysqld</strong></a>). </p>
<p>To understand how third-party client programs that use the C API can use option files, see the C API documentation at <a href="https://dev.mysql.com/doc/c-api/5.7/en/mysql-options.html">mysql_options()</a>.</p>
</li>
<li>
<p><code>[mysqldump]</code> enables <a href="https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html"><strong>mysqldump</strong></a>-specific options to override <code>[client]</code> options. 可以覆盖前面的 client指定的同名选项</p>
</li>
<li>
<p>指定版本</p>
<pre><code>[mysqld-5.7]
sql_mode=TRADITIONAL
</code></pre>
</li>
<li>
<p>包含其他 选项文件</p>
<pre><code>!include /home/mydir/myopt.cnf

查找目录
!includedir /home/mydir, 查找任何以 .cnf .ini
Any files to be found and included using the !includedir directive on Unix operating systems must have file names ending in .cnf. On Windows, this directive checks for files with the .ini or .cnf extension.
</code></pre>
</li>
</ul>
</li>
<li>
<p><code>opt_name</code><em>=</em><code>value</code></p>
<p>选项名, 相比命令行 选项 去掉 短横杠</p>
</li>
<li>
<p>转义处理</p>
<pre><code>\b, \t, \n, \r, \\ \s
backspace, tab, newline, carriage return, backslash, and space characters.
windows路径目录特殊处理


basedir=&quot;C:\Program Files\MySQL\MySQL Server 5.7&quot;
basedir=&quot;C:\\Program Files\\MySQL\\MySQL Server 5.7&quot;
basedir=&quot;C:/Program Files/MySQL/MySQL Server 5.7&quot;
basedir=C:\\Program\sFiles\\MySQL\\MySQL\sServer\s5.7
</code></pre>
</li>
<li>
<p>影响 选项文件处理 的命令行参数</p>
<ul>
<li>
<p><code>--print-defaults</code></p>
<p>打印所有从 文件中读取的选项,密码会被掩盖</p>
</li>
<li>
<p><code>--no-defaults</code></p>
<p>不要从文件中读取选项</p>
<p>有一个例外是 客户端程序 从  .mylogin.cnf  文件中读取选项  不受影响</p>
<p><code>.mylogin.cnf</code> is created by the <a href="https://dev.mysql.com/doc/refman/5.7/en/mysql-config-editor.html"><strong>mysql_config_editor</strong></a> </p>
</li>
<li>
<p><code>--login-path=name</code></p>
<p>从 loginFile中读取 选项 , 其中的选项 涉及 登录信息,  mysql_config_editor 工具来编辑此文件</p>
<pre><code>mysql --login-path=mypath
默认读取 [client] [mysql] 组
还有  [mypath] 组
</code></pre>
<p>指定备用登录文件组名</p>
<pre><code>MYSQL_TEST_LOGIN_FILE 环境变量
</code></pre>
</li>
<li>
<p><code>--defaults-group-suffix=str</code></p>
<pre><code>指定其他组名前缀读取
[client] and [mysql]
--defaults-group-suffix=_other
mysql also reads the [client_other] and [mysql_other] groups.
</code></pre>
</li>
<li>
<p><code>--defaults-file=file_name</code></p>
<p>给定 指定的 选项文件读取</p>
<p>同样不会影响  <code>.mylogin.cnf</code></p>
</li>
<li>
<p><code>--defaults-extra-file=file_name</code></p>
<p>全局选项文件 读取后 , 用户选项文件读取前,  .mylogin.cnf 读取前</p>
</li>
</ul>
</li>
<li>
<p>程序选项 修饰符</p>
<p>在查询时 不输出字段名</p>
<pre><code>--disable-column-names 
--skip-column-names 
--column-names=0
</code></pre>
<pre><code>--column-names
--enable-column-names
--column-names=1
</code></pre>
<p>使用 <code>--loose</code> 前缀 时 如果不存在该选项 会警告 而不报错退出</p>
<pre><code>shell&gt; mysql --loose-no-such-option
mysql: WARNING: unknown option '--loose-no-such-option'
</code></pre>
<p><code>--maximum</code> 设置 session级别 变量 的最大值</p>
<pre><code> --maximum-max_heap_table_size=32M  最大表堆的大小
</code></pre>
</li>
<li>
<p>在执行时可以使用表达式. 在启动时使用标识符 </p>
<pre><code> mysql --max_allowed_packet=16M
 SET GLOBAL max_allowed_packet=16*1024*1024;
</code></pre>
</li>
</ul>
<p><strong>建立连接的命令行选项</strong></p>
<table><thead><tr><th style="text-align: left"></th><th style="text-align: left"></th><th style="text-align: left"></th></tr></thead><tbody>
<tr><td style="text-align: left">Option Name</td><td style="text-align: left">Description</td><td style="text-align: left">Deprecated</td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_default-auth">--default-auth</a></td><td style="text-align: left">客户端认证所使用的插件,See <a href="https://dev.mysql.com/doc/refman/5.7/en/pluggable-authentication.html">Section 6.2.13, “Pluggable Authentication”</a>.</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_host">--host</a></td><td style="text-align: left">主机名或者IPV4 地址</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_password">--password</a></td><td style="text-align: left">密码,使用命令行输入密码不安全, See <a href="https://dev.mysql.com/doc/refman/5.7/en/password-security-user.html">Section 6.1.2.1, “End-User Guidelines for Password Security”</a>.</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_pipe">--pipe</a></td><td style="text-align: left">Connect to server using named pipe (Windows only)<br />This option applies only if the server was started with the <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_named_pipe"><code>named_pipe</code></a> system variable enabled to support named-pipe connections. In addition, the user making the connection must be a member of the Windows group specified by the <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_named_pipe_full_access_group"><code>named_pipe_full_access_group</code></a> system variable.</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_plugin-dir">--plugin-dir</a></td><td style="text-align: left">寻找插件的目录,See <a href="https://dev.mysql.com/doc/refman/5.7/en/pluggable-authentication.html">Section 6.2.13, “Pluggable Authentication”</a>.</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_port">--port</a></td><td style="text-align: left">TCP/IP port number for connection</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_protocol">--protocol</a></td><td style="text-align: left">`TCP</td><td style="text-align: left">SOCKET</td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_secure-auth">--secure-auth</a></td><td style="text-align: left">Do not send passwords to server in old (pre-4.1) format<br /><a href="https://dev.mysql.com/doc/refman/5.7/en/account-upgrades.html">Section 6.4.1.3, “Migrating Away from Pre-4.1 Password Hashing and the mysql_old_password Plugin”</a>.</td><td style="text-align: left">Yes</td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_shared-memory-base-name">--shared-memory-base-name</a></td><td style="text-align: left">On Windows, the shared-memory name to use for connections made using shared memory to a local server. The default value is <code>MYSQL</code>. The shared-memory name is case-sensitive.<br />This option applies only if the server was started with the <a href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_shared_memory"><code>shared_memory</code></a> system variable enabled to support shared-memory connections.</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_socket">--socket</a></td><td style="text-align: left">Unix socket file 用来做本地连接,默认名是 /tmp/mysql.sock<br />在Windows是命名管道,默认名是MySQL<br />On Windows, this option applies only if the server was started with the named_pipe system variable enabled to support named-pipe connections. In addition, the user making the connection must be a member of the Windows group specified by the named_pipe_full_access_group system variable.</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_user">--user</a></td><td style="text-align: left">MySQL user name to use when connecting to server</td><td style="text-align: left"></td></tr>
</tbody></table>
<p><strong>协议介绍</strong></p>
<table><thead><tr><th style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_protocol"><code>--protocol</code></a> Value</th><th style="text-align: left">Transport Protocol Used</th><th style="text-align: left">Applicable Platforms</th></tr></thead><tbody>
<tr><td style="text-align: left"><code>TCP</code></td><td style="text-align: left">TCP/IP transport to local or remote server</td><td style="text-align: left">All</td></tr>
<tr><td style="text-align: left"><code>SOCKET</code></td><td style="text-align: left">Unix socket-file transport to local server</td><td style="text-align: left">Unix and Unix-like systems</td></tr>
<tr><td style="text-align: left"><code>PIPE</code></td><td style="text-align: left">Named-pipe transport to local server</td><td style="text-align: left">Windows</td></tr>
<tr><td style="text-align: left"><code>MEMORY</code></td><td style="text-align: left">Shared-memory transport to local server</td><td style="text-align: left">Windows</td></tr>
</tbody></table>
<p><strong>加密连接选项</strong></p>
<table><thead><tr><th style="text-align: left">Option Name</th><th style="text-align: left">Description</th><th style="text-align: left">Introduced</th></tr></thead><tbody>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_get-server-public-key">--get-server-public-key</a></td><td style="text-align: left">Request RSA public key from server</td><td style="text-align: left">5.7.23</td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_server-public-key-path">--server-public-key-path</a></td><td style="text-align: left">Path name to file containing RSA public key</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl">--skip-ssl</a></td><td style="text-align: left">Disable connection encryption</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl">--ssl</a></td><td style="text-align: left">Enable connection encryption</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-ca">--ssl-ca</a></td><td style="text-align: left">File that contains list of trusted SSL Certificate Authorities</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-capath">--ssl-capath</a></td><td style="text-align: left">Directory that contains trusted SSL Certificate Authority certificate files</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-cert">--ssl-cert</a></td><td style="text-align: left">File that contains X.509 certificate</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-cipher">--ssl-cipher</a></td><td style="text-align: left">Permissible ciphers for connection encryption</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-crl">--ssl-crl</a></td><td style="text-align: left">File that contains certificate revocation lists</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-crlpath">--ssl-crlpath</a></td><td style="text-align: left">Directory that contains certificate revocation-list files</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-key">--ssl-key</a></td><td style="text-align: left">File that contains X.509 key</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-mode">--ssl-mode</a></td><td style="text-align: left">Desired security state of connection to server</td><td style="text-align: left">5.7.11</td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_ssl-verify-server-cert">--ssl-verify-server-cert</a></td><td style="text-align: left">Verify host name against server certificate Common Name identity</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left"><a href="https://dev.mysql.com/doc/refman/5.7/en/connection-options.html#option_general_tls-version">--tls-version</a></td><td style="text-align: left">Permissible TLS protocols for encrypted connections</td><td style="text-align: left">5.7.10</td></tr>
</tbody></table>
<p><strong>--ssl-mode=mode</strong></p>
<ul>
<li>
<p>disable</p>
<p>没有加密的连接</p>
<pre><code> --ssl=0 option or its synonyms (--skip-ssl, --disable-ssl).
</code></pre>
</li>
<li>
<p>PREFERRED</p>
<p>尝试建立加密连接,如果不能建立 则建立非加密连接</p>
</li>
<li>
<p>REQUIRED</p>
<p>需要建立加密连接,否则无法建立连接</p>
</li>
<li>
<p>VERIFY_CA</p>
</li>
</ul>
<h3 id="读环境变量"><a class="header" href="#读环境变量">读环境变量</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mysql日志"><a class="header" href="#mysql日志">MySQL日志</a></h1>
<p>任何一种数据库，都会拥有各种各样的日志，用来记录数据库的运行情况、日常操作和错误等信息，可以帮助我们诊断数据库出现的各种问题。</p>
<p>MySQL 也不例外，它有不同类型的日志文件，各自存储了不同类型的日志。分析这些日志文件，除了可以了解 MySQL 数据库的运行情况，还可以为 MySQL 的管理和优化提供必要的信息。</p>
<p>日志管理是维护数据库的重要步骤，所以经常需要在 MySQL 中进行日志启动、查看、停止和删除等操作。这些操作是数据库管理中最基本、最重要的操作。本章将介绍 MySQL 中各种日志的作用和使用。</p>
<p><a href="mysql/mysql%E6%97%A5%E5%BF%97/mysql%E6%97%A5%E5%BF%97%E5%8F%8A%E5%88%86%E7%B1%BB.html">MySQL日志及分类</a></p>
<p><a href="mysql/mysql%E6%97%A5%E5%BF%97/%E9%94%99%E8%AF%AF%E6%97%A5%E5%BF%97.html">MySQL错误日志（Error Log）详解</a></p>
<p><a href="mysql/mysql%E6%97%A5%E5%BF%97/%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%97%A5%E5%BF%97.html">MySQL二进制日志（Binary Log）详解</a></p>
<p><a href="mysql/mysql%E6%97%A5%E5%BF%97/%E9%80%9A%E7%94%A8%E6%97%A5%E5%BF%97.html">MySQL通用查询日志（General Query Log）</a></p>
<p><a href="mysql/mysql%E6%97%A5%E5%BF%97/%E6%85%A2sql%E6%97%A5%E5%BF%97.html">MySQL慢查询日志（Slow Query Log）</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="前言"><a class="header" href="#前言">前言</a></h1>
<p>日志是数据库的重要组成部分，主要用来记录数据库的运行情况、日常操作和错误信息。</p>
<p>在 MySQL 中，日志可以分为</p>
<ul>
<li>
<p>二进制日志</p>
<p>该日志文件会以二进制的形式记录数据库的各种操作，但不记录查询语句。</p>
</li>
<li>
<p>错误日志</p>
<p>该日志文件会记录 MySQL 服务器的启动、关闭和运行错误等信息。</p>
</li>
<li>
<p>通用查询日志</p>
<p>该日志记录 MySQL 服务器的启动和关闭信息、客户端的连接信息、更新、查询数据记录的 SQL 语句等。</p>
</li>
<li>
<p>慢查询日志</p>
<p>记录执行事件超过指定时间的操作，通过工具分析慢查询日志可以定位 MySQL 服务器性能瓶颈所在。</p>
</li>
</ul>
<p>为了维护 MySQL 数据库，经常需要在 MySQL 中进行日志操作，包含日志文件的启动、查看、停止和删除等，这些操作都是数据库管理中最基本、最重要的操作。</p>
<p>例如，当用户 root 登录到 MySQL 服务器后，就会在日志文件里记录该用户的登录事件、执行操作等信息。当 MySQL 服务器运行时出错，出错信息就会被记录到日志文件里。</p>
<p>日志操作是数据库维护中最重要的手段之一</p>
<ol>
<li>
<p>如果 MySQL 数据库系统意外停止服务，我们可以通过错误日志查看出现错误的原因</p>
</li>
<li>
<p>还可以通过二进制日志文件来查看用户分别执行了哪些操作、对数据库文件做了哪些修改</p>
</li>
<li>
<p>然后，还可以根据二进制日志中的记录来修复数据库。</p>
</li>
</ol>
<p><strong>默认只启动错误日志</strong></p>
<p>在 MySQL 所支持的日志文件里，除了二进制日志文件外，其它日志文件都是文本文件。默认情况下，<strong>MySQL 只会启动错误日志文件</strong>，而其它日志则需要手动启动。</p>
<p><strong>缺点与优点</strong></p>
<p>使用日志有优点也有缺点。启动日志后，虽然可以对 MySQL 服务器性能进行维护，但是会降低 MySQL 的执行速度。例如，一个查询操作比较频繁的 MySQL 中，记录通用查询日志和慢查询日志要花费很多的时间。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="前言-1"><a class="header" href="#前言-1">前言</a></h1>
<p>二进制日志（Binary Log）也可叫作变更日志（Update Log），是 MySQL 中非常重要的日志。</p>
<p>主要用于记录数据库的变化情况，即 SQL 语句的 DDL 和 DML 语句，不包含数据记录查询操作。</p>
<p>如果 MySQL 数据库意外停止，可以通过二进制日志文件来查看用户执行了哪些操作，对数据库服务器文件做了哪些修改，然后根据二进制日志文件中的记录来恢复数据库服务器。</p>
<p>默认情况下，二进制日志功能是关闭的。可以通过以下命令查看二进制日志是否开启，命令如下：</p>
<pre><code> SHOW VARIABLES LIKE 'log_bin';
</code></pre>
<h1 id="启动和设置二进制日志"><a class="header" href="#启动和设置二进制日志">启动和设置二进制日志</a></h1>
<p>在 MySQL 中，可以通过在配置文件中添加 log-bin 选项来开启二进制日志，格式如下：</p>
<pre><code>[mysqld]
log-bin=dir/[filename]
</code></pre>
<p>其中，dir 参数指定二进制文件的存储路径；filename 参数指定二进制文件的文件名，其形式为 filename.number，number 的形式为 000001、000002 等。</p>
<p>每次重启 MySQL 服务后，都会生成一个新的二进制日志文件，这些日志文件的文件名中 filename 部分不会改变，number 会不断递增。</p>
<p>如果没有 dir 和 filename 参数，二进制日志将默认存储在数据库的数据目录下，</p>
<p>默认的文件名为 <strong>hostname-bin.number</strong>，其中 hostname 表示主机名。</p>
<p>下面在 my.ini 文件的 [mysqld] 组中添加以下语句：</p>
<pre><code>log-bin
</code></pre>
<p>重启 MySQL 服务器后，可以在 MySQL 数据库的数据目录下看到 LAPTOP-UHQ6V8KP-bin.000001 这个文件，同时还生成了 LAPTOP-UHQ6V8KP-bin.index 文件。</p>
<p>还可以在 my.ini 文件的 [mysqld] 组中进行如下修改。语句如下：</p>
<pre><code>log-bin=C:log\mylog
</code></pre>
<h1 id="查看二进制日志"><a class="header" href="#查看二进制日志">查看二进制日志</a></h1>
<h4 id="查看二进制日志文件列表"><a class="header" href="#查看二进制日志文件列表">查看二进制日志文件列表</a></h4>
<pre><code class="language-sh"> SHOW binary logs;
</code></pre>
<h4 id="查看当前正在写入的二进制日志文件"><a class="header" href="#查看当前正在写入的二进制日志文件">查看当前正在写入的二进制日志文件</a></h4>
<pre><code> SHOW master status;
</code></pre>
<h4 id="查看二进制日志文件内容"><a class="header" href="#查看二进制日志文件内容">查看二进制日志文件内容</a></h4>
<p>二进制日志使用二进制格式存储，不能直接打开查看。如果需要查看二进制日志，必须使用 mysqlbinlog 命令。</p>
<pre><code class="language-sql">mysqlbinlog filename.number
</code></pre>
<p>mysqlbinlog 命令只在当前文件夹下查找指定的二进制日志，因此需要在二进制日志所在的目录下运行该命令，否则将会找不到指定的二进制日志文件。</p>
<p>除了 filename.number 文件，MySQL 还会生成一个名为 filename.index 的文件，这个文件存储着<strong>所有二进制日志文件的列表，可以用记事本打开该文件。</strong></p>
<blockquote>
<p>小技巧：实际工作中，二进制日志文件与数据库的数据文件不放在同一块硬盘上，这样即使数据文件所在的硬盘被破坏，也可以使用另一块硬盘上的二进制日志来恢复数据库文件。两块硬盘同时坏了的可能性要小得多，这样可以保证数据库中数据的安全。</p>
</blockquote>
<h1 id="删除二进制日志"><a class="header" href="#删除二进制日志">删除二进制日志</a></h1>
<p>二进制日志中记录着大量的信息，如果很长时间不清理二进制日志，将会浪费很多的磁盘空间。删除二进制日志的方法很多，下面介绍几种删除二进制日志的方法。</p>
<h2 id="删除所有二进制日志"><a class="header" href="#删除所有二进制日志">删除所有二进制日志</a></h2>
<pre><code class="language-sh">RESET MASTER;
</code></pre>
<p>登录 MySQL 数据库后，可以执行该语句来删除所有二进制日志。删除所有二进制日志后，MySQL 将会重新创建新的二进制日志，新二进制日志的编号从 000001 开始。</p>
<h2 id="根据编号删除二进制日志"><a class="header" href="#根据编号删除二进制日志">根据编号删除二进制日志</a></h2>
<p>每个二进制日志文件后面有一个 6 位数的编号，如 000001。使用 PURGE MASTER LOGS TO 语句，可以删除指定二进制日志的编号之前的日志。该语句的基本语法形式如下：</p>
<pre><code># 该语句将删除编号小于 filename.number 的所有二进制日志。
PURGE MASTER LOGS TO 'filename.number';

# 下面删除 mylog.000004 之前的二进制日志
PURGE MASTER LOGS TO 'mylog.000004';
#代码执行完后，编号为 000001、000002 和 000003 的二进制日志将被删除。
</code></pre>
<h2 id="根据创建时间删除二进制日志"><a class="header" href="#根据创建时间删除二进制日志">根据创建时间删除二进制日志</a></h2>
<p>使用 PURGE MASTER LOGS TO 语句，可以删除指定时间之前创建的二进制日志，该语句的基本语法格式如下：</p>
<p>其中，“hh”为 24 制的小时。该语句将删除在指定时间之前创建的所有二进制日志。</p>
<pre><code>PURGE MASTER LOGS TO 'yyyy-mm-dd hh:MM:ss';
PURGE MASTER LOGS TO '2019-12-20 15:00:00&quot;;
</code></pre>
<h2 id="暂时停止二进制日志"><a class="header" href="#暂时停止二进制日志">暂时停止二进制日志</a></h2>
<p>在配置文件中设置了 log_bin 选项之后，MySQL 服务器将会一直开启二进制日志功能。删除该选项后就可以停止二进制日志功能，如果需要再次启动这个功能，需要重新添加 log_bin 选项。由于这样比较麻烦，所以 MySQL 提供了暂时停止二进制日志功能的语句。</p>
<pre><code>SET SQL_LOG_BIN=0/1;
</code></pre>
<h1 id="其他参数"><a class="header" href="#其他参数">其他参数</a></h1>
<pre><code># 定义了 MySQL 清除过期日志的时间、二进制日志自动删除的天数。默认值为 0，表示“没有自动删除”，当 MySQL 启动或刷新二进制日志时可能删除。
expire_logs_days = 10
# 定义了单个文件的大小限制，如果二进制日志写入的内容大小超出给定值，日志就会发生滚动（关闭当前文件，重新打开一个新的日志文件  不能将该变量设置为大于 1GB 或小于 4096B（字节），其默认值是 1GB。
max_binlog_size = 100M
</code></pre>
<h1 id="mysql使用二进制日志还原数据库"><a class="header" href="#mysql使用二进制日志还原数据库">MySQL使用二进制日志还原数据库</a></h1>
<p>数据库遭到意外损坏时，应该先使用最近的备份文件来还原数据库。另外备份之后，数据库可能进行了一些更新，这时可以使用二进制日志来还原。因为二进制日志中存储了更新数据库的语句，如 UPDATE 语句、INSERT 语句等。</p>
<pre><code>mysqlbinlog filename.number | mysql -u root -p
</code></pre>
<p>以上命令可以理解成，先使用 mysqlbinlog 命令来读取 filename.number 中的内容，再使用 mysql 命令将这些内容还原到数据库中。</p>
<p>因此，在备份 MySQL 数据库之后，应该删除备份之前的二进制日志。如果备份之后发生异常，造成数据库的数据损失，可以通过备份之后的二进制日志进行还原。</p>
<p>使用 mysqlbinlog 命令进行还原操作时，必须是编号（number）小的先还原。例如，mylog.000001 必须在 mylog.000002 之前还原。</p>
<pre><code class="language-sql">mysqlbinlog mylog.000001 | mysql -u root -p
mysqlbinlog mylog.000002 | mysql -u root -p
mysqlbinlog mylog.000003 | mysql -u root -p
mysqlbinlog mylog.000004 | mysql -u root -p
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="慢查询"><a class="header" href="#慢查询"><strong>慢查询</strong></a></h1>
<p>慢查询，到底多慢才叫慢？有没有统一的标准？其实呀，这并没有统一的标准，每个公司，甚至同一公司不同场景(数据库)都会有不同标准</p>
<p>像OLTP(联机事务处理), OLAP (联机分析处理) 这两者对慢查询的标准就不一样</p>
<p>OLAP 实时性则没那么高，对慢查询容错性也会更高些</p>
<p>而OLTP(联机事务处理)属于事务处理型，实时性要求高，响应时间快，对慢查询几乎零容忍</p>
<p>一个成熟系统中，有监控系统实时监控线上查询运行状态，提前将慢查询筛选出来，也是避免生产事故，降低风险的有效措施</p>
<p>有些数据库中也内置慢查询监控，如：MySQL慢查询日志就是其一。</p>
<h1 id="开启慢查询日志"><a class="header" href="#开启慢查询日志"><strong>开启慢查询日志</strong></a></h1>
<p>在 MySQL中，提供了慢查询查询日志，基于性能方面的考虑，该配置默认为OFF(关闭) 状态。那么如何开启慢日志查询呢？其步骤如下：</p>
<pre><code class="language-sql">show variables like &quot;slow_query_log&quot;;
set global slow_query_log = &quot;ON&quot;;
show variables like &quot;slow_query_log_file&quot;;
# 其中: path 表示路径， filename 表示文件名，如果不指定，其默认filename 为hostname。
set global slow_query_log_file = ${path}/${filename}.log;
</code></pre>
<p>慢查询 查询时间，当SQL执行时间超过该值时，则会记录在slow_query_log_file 文件中，其默认为 10 ，最小值为 0，(单位：秒)。</p>
<pre><code class="language-sql"> show variables like &quot;long_query_time&quot;;
 set global long_query_time = 5;
</code></pre>
<p>当设置值小于0时，默认为 0。</p>
<p>通过上述设置后，退出当前会话或者开启一个新的会话，执行如下命令：</p>
<pre><code class="language-sql">select sleep(11);
</code></pre>
<pre><code class="language-log"># Time: 200310 13:30:57
# User@Host: root[root] @ localhost []  Id: 21528
# Query_time: 6.000164  Lock_time: 0.000000 Rows_sent: 1  Rows_examined: 0
SET timestamp=1583818257;
select sleep(6);
</code></pre>
<h1 id="慢查询日志文件"><a class="header" href="#慢查询日志文件"><strong>慢查询日志文件</strong></a></h1>
<ol>
<li>慢查询日志以#作为起始符。</li>
<li>User@Host：表示用户 和 慢查询查询的ip地址。</li>
<li>如上所述，表示 root用户 localhost地址。</li>
<li>Query_time: 表示SQL查询持续时间， 单位 (秒)。</li>
<li>Lock_time: 表示获取锁的时间， 单位(秒)。</li>
<li>Rows_sent: 表示发送给客户端的行数。</li>
<li>Rows_examined: 表示：服务器层检查的行数。</li>
<li>set timestamp ：表示 慢SQL 记录时的时间戳。</li>
<li>其中 select sleep(6) 则表示慢SQL语句。</li>
</ol>
<h3 id="注意事项-1"><a class="header" href="#注意事项-1"><strong>注意事项</strong></a></h3>
<ol>
<li>在 MySQL 中，慢查询日志中默认不记录管理语句，如：</li>
</ol>
<p><strong>alter table, analyze table，check table等。</strong></p>
<pre><code class="language-sql">set global log_slow_admin_statements = &quot;ON&quot;;
</code></pre>
<ol start="2">
<li>在 MySQL 中，还可以设置将<strong>未走索引的SQL语句记录</strong>在慢日志查询文件中(默认为关闭状态)。通过下述属性即可进行设置：</li>
</ol>
<pre><code class="language-sql">set global log_queries_not_using_indexes = &quot;ON&quot;;
</code></pre>
<ol start="3">
<li>日志输出格式有支持：FILE(默认)，TABLE 两种，可进行组合使用。如下所示:
set global log_output = &quot;FILE,TABLE&quot;;</li>
</ol>
<p>这样设置会同时在 FILE, mysql库中的slow_log表中同时写入。</p>
<pre><code class="language-text"> select * from slow_log;
</code></pre>
<h1 id="特别注意"><a class="header" href="#特别注意">特别注意</a></h1>
<ol>
<li><strong>设置该属性后，只要SQL未走索引，即使查询时间小于long_query_time值，也会记录在慢SQL日志文件中。</strong></li>
<li><strong>该设置会导致慢日志快速增长，开启前建议检查慢查询日志文件所在磁盘空间是否充足。</strong></li>
<li><strong>在生产环境中，不建议开启该参数。</strong></li>
</ol>
<h1 id="删除慢查询日志"><a class="header" href="#删除慢查询日志">删除慢查询日志</a></h1>
<pre><code class="language-shell">mysqladmin -uroot -p flush-logs
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mysql通用查询日志general-query-log"><a class="header" href="#mysql通用查询日志general-query-log">MySQL通用查询日志（General Query Log）</a></h1>
<p>通用查询日志（General Query Log）用来记录用户的所有操作，包括启动和关闭 MySQL 服务、更新语句和查询语句等。</p>
<p>默认情况下，通用查询日志功能是关闭的。可以通过以下命令查看通用查询日志是否开启，命令如下：</p>
<pre><code class="language-sql"> SHOW VARIABLES LIKE '%general%';
</code></pre>
<p>从结果可以看出，通用查询日志是关闭的，general_log_file 变量指定了通用查询日志文件所在的位置。</p>
<h1 id="启动和设置通用查询日志"><a class="header" href="#启动和设置通用查询日志">启动和设置通用查询日志</a></h1>
<pre><code class="language-sql">[mysqld]
log=dir/filename
</code></pre>
<p>其中，dir 参数指定通用查询日志的存储路径；filename 参数指定日志的文件名。如果不指定存储路径，通用查询日志将默认存储到 MySQL 数据库的数据文件夹下。如果不指定文件名，默认文件名为 hostname.log，其中 hostname 表示主机名。</p>
<h1 id="查看通用查询日志"><a class="header" href="#查看通用查询日志">查看通用查询日志</a></h1>
<p>如果希望了解用户最近的操作，可以查看通用查询日志。通用查询日志以文本文件的形式存储，可以使用普通文本文件查看该类型日志内容。</p>
<pre><code>SHOW VARIABLES LIKE '%general%';
use test;
SELECT * FROM tb_student;
</code></pre>
<p>执行成功后，打开通用查询日志，这里日志名称为 LAPTOP-UHQ6V8KP.log，下面是通用查询日志中的部分内容。</p>
<pre><code>Time                 Id Command    Argument
2020-05-29T06:43:44.382878Z     7 Quit
2020-05-29T06:44:10.001382Z     8 Connect root@localhost on  using SSL/TLS
2020-05-29T06:44:10.007532Z     8 Query select @@version_comment limit 1
2020-05-29T06:44:11.748179Z     8 Query SHOW VARIABLES LIKE '%general%'
2020-05-29T06:44:25.487472Z     8 Query SELECT DATABASE()
2020-05-29T06:44:25.487748Z     8 Init DB test
2020-05-29T06:44:35.390523Z     8 Query SELECT * FROM tb_student
</code></pre>
<h1 id="停止通用查询日志"><a class="header" href="#停止通用查询日志">停止通用查询日志</a></h1>
<pre><code> SET GLOBAL general_log=off;
</code></pre>
<h1 id="删除通用查询日志"><a class="header" href="#删除通用查询日志">删除通用查询日志</a></h1>
<pre><code class="language-sql">mysqladmin -uroot -p flush-logs
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="前言-2"><a class="header" href="#前言-2">前言</a></h1>
<p>错误日志（Error Log）是 MySQL 中最常用的一种日志，主要记录 MySQL 服务器启动和停止过程中的信息、服务器在运行过程中发生的故障和异常情况等。</p>
<h1 id="启动和设置错误日志"><a class="header" href="#启动和设置错误日志">启动和设置错误日志</a></h1>
<p><strong>在 MySQL 数据库中，默认开启错误日志功能</strong></p>
<p>一般情况下，错误日志存储在 MySQL 数据库的数据文件夹下，通常名称为</p>
<p><strong>hostname.err</strong> 其中，hostname 表示 MySQL 服务器的主机名。</p>
<p><strong>配置文件定义错误日志</strong></p>
<p>在 MySQL 配置文件中，错误日志所记录的信息可以通过 log-error 和 log-warnings 来定义</p>
<p>log-error：  定义是否启用错误日志功能和错误日志的存储位置</p>
<p>log-warnings: 定义是否将警告信息也记录到错误日志中</p>
<p>将 log_error 选项加入到 MySQL 配置文件的 [mysqld] 组中，形式如下：</p>
<pre><code># 其中，dir 参数指定错误日志的存储路径；filename 参数指定错误日志的文件名；省略参数时文件名默认为主机名，存放在 Data 目录中。
[mysqld]
log-error=dir/{filename}
</code></pre>
<p>重启 MySQL 服务后，参数开始生效，可以在指定路径下看到 filename.err 的文件，如果没有指定 filename，那么错误日志将直接默认为 hostname.err。</p>
<p>注意：<strong>错误日志中记录的并非全是错误信息</strong>，例如 MySQL 如何启动 InnoDB 的表空间文件、如何初始化自己的存储引擎等，这些也记录在错误日志文件中。</p>
<h1 id="查看错误日志"><a class="header" href="#查看错误日志">查看错误日志</a></h1>
<p>错误日志中记录着开启和关闭 MySQL 服务的时间，以及服务运行过程中出现哪些异常等信息。如果 MySQL 服务出现异常，可以到错误日志中查找原因。</p>
<p>在 MySQL 中，通过 SHOW 命令可以查看错误日志文件所在的目录及文件名信息。</p>
<pre><code>SHOW VARIABLES LIKE 'log_error';
</code></pre>
<p>错误日志以文本文件的形式存储，直接使用普通文本工具就可以查看</p>
<h1 id="删除错误日志"><a class="header" href="#删除错误日志">删除错误日志</a></h1>
<pre><code># 执行该命令后，MySQL 服务器首先会自动创建一个新的错误日志，然后将旧的错误日志更名为 filename.err-old。
mysqladmin -uroot -p flush-logs
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="索引-1"><a class="header" href="#索引-1">索引</a></h2>
<p>key value key是某种属性，value是文档列表</p>
<p><strong>正排索引</strong></p>
<p>根据文档相关的属性形成的key。例如 文档ID、文档创建时间等等</p>
<p><strong>倒排索引</strong></p>
<p>key为 文档内容通过分词形成的单词</p>
<p><strong>倒排列表</strong></p>
<p>包含出现过某个单词的所有文档列表、出现的位置信息、以及出现的词频</p>
<h2 id="相关度"><a class="header" href="#相关度">相关度</a></h2>
<p>衡量某个文档与查询的匹配程度</p>
<p><strong>TF-IDF</strong></p>
<p>词频：某个单词在某篇文章的出现次数，词频与相关度正相关</p>
<p>逆文档频率：某个单词在整个 文档库出现的次数，逆文档频率与相关度逆相关</p>
<p><strong>BM-25</strong></p>
<p>基于</p>
<h2 id="搜索引擎的三个核心指标"><a class="header" href="#搜索引擎的三个核心指标">搜索引擎的三个核心指标</a></h2>
<h3 id="更全"><a class="header" href="#更全">更全</a></h3>
<h3 id="更准"><a class="header" href="#更准">更准</a></h3>
<h3 id="更块"><a class="header" href="#更块">更块</a></h3>
<h3 id="搜索引擎的三个核心问题"><a class="header" href="#搜索引擎的三个核心问题">搜索引擎的三个核心问题</a></h3>
<p><strong>用户的真正需求是什么</strong></p>
<p><strong>哪些信息是和用户的需求真正相关的</strong></p>
<p><strong>哪些信息是用户可以信赖的</strong></p>
<h3 id="评价搜索引擎的指标"><a class="header" href="#评价搜索引擎的指标">评价搜索引擎的指标</a></h3>
<table><thead><tr><th></th><th>在搜索结果中</th><th>不在搜索结果中</th></tr></thead><tbody>
<tr><td>相关</td><td>N</td><td>K</td></tr>
<tr><td>不相关</td><td>M</td><td>L</td></tr>
</tbody></table>
<p>精准率 = N/(N+M)</p>
<p>召回率=N/(N+K)</p>
<p>针对搜索引擎的业务场景：精准率更为重要</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="back-up-a-cluster"><a class="header" href="#back-up-a-cluster">Back up a cluster</a></h2>
<ol>
<li>
<p>备份群集的唯一可靠且受支持的方法是拍摄快照。您不能通过复制Elasticsearch集群节点的数据目录来备份它。没有支持的方法可以从文件系统级备份中还原任何数据。</p>
</li>
<li>
<p>如果您尝试从这样的备份中恢复群集，它可能会因损坏或丢失文件或其他数据不一致的报告而失败，或者它似乎已经成功地无声地丢失了一些数据。</p>
</li>
</ol>
<p>To have a complete backup for your cluster:</p>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster-data.html">Back up the data</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster-configuration.html">Back up the cluster configuration</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-backup.html">Back up the security configuration</a></li>
</ol>
<p>To restore your cluster from a backup:</p>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/restore-cluster-data.html">Restore the data</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/restore-security-configuration.html">Restore the security configuration</a></li>
</ol>
<h3 id="back-up-a-clusters-data"><a class="header" href="#back-up-a-clusters-data">Back up a cluster’s data</a></h3>
<h3 id="back-up-the-cluster-configuration"><a class="header" href="#back-up-the-cluster-configuration">Back up the cluster configuration</a></h3>
<h3 id="back-up-the-security-configuration"><a class="header" href="#back-up-the-security-configuration">Back up the security configuration</a></h3>
<h3 id="restore-the-security-configuration"><a class="header" href="#restore-the-security-configuration">Restore the security configuration</a></h3>
<h3 id="restore-the-data"><a class="header" href="#restore-the-data">Restore the data</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h2 id="kibana-query-language"><a class="header" href="#kibana-query-language">Kibana Query Language</a></h2>
<p>Kibana查询语言 (KQL) 是一种简单的语法，用于使用自由文本搜索或基于字段的搜索来过滤Elasticsearch数据。KQL仅用于过滤数据，没有对数据进行排序或聚合的作用。</p>
<p>KQL能够在您键入时提示字段名称、值和运算符。</p>
<p>提示的性能由Kibana<a href="https://www.elastic.co/guide/en/kibana/7.16/settings.html">设置控制</a>。</p>
<p>KQL具有与Lucene查询语法不同的功能集。KQL能够查询嵌套字段和<a href="https://www.elastic.co/guide/en/kibana/7.16/managing-index-patterns.html#scripted-fields">脚本字段</a>。</p>
<p>KQL不支持正则表达式或用模糊项搜索。要使用旧版Lucene语法，请单击搜索字段旁边的KQL，然后关闭KQL。</p>
<h2 id="terms-query"><a class="header" href="#terms-query">Terms query</a></h2>
<ol>
<li>
<p>术语查询使用精确的搜索词。空格分隔每个搜索词</p>
</li>
<li>
<p>并且只需要一个词就可以匹配文档。</p>
</li>
<li>
<p>使用引号表示短语匹配。</p>
</li>
</ol>
<p>要使用精确的搜索词进行查询，请输入字段名称，后跟冒号，然后输入以空格分隔的值:</p>
<pre><code class="language-yaml">http.response.status_code:400 401 404
</code></pre>
<p>对于文本字段，无论顺序如何，这都将匹配任何值:</p>
<pre><code class="language-yaml">http.response.body.content.text:quick brown fox
</code></pre>
<p>要查询确切的短语，请在值周围使用引号:</p>
<pre><code class="language-yaml">http.response.body.content.text:&quot;quick brown fox&quot;
</code></pre>
<p>字段名称不是KQL所必需的。如果未提供字段名称，则术语将与索引设置中的默认字段匹配。要跨字段搜索:</p>
<pre><code class="language-yaml">&quot;quick brown fox&quot;
</code></pre>
<h2 id="boolean-queries"><a class="header" href="#boolean-queries">Boolean queries</a></h2>
<p>KQL支持or，and，not。</p>
<p>默认情况下，and 具有比 or 更高的优先级。</p>
<p>要覆盖默认优先级，请在括号中对运算符进行分组。这些运算符可以是大写或小写的。</p>
<pre><code class="language-yaml">response:200 or extension:php
</code></pre>
<pre><code class="language-yaml">response:(200 or 404)
</code></pre>
<pre><code class="language-yaml">response:200 and (extension:php or extension:css)
</code></pre>
<pre><code class="language-yaml">response:200 and extension:php or extension:css
</code></pre>
<pre><code class="language-yaml">not response:200
</code></pre>
<pre><code class="language-yaml">response:200 and not (extension:php or extension:css)
</code></pre>
<pre><code class="language-yaml">tags:(success and info and security)
</code></pre>
<h2 id="range-queries"><a class="header" href="#range-queries">Range queries</a></h2>
<p>KQL supports <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;</code>, and <code>&lt;=</code> on numeric and date types.</p>
<pre><code class="language-yaml">account_number &gt;= 100 and items_sold &lt;= 200
</code></pre>
<h2 id="date-range-queries"><a class="header" href="#date-range-queries">Date range queries</a></h2>
<pre><code class="language-yaml">@timestamp &lt; &quot;2021-01-02T21:55:59&quot;
</code></pre>
<pre><code class="language-yaml">@timestamp &lt; &quot;2021-01&quot;
</code></pre>
<pre><code class="language-yaml">@timestamp &lt; &quot;2021&quot;
</code></pre>
<h2 id="exist-queries"><a class="header" href="#exist-queries">Exist queries</a></h2>
<pre><code class="language-yaml">response:*
</code></pre>
<h2 id="wildcard-queries"><a class="header" href="#wildcard-queries">Wildcard queries</a></h2>
<p>通配符查询可用于按术语前缀搜索或搜索多个字段。</p>
<p>The default settings of Kibana <strong>prevent leading wildcards</strong> for performance reasons, but this can be allowed with an <a href="https://www.elastic.co/guide/en/kibana/7.16/advanced-options.html#query-allowleadingwildcards">advanced setting</a>.</p>
<p>To match documents where <code>machine.os</code> starts with <code>win</code>, such as &quot;windows 7&quot; and &quot;windows 10&quot;:</p>
<pre><code class="language-yaml">machine.os:win*
</code></pre>
<pre><code class="language-yaml">machine.os*:windows 10
</code></pre>
<p>当您具有字段的文本和关键字版本时，此语法非常方便。该查询检查术语为windows 10的machine.os和machine.os.关键字。</p>
<h2 id="nested-field-queries"><a class="header" href="#nested-field-queries">Nested field queries</a></h2>
<p>查询嵌套字段的主要考虑因素是如何将嵌套查询的部分与单个嵌套文档进行匹配。您可以:</p>
<ul>
<li>仅将查询的部分与单个嵌套文档匹配。这是大多数用户在嵌套字段上查询时想要的。</li>
<li>将查询的部分与不同的嵌套文档进行匹配。这就是常规对象字段的工作方式。此查询通常不如匹配单个文档有用。</li>
</ul>
<p>在下面的文档中，items是一个嵌套字段。嵌套字段中的每个文档都包含名称，股票和类别。</p>
<pre><code class="language-json">{
  &quot;grocery_name&quot;: &quot;Elastic Eats&quot;,
  &quot;items&quot;: [
    {
      &quot;name&quot;: &quot;banana&quot;,
      &quot;stock&quot;: &quot;12&quot;,
      &quot;category&quot;: &quot;fruit&quot;
    },
    {
      &quot;name&quot;: &quot;peach&quot;,
      &quot;stock&quot;: &quot;10&quot;,
      &quot;category&quot;: &quot;fruit&quot;
    },
    {
      &quot;name&quot;: &quot;carrot&quot;,
      &quot;stock&quot;: &quot;9&quot;,
      &quot;category&quot;: &quot;vegetable&quot;
    },
    {
      &quot;name&quot;: &quot;broccoli&quot;,
      &quot;stock&quot;: &quot;5&quot;,
      &quot;category&quot;: &quot;vegetable&quot;
    }
  ]
}
</code></pre>
<h3 id="match-a-single-document"><a class="header" href="#match-a-single-document">Match a single document</a></h3>
<p>To match stores that have more than 10 bananas in stock:</p>
<pre><code class="language-yaml">items:{ name:banana and stock &gt; 10 }
</code></pre>
<p>items是嵌套路径。花括号 (嵌套组) 内的所有内容都必须与单个嵌套文档匹配。</p>
<p>以下查询不返回任何匹配项，因为没有单个嵌套文档具有库存为9的香蕉。</p>
<pre><code class="language-yaml">items:{ name:banana and stock:9 }
</code></pre>
<h3 id="match-different-documents"><a class="header" href="#match-different-documents">Match different documents</a></h3>
<p>以下子查询位于单独的嵌套组中，可以匹配不同的嵌套文档:</p>
<pre><code class="language-yaml">items:{ name:banana } and items:{ stock:9 }
</code></pre>
<p>名称: banana匹配数组中的第一个文档，stock:9匹配数组中的第三个文档。</p>
<h3 id="match-single-and-different-documents"><a class="header" href="#match-single-and-different-documents">Match single and different documents</a></h3>
<pre><code class="language-yaml">items:{ name:banana and stock &gt; 10 } and items:{ category:vegetable }
</code></pre>
<p>The first nested group (<code>name:banana and stock &gt; 10</code>) must match a single document, but the <code>category:vegetables</code> subquery can match a different nested document because it is in a separate group.</p>
<h3 id="nested-fields-inside-other-nested-fields"><a class="header" href="#nested-fields-inside-other-nested-fields">Nested fields inside other nested fields</a></h3>
<p>KQL支持其他嵌套字段内部的嵌套字段-您必须指定完整路径。在本文档中，level1和level2是嵌套字段:</p>
<pre><code class="language-json">{
  &quot;level1&quot;: [
    {
      &quot;level2&quot;: [
        {
          &quot;prop1&quot;: &quot;foo&quot;,
          &quot;prop2&quot;: &quot;bar&quot;
        },
        {
          &quot;prop1&quot;: &quot;baz&quot;,
          &quot;prop2&quot;: &quot;qux&quot;
        }
      ]
    }
  ]
}
</code></pre>
<pre><code class="language-yaml">level1.level2:{ prop1:foo and prop2:bar }
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h3 id="疑问"><a class="header" href="#疑问">疑问</a></h3>
<ul>
<li>没有 data stream 的时候，如何管理时序型数据？</li>
<li>什么是 data stream？</li>
<li>data stream 的特点有哪些？</li>
<li>为什么要有 data stream？</li>
<li>data stream 能做什么？</li>
<li>data stream 应用场景？</li>
<li>data stream 和 索引 index 的关系？</li>
<li>data stream 和 索引生命周期管理 ILM 的关系？</li>
<li>data stream 实操有哪些注意事项？</li>
</ul>
<h3 id="没有-data-stream-的时候如何管理时序型数据"><a class="header" href="#没有-data-stream-的时候如何管理时序型数据">没有 data stream 的时候，如何管理时序型数据？</a></h3>
<h4 id="基于-rollover-滚动索引机制管理时序数据"><a class="header" href="#基于-rollover-滚动索引机制管理时序数据">基于 rollover 滚动索引机制管理时序数据</a></h4>
<pre><code>PUT mylogs-2021.07.24-1
{
  &quot;aliases&quot;: {
    &quot;mylogs_write&quot;: {}
  }
}

GET mylogs-2021.07.24-1

PUT mylogs_write/_doc/1
{
  &quot;message&quot;: &quot;a dummy log&quot;
}

POST mylogs_write/_bulk
{&quot;index&quot;:{&quot;_id&quot;:4}}
{&quot;title&quot;:&quot;test 04&quot;}
{&quot;index&quot;:{&quot;_id&quot;:2}}
{&quot;title&quot;:&quot;test 02&quot;}
{&quot;index&quot;:{&quot;_id&quot;:3}}
{&quot;title&quot;:&quot;test 03&quot;}


POST  mylogs_write/_rollover
{
  &quot;conditions&quot;: {
    &quot;max_docs&quot;:   3
  }
}


再次导入批量数据

POST mylogs_write/_doc/14
{&quot;title&quot;:&quot;test 14&quot;}

POST mylogs_write/_bulk
{&quot;index&quot;:{&quot;_id&quot;:5}}
{&quot;title&quot;:&quot;test 05&quot;}
{&quot;index&quot;:{&quot;_id&quot;:6}}
{&quot;title&quot;:&quot;test 06&quot;}
{&quot;index&quot;:{&quot;_id&quot;:7}}
{&quot;title&quot;:&quot;test 07&quot;}
{&quot;index&quot;:{&quot;_id&quot;:8}}
{&quot;title&quot;:&quot;test 08&quot;}
{&quot;index&quot;:{&quot;_id&quot;:9}}
{&quot;title&quot;:&quot;test 09&quot;}
{&quot;index&quot;:{&quot;_id&quot;:10}}
{&quot;title&quot;:&quot;test 10&quot;}
</code></pre>
<p>早期生产环境使用 rollover，有个比较<strong>麻烦</strong>的地方就在于——需要自己结合滚动的三条件，在给定的时间点（比如凌晨0:00）定时脚本执行一下 rollover，滚动才能生效。</p>
<p>看似脚本处理很简单，实际会有这样那样的问题，用过你就知道有多苦。</p>
<ul>
<li>rollover 优点：实现了最原始的索引滚动。</li>
<li>rollover 缺点：需要手动或者脚本定时 rollover 非常麻烦。</li>
</ul>
<h3 id="ilm-索引生命周期管理时序数据"><a class="header" href="#ilm-索引生命周期管理时序数据">ILM 索引生命周期管理时序数据</a></h3>
<p>ILM 是模板、别名、生命周期 policy 的综合体。</p>
<ul>
<li>ILM 优点：一次配置，索引生命周期全自动化。</li>
<li>ILM 适用场景：更适合和冷热集群架构结合的业务场景。</li>
<li>ILM 缺点：ILM是普适的概念，强调大而全，不是专门针对时序数据特点的方案，且需要为 ilm 配置 index.lifecycle.rollover_alias 设置（对时序数据场景，这非常麻烦）。</li>
</ul>
<p>官方强调：别名在 Elasticsearch 中的实现方式存在一些不足（官方没有细说哪些不足。我实战环境发现：一个别名对应多个索引，一个索引对应多个别名，索引滚动关联别名也可能滚动，开发者可能很容易出错和混淆），使用起来很混乱。</p>
<p>相比于别名具有广泛的用途，而数据流将是针对时序数据的解决方案。</p>
<h3 id="什么是-data-stream"><a class="header" href="#什么是-data-stream">什么是 data stream？</a></h3>
<p>存储时序数据的多个索引的抽象集合，简称为：数据流（data stream）</p>
<p>数据流可以跨多个后备索引存储仅追加（append-only，下文有详细解释）的时间序列数据，同时对外提供一个同一访问入口。</p>
<p>所以，它是索引、模板、rollover、ilm 基于时序性数据的综合产物。</p>
<h3 id="data-stream-的特点有哪些"><a class="header" href="#data-stream-的特点有哪些">data stream 的特点有哪些？</a></h3>
<h4 id="关联后备支撑索引backing-indices"><a class="header" href="#关联后备支撑索引backing-indices">关联后备支撑索引（backing indices）</a></h4>
<h4 id="timestamp-字段不可缺"><a class="header" href="#timestamp-字段不可缺">@timestamp 字段不可缺</a></h4>
<ul>
<li>每个写入到 dataSteam 的文档必须包含 @timestamp 字段。</li>
<li>@timestamp 字段必须是：date 类型（若不指定，默认：date 类型）或者 date_nanos 类型。</li>
</ul>
<h4 id="data-stream-后备索引规范"><a class="header" href="#data-stream-后备索引规范">data stream 后备索引规范</a></h4>
<p><code>.ds-&lt;data-stream&gt;-&lt;yyyy.MM.dd&gt;-&lt;generation&gt;</code></p>
<p>举例索引真实名称：data-stream-2021.07.25-000001。</p>
<ul>
<li>.ds：前缀开头不可少。</li>
<li>data-stream： 自定义的数据流的名称。</li>
<li>yyyy.MM.dd：日期格式</li>
<li>generation：rollover 累积值：—— 默认从：000001 开始。</li>
</ul>
<h4 id="append-only-仅追加"><a class="header" href="#append-only-仅追加">Append-only 仅追加</a></h4>
<p>仅追加：指只支持 <strong>op_type=create</strong> 的索引请求，我理解的是仅支持向后追加（区别于对历史数据的删除、更新操作）。</p>
<p>数据流只支持：update_by_query 和 delete_by_query 实现批量操作，单条文档的更新和删除操作只能通过指定后备索引的方式实现。</p>
<p>对于频繁更新或者删除文档的业务场景，用 data stream 不合适，而相反的，使用：模板+别名+ILM更为合适。</p>
<h3 id="为什么要有-data-stream"><a class="header" href="#为什么要有-data-stream">为什么要有 data stream？</a></h3>
<p>原有实现由于别名的缺陷实现不了时序数据的管理或实现起来会繁琐、麻烦，data stream 是更为纯粹的存储仅追加时序数据的方式。</p>
<h3 id="data-stream-能做什么"><a class="header" href="#data-stream-能做什么">data stream 能做什么？</a></h3>
<ul>
<li>data stream 支持直接的写入、查询请求。</li>
<li>data stream 会自动将客户端请求路由至关联索引，以用来存储流式数据。</li>
<li>可以使用索引生命周期管理 ILM 自动管理这些关联索引。</li>
</ul>
<h3 id="data-stream-的适用场景"><a class="header" href="#data-stream-的适用场景">data stream 的适用场景</a></h3>
<p>日志（logs）、事件（events）、指标（metrics）和其他持续生成的数据。</p>
<h3 id="data-stream-和-模板的关系"><a class="header" href="#data-stream-和-模板的关系">data stream 和 模板的关系？</a></h3>
<p>相同的索引模板可以用来支撑多个 data streams。可以类比为：1：N 关系。</p>
<h3 id="data-stream--和-ilm-的关系"><a class="header" href="#data-stream--和-ilm-的关系">data stream  和 ilm 的关系？</a></h3>
<p>ILM 在 data stream 中起到索引生命周期管理的作用。</p>
<p>data stream 操作时序数据优势体现在：不再需要为 ilm 配置 index.lifecycle.rollover_alias。</p>
<h3 id="data-stream-实操指南"><a class="header" href="#data-stream-实操指南">data stream 实操指南</a></h3>
<h4 id="创建索引生命周期-policy"><a class="header" href="#创建索引生命周期-policy">创建索引生命周期 policy。</a></h4>
<pre><code>
PUT _ilm/policy/my-lifecycle-policy
{
  &quot;policy&quot;: {
    &quot;phases&quot;: {
      &quot;hot&quot;: {
        &quot;actions&quot;: {
          &quot;rollover&quot;: {
            &quot;max_primary_shard_size&quot;: &quot;50gb&quot;,
            &quot;max_docs&quot;: 10
          }
        }
      },
      &quot;warm&quot;: {
        &quot;min_age&quot;: &quot;1m&quot;,
        &quot;actions&quot;: {
          &quot;shrink&quot;: {
            &quot;number_of_shards&quot;: 1
          },
          &quot;forcemerge&quot;: {
            &quot;max_num_segments&quot;: 1
          },
          &quot;allocate&quot;: {
            &quot;include&quot;: {
              &quot;nodeType&quot;:&quot;warm&quot;
            }
          }
        }
      },
      &quot;cold&quot;: {
        &quot;min_age&quot;: &quot;2m&quot;,
        &quot;actions&quot;: {
          &quot;allocate&quot;: {
            &quot;include&quot;: {
              &quot;nodeType&quot;:&quot;cold&quot;
            }
          }
        }
      },
      &quot;delete&quot;: {
        &quot;min_age&quot;: &quot;4m&quot;,
        &quot;actions&quot;: {
          &quot;delete&quot;: {}
        }
      }
    }
  }
}

</code></pre>
<h4 id="创建模板"><a class="header" href="#创建模板">创建模板</a></h4>
<pre><code>PUT _component_template/my-mappings
{
  &quot;template&quot;: {
    &quot;mappings&quot;: {
      &quot;properties&quot;: {
        &quot;@timestamp&quot;: {
          &quot;type&quot;: &quot;date&quot;,
          &quot;format&quot;: &quot;date_optional_time||epoch_millis&quot;
        },
        &quot;message&quot;: {
          &quot;type&quot;: &quot;wildcard&quot;
        }
      }
    }
  }
}

# Creates a component template for index settings
PUT _component_template/my-settings
{
  &quot;template&quot;: {
    &quot;settings&quot;: {
      &quot;index.lifecycle.name&quot;: &quot;my-lifecycle-policy&quot;
    }
  }
}


PUT _index_template/my-index-template
{
  &quot;index_patterns&quot;: [&quot;my-data-stream*&quot;],
  &quot;data_stream&quot;: { },
  &quot;composed_of&quot;: [ &quot;my-mappings&quot;, &quot;my-settings&quot; ],
  &quot;priority&quot;: 500
}
</code></pre>
<h3 id="创建-data-stream"><a class="header" href="#创建-data-stream">创建 data stream</a></h3>
<ul>
<li>方式一：直接创建数据流 my-data-stream。</li>
</ul>
<p><code>PUT _data_stream/my-data-stream</code> </p>
<ul>
<li>方式二：直接批量或者逐个导入数据（会间接生成 data stream 的创建）。</li>
</ul>
<pre><code>PUT my-data-stream/_bulk
{ &quot;create&quot;:{ } }
{ &quot;@timestamp&quot;: &quot;2099-05-06T16:21:15.000Z&quot;, &quot;message&quot;: &quot;192.0.2.42 - - [06/May/2099:16:21:15 +0000] \&quot;GET /images/bg.jpg HTTP/1.0\&quot; 200 24736&quot; }
{ &quot;create&quot;:{ } }
{ &quot;@timestamp&quot;: &quot;2099-05-06T16:25:42.000Z&quot;, &quot;message&quot;: &quot;192.0.2.255 - - [06/May/2099:16:25:42 +0000] \&quot;GET /favicon.ico HTTP/1.0\&quot; 200 3638&quot; }

POST my-data-stream/_doc
{
  &quot;@timestamp&quot;: &quot;2099-05-06T16:21:15.000Z&quot;,
  &quot;message&quot;: &quot;192.0.2.42 - - [06/May/2099:16:21:15 +0000] \&quot;GET /images/bg.jpg HTTP/1.0\&quot; 200 24736&quot;
}
</code></pre>
<ul>
<li>第一：批量 bulk 操作，必须使用：create 指令，而非 index（使用 index 不会报错， 会把流当做索引处理了）。</li>
<li>第二：文档必须包含：@timestamp  时间戳字段。</li>
</ul>
<p>如果不包含 @timestamp 会报错如下：</p>
<p>&quot;reason&quot; : &quot;data stream timestamp field [@timestamp] is missing&quot; </p>
<h3 id="data-stream-删"><a class="header" href="#data-stream-删">data stream 删</a></h3>
<pre><code>DELETE _data_stream/my-data-stream
</code></pre>
<h3 id="单条删除文档"><a class="header" href="#单条删除文档">单条删除文档</a></h3>
<pre><code>DELETE data-stream-2021.07.25-000001/_doc/1 
</code></pre>
<h3 id="批量删除文档"><a class="header" href="#批量删除文档">批量删除文档</a></h3>
<pre><code>POST /my-data-stream/_delete_by_query
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;user.id&quot;: &quot;vlb44hny&quot;
    }
  }
}
</code></pre>
<h3 id="data-stream-改"><a class="header" href="#data-stream-改">data stream 改</a></h3>
<pre><code># 插入一条数据
POST my-data-stream/_bulk
{&quot;create&quot;:{&quot;_id&quot;:1}}
{&quot;@timestamp&quot;:&quot;2099-05-06T16:21:15.000Z&quot;,&quot;message&quot;:&quot;192.0.2.42 - - [06/May/2099:16:21:15 +0000] \&quot;GET /images/bg.jpg HTTP/1.0\&quot; 200 24736&quot;}

# 获取数据流关联索引
GET /_data_stream/my-data-stream

# 执行更新
PUT .ds-my-data-stream-2021.07.25-000001/_doc/1?if_seq_no=1&amp;if_primary_term=1
{
  &quot;@timestamp&quot;: &quot;2099-03-08T11:06:07.000Z&quot;,
  &quot;user&quot;: {
    &quot;id&quot;: &quot;8a4f500d&quot;
  },
  &quot;message&quot;: &quot;Login successful&quot;
}

# 查看验证是否已经更新（已经验证，可以更新）
GET .ds-my-data-stream-2021.07.25-000001/_doc/1
</code></pre>
<h3 id="批量更新"><a class="header" href="#批量更新">批量更新</a></h3>
<pre><code>POST /my-data-stream/_update_by_query
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;user.id&quot;: &quot;l7gk7f82&quot;
    }
  },
  &quot;script&quot;: {
    &quot;source&quot;: &quot;ctx._source.user.id = params.new_id&quot;,
    &quot;params&quot;: {
      &quot;new_id&quot;: &quot;XgdX0NoX&quot;
    }
  }
}
</code></pre>
<h3 id="data-stream-查"><a class="header" href="#data-stream-查">data stream 查</a></h3>
<p><code>GET _data_stream/my-data-stream</code> </p>
<pre><code>{
  &quot;data_streams&quot; : [
    {
      &quot;name&quot; : &quot;my-data-stream&quot;,
      &quot;timestamp_field&quot; : {
        &quot;name&quot; : &quot;@timestamp&quot;
      },
      &quot;indices&quot; : [
        {
          &quot;index_name&quot; : &quot;.ds-my-data-stream-2021.07.25-000001&quot;,
          &quot;index_uuid&quot; : &quot;Akg3-bWgStiKG_39Tk5PRw&quot;
        }
      ],
      &quot;generation&quot; : 1,
      &quot;status&quot; : &quot;GREEN&quot;,
      &quot;template&quot; : &quot;my-index-template&quot;,
      &quot;ilm_policy&quot; : &quot;my-lifecycle-policy&quot;,
      &quot;hidden&quot; : false
    }
  ]
}
</code></pre>
<h3 id="reindex-操作"><a class="header" href="#reindex-操作">reindex 操作</a></h3>
<pre><code>POST /_reindex
{
  &quot;source&quot;: {
    &quot;index&quot;: &quot;archive&quot;
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;my-data-stream&quot;,
    &quot;op_type&quot;: &quot;create&quot;
  }
}
</code></pre>
<h3 id="滚动操作"><a class="header" href="#滚动操作">滚动操作</a></h3>
<pre><code>POST my-data-stream/_rollover 
</code></pre>
<h3 id="查看-data-stream-基础信息"><a class="header" href="#查看-data-stream-基础信息">查看 data stream 基础信息</a></h3>
<pre><code>GET /_data_stream/my-data-stream 
</code></pre>
<h2 id="data-streams"><a class="header" href="#data-streams">Data streams</a></h2>
<ol>
<li>
<p>数据流使您可以跨多个索引存储仅追加的时间序列数据，同时为您提供用于请求的单个命名资源。数据流非常适合日志、事件、度量和其他连续生成的数据。</p>
</li>
<li>
<p>您可以直接向数据流提交索引和搜索请求</p>
</li>
<li>
<p>流自动将请求路由到存储流数据的支持索引。</p>
</li>
<li>
<p>您可以使用索引生命周期管理 (ILM) 来自动化这些支持索引的管理。</p>
</li>
<li>
<p>例如，您可以使用ILM自动将较旧的备份索引移动到较便宜的硬件并删除不需要的索引。随着数据的增长，ILM可以帮助您降低成本和开销。</p>
</li>
</ol>
<h3 id="backing-indices"><a class="header" href="#backing-indices">Backing indices</a></h3>
<ol>
<li>数据流由一个或多个<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-streams.html#backing-indices:%7E:text=one%20or%20more-,hidden,-%2C%20auto%2Dgenerated%20backing">隐藏</a>的自动生成的后备索引组成。</li>
<li>数据流需要匹配一个<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-templates.html">索引模板</a>。模板包含用于配置 后备索引的mappings and settings 。</li>
<li>索引到数据流的每个文档都必须包含一个 @ timestamp字段，映射为日期或date_nanos字段类型。如果索引模板没有为 @ timestamp字段指定映射，Elasticsearch将 @ timestamp映射为具有默认选项的日期字段。</li>
<li>相同的索引模板可以用于多个数据流。不能删除数据流使用的索引模板。</li>
</ol>
<h3 id="read-requests"><a class="header" href="#read-requests">Read requests</a></h3>
<p>当您向数据流提交读取请求时，该流将请求路由到其所有的后备索引。</p>
<h3 id="write-index"><a class="header" href="#write-index">Write index</a></h3>
<p>最近创建的备份索引是数据流的写入索引。流仅将新文档添加到此索引。</p>
<p>即使直接向索引发送请求，也无法将新文档添加到其他支持索引。</p>
<p>您也不能对可能阻碍索引的写索引执行操作，例如:·</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-clone-index.html">Clone</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-delete-index.html">Delete</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/freeze-index-api.html">Freeze</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-shrink-index.html">Shrink</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-split-index.html">Split</a></li>
</ul>
<h3 id="rollover"><a class="header" href="#rollover">Rollover</a></h3>
<ol>
<li>
<p>A <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-rollover-index.html">rollover</a> creates a new backing index that becomes the stream’s new write index.</p>
</li>
<li>
<p>We recommend using <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html">ILM</a> to automatically roll over data streams when the write index reaches a specified age or size. </p>
</li>
</ol>
<p>If needed, you can also <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/use-a-data-stream.html#manually-roll-over-a-data-stream">manually roll over</a> a data stream.</p>
<h3 id="generation"><a class="header" href="#generation">Generation</a></h3>
<p>每个数据流跟踪其生成: 一个六位数、零填充的整数，作为流的滚动的累积计数，从000001开始。</p>
<p>创建支持索引时，索引将使用以下约定命名:</p>
<pre><code>.ds-&lt;data-stream&gt;-&lt;yyyy.MM.dd&gt;-&lt;generation&gt;
</code></pre>
<p>&lt;yyyy.MM.dd&gt; 是支持索引的创建日期。具有较高的代的支持索引包含较新的数据。例如， <code>web-server-logs</code> 数据流具有生成34。2099年3月7日创建的流的最新后备索引被命名。<code>ds-web-server-logs-2099.03.07-000034。</code></p>
<p>某些操作 (例如收缩或还原) 可以更改支持索引的名称。这些名称更改不会从其数据流中删除后备索引。</p>
<h3 id="append-only"><a class="header" href="#append-only">Append-only</a></h3>
<p>Data streams are designed for use cases where existing data is rarely, if ever, updated. You cannot send update or deletion requests for existing documents directly to a data stream. Instead, use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/use-a-data-stream.html#update-docs-in-a-data-stream-by-query">update by query</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/use-a-data-stream.html#delete-docs-in-a-data-stream-by-query">delete by query</a> APIs.</p>
<p>If needed, you can <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/use-a-data-stream.html#update-delete-docs-in-a-backing-index">update or delete documents</a> by submitting requests directly to the document’s backing index.</p>
<p><strong>注意</strong></p>
<p>If you frequently update or delete existing documents, use an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-add-alias.html">index alias</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-templates.html">index template</a> instead of a data stream. You can still use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html">ILM</a> to manage indices for the alias.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="什么是堆内存"><a class="header" href="#什么是堆内存">什么是<a href="https://so.csdn.net/so/search?q=%E5%A0%86%E5%86%85%E5%AD%98">堆内存</a></a></h2>
<p>Java 中的堆是 <a href="https://so.csdn.net/so/search?q=JVM&amp;spm=1001.2101.3001.7020">JVM</a> 所管理的最大的一块内存空间，主要用于存放各种类的实例对象。</p>
<p>在 Java 中，堆被划分成两个不同的区域：</p>
<ul>
<li>新生代 ( Young )、</li>
<li>老年代 ( Old )。</li>
</ul>
<p>新生代 ( Young ) 又被划分为三个区域：</p>
<ul>
<li>Eden、</li>
<li>From Survivor、</li>
<li>To Survivor。</li>
</ul>
<p>这样划分的目的是为了使 JVM 能够更好的管理堆内存中的对象，包括<a href="https://so.csdn.net/so/search?q=%E5%86%85%E5%AD%98&amp;spm=1001.2101.3001.7020">内存</a>的分配以及回收。</p>
<h2 id="堆内存的作用是什么"><a class="header" href="#堆内存的作用是什么">堆内存的作用是什么？</a></h2>
<p>在虚拟机启动时创建。</p>
<p>堆内存的<strong>唯一目的就是创建对象实例</strong>，所有的对象实例和数组都要在堆上分配。</p>
<p>堆是由垃圾回收来负责的，因此也叫做“GC堆”，垃圾回收采用分代算法，</p>
<p>堆由此分为新生代和老年代。</p>
<p><strong>堆的优势是可以动态地分配内存大小</strong>，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，Java的垃圾收集器会自动收走这些不再使用的数据。</p>
<p>但<strong>缺点</strong>是，由于要在运行时动态分配内存，存取速度较慢。当堆内存因为满了无法扩展时就会抛出java.lang.OutOfMemoryError:Java heap space异常。出现这种情况的解决办法具体参见java调优。</p>
<h2 id="堆内存如何配置"><a class="header" href="#堆内存如何配置">堆内存如何配置？</a></h2>
<p>默认情况下，<a href="https://so.csdn.net/so/search?q=Elasticsearch&amp;spm=1001.2101.3001.7020">Elasticsearch</a> JVM使用堆内存最小和最大大小为2 GB（5.X版本以上）。</p>
<p>早期版本默认1GB，官网指出：这明显不够。</p>
<p>在转移到生产环境时，配置足够容量的堆大小以确保Elasticsearch功能和性能是必要的。</p>
<p>Elasticsearch将通过Xms（最小堆大小）和Xmx（最大堆大小）设置来分配jvm.options中指定的整个堆。</p>
<pre><code>-Xms2g 
-Xmx2g
</code></pre>
<p><strong>通过环境变量设置。</strong></p>
<pre><code>ES_JAVA_OPTS=&quot;-Xms2g -Xmx2g&quot; ./bin/elasticsearch 
ES_JAVA_OPTS=&quot;-Xms4000m -Xmx4000m&quot; ./bin/elasticsearch

</code></pre>
<h2 id="堆内存配置建议"><a class="header" href="#堆内存配置建议">堆内存配置建议</a></h2>
<ul>
<li>将最小堆大小（Xms）和最大堆大小（Xmx）设置为彼此相等。</li>
<li>Elasticsearch可用的堆越多，可用于缓存的内存就越多。但请注意，太多的堆内存可能会使您长时间垃圾收集暂停。</li>
<li><strong>将Xmx设置为不超过物理内存的50％，以确保有足够的物理内存留给内核文件系统缓存。</strong></li>
</ul>
<p><strong>- 不要将Xmx设置为JVM超过32GB。</strong></p>
<pre><code>宿主机内存大小的一半和31GB，取最小值。
</code></pre>
<h2 id="堆内存为什么不能超过物理机内存的一半"><a class="header" href="#堆内存为什么不能超过物理机内存的一半">堆内存为什么不能超过物理机内存的一半？</a></h2>
<h3 id="堆对于elasticsearch绝对重要"><a class="header" href="#堆对于elasticsearch绝对重要"><strong>堆对于Elasticsearch绝对重要</strong>。</a></h3>
<p>它被许多内存数据结构用来提供快速操作。但还有另外一个非常重要的内存使用者：Lucene。</p>
<ol>
<li>
<p>Lucene旨在利用底层操作系统来缓存内存中的数据结构</p>
</li>
<li>
<p>Lucene段(segment)存储在单个文件中。因为段是一成不变的，所以这些文件永远不会改变</p>
</li>
<li>
<p>这使得它们非常容易缓存，并且底层操作系统将愉快地将热段（hot segments）保留在内存中以便更快地访问</p>
</li>
<li>
<p>这些段包括倒排索引（用于全文搜索）和dov values（用于聚合）。</p>
</li>
</ol>
<h3 id="lucene与操作系统缓存"><a class="header" href="#lucene与操作系统缓存"><strong>Lucene与操作系统缓存</strong></a></h3>
<p>Lucene的性能依赖于与操作系统的这种交互。<strong>但是如果你把所有可用的内存都给了Elasticsearch的堆</strong>，<strong>那么Lucene就不会有任何剩余的内存</strong>。这会严重影响性能。</p>
<h3 id="建议"><a class="header" href="#建议"><strong>建议</strong></a></h3>
<p><strong>标准建议是将可用内存的50％提供给Elasticsearch堆，而将其他50％空闲</strong>。它不会被闲置; Lucene会高兴地吞噬掉剩下的东西。</p>
<h3 id="无聚合场景"><a class="header" href="#无聚合场景"><strong>无聚合场景</strong></a></h3>
<p>如果您不在字符串字段上做聚合操作（例如，您不需要fielddata），则可以考虑进一步降低堆。堆越小，您可以从Elasticsearch（更快的GC）和Lucene（更多内存缓存）中获得更好的性能。</p>
<h2 id="堆内存为什么不能超过32gb"><a class="header" href="#堆内存为什么不能超过32gb">堆内存为什么不能超过32GB？</a></h2>
<p>在Java中，所有对象都分配在堆上并由指针引用。普通的对象指针（OOP）指向这些对象，传统上它们是CPU本地字的大小：32位或64位，取决于处理器。</p>
<ol>
<li>
<p>对于32位系统，这意味着最大堆大小为4 GB</p>
</li>
<li>
<p>对于64位系统，堆大小可能会变得更大，<strong>但是64位指针的开销意味着仅仅因为指针较大而存在更多的浪费空间</strong></p>
</li>
<li>
<p>并且比浪费的空间更糟糕，当在主存储器和各种缓存（LLC，L1等等）之间移动值时，较大的指针<strong>消耗更多的带宽</strong>。</p>
</li>
</ol>
<p>Java使用称为压缩oops的技巧来解决这个问题。而不是指向内存中的确切字节位置，指针引用对象偏移量。这意味着一个32位指针可以引用40亿个对象，而不是40亿个字节。最终，这意味着堆可以增长到约32 GB的物理尺寸，同时仍然使用32位指针。</p>
<p><strong>一旦你穿越了这个神奇的〜32 GB的边界</strong>，指针就会切换回普通的对象指针。每个指针的大小增加，使用<strong>更多的CPU内存带宽</strong>，并且实际上会丢失内存。实际上，在使用压缩oops获得32 GB以下堆的相同有效内存之前，需要大约40-50 GB的分配堆。</p>
<p>以上小结为：<strong>即使你有足够的内存空间，尽量避免跨越32GB的堆边界</strong>。否则会导致浪费了内存，降低了CPU的性能，并使GC在大堆中挣扎。</p>
<h2 id="最新认知"><a class="header" href="#最新认知">最新认知</a></h2>
<p>事实上，给ES分配的内存有一个魔法上限值26GB，</p>
<p>这样可以确保启用zero based Compressed Oops，这样性能才是最佳的。</p>
<p>参考:https://elasticsearch.cn/question/3995
https://www.elastic.co/blog/a-heap-of-trouble</p>
<p><img src="elasticSearch/../../images/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dvaml1c2hpd285ODc=,size_16,color_FFFFFF,t_70.png" alt="在这里插入图片描述" /></p>
<p><a href="https://blog.csdn.net/laoyang360/article/details/79998974">参考链接</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="游标查询-scroll"><a class="header" href="#游标查询-scroll">游标查询 <em>Scroll</em></a></h1>
<p><code>scroll</code> 查询 可以用来对 Elasticsearch 有效地执行大批量的文档查询，而又不用付出深度分页那种代价。</p>
<p>游标查询允许我们 先做查询初始化，然后再批量地拉取结果。 这有点儿像传统数据库中的 <em>cursor</em> 。</p>
<p>游标查询会取某个时间点的快照数据。 查询初始化之后索引上的任何变化会被它忽略。 它通过保存旧的数据文件来实现这个特性，结果就像保留初始化时的索引 <em>视图</em> 一样。</p>
<p>深度分页的代价根源是结果集全局排序，如果去掉全局排序的特性的话查询结果的成本就会很低。 游标查询用字段 <code>_doc</code> 来排序。 这个指令让 Elasticsearch 仅仅从还有结果的分片返回下一批结果。</p>
<p>启用游标查询可以通过在查询的时候设置参数 <code>scroll</code> 的值为我们期望的游标查询的过期时间。</p>
<p>游标查询的过期时间会在每次做查询的时候刷新，所以这个时间只需要足够处理当前批的结果就可以了，而不是处理查询结果的所有文档的所需时间</p>
<p>这个过期时间的参数很重要，因为保持这个游标查询窗口需要消耗资源，所以我们期望如果不再需要维护这种资源就该早点儿释放掉。</p>
<p>设置这个超时能够让 Elasticsearch 在稍后空闲的时候自动释放这部分资源。</p>
<pre><code class="language-js">GET /old_index/_search?scroll=1m 
{
    &quot;query&quot;: { &quot;match_all&quot;: {}},
    &quot;sort&quot; : [&quot;_doc&quot;], 
    &quot;size&quot;:  1000
}
</code></pre>
<p>保持游标查询窗口一分钟。</p>
<p>关键字 <code>_doc</code> 是最有效的排序顺序。</p>
<p>这个查询的返回结果包括一个字段 <code>_scroll_id</code>， 它是一个base64编码的长字符串 。 现在我们能传递字段 <code>_scroll_id</code> 到 <code>_search/scroll</code> 查询接口获取下一批结果：</p>
<pre><code class="language-js">GET /_search/scroll
{
    &quot;scroll&quot;: &quot;1m&quot;, 
    &quot;scroll_id&quot; : &quot;cXVlcnlUaGVuRmV0Y2g7NTsxMDk5NDpkUmpiR2FjOFNhNnlCM1ZDMWpWYnRROzEwOTk1OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MTA5OTM6ZFJqYkdhYzhTYTZ5QjNWQzFqVmJ0UTsxMTE5MDpBVUtwN2lxc1FLZV8yRGVjWlI2QUVBOzEwOTk2OmRSamJHYWM4U2E2eUIzVkMxalZidFE7MDs=&quot;
}
</code></pre>
<p>这个游标查询返回的下一批结果。 尽管我们指定字段 <code>size</code> 的值为1000，我们有可能取到超过这个值数量的文档。 当查询的时候， 字段 <code>size</code> 作用于单个分片，所以每个批次实际返回的文档数量最大为 <code>size * number_of_primary_shards</code> 。</p>
<p>注意游标查询每次返回一个新字段 <code>_scroll_id</code>。每次我们做下一次游标查询， 我们必须把前一次查询返回的字段 <code>_scroll_id</code> 传递进去。 当没有更多的结果返回的时候，我们就处理完所有匹配的文档了。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="乐观并发控制"><a class="header" href="#乐观并发控制">乐观并发控制</a></h1>
<h2 id="场景描述"><a class="header" href="#场景描述">场景描述</a></h2>
<p>Elasticsearch 是分布式的。当文档创建、更新或删除时， 新版本的文档必须复制到集群中的其他节点</p>
<p>Elasticsearch 也是异步和并发的，这意味着这些复制请求被并行发送，并且到达目的地时也许 <em>顺序是乱的</em> </p>
<p>Elasticsearch 需要一种方法确保文档的旧版本不会覆盖新的版本。</p>
<h2 id="解决方式版本号"><a class="header" href="#解决方式版本号">解决方式：版本号</a></h2>
<p>每个文档都有一个 <code>_version</code> （版本）号，当文档被修改时版本号递增。 Elasticsearch 使用这个 <code>_version</code> 号来确保变更以正确顺序得到执行。如果旧版本的文档在新版本之后到达，它可以被简单的忽略。</p>
<h2 id="通过外部系统使用版本控制"><a class="header" href="#通过外部系统使用版本控制">通过外部系统使用版本控制</a></h2>
<p><strong>主副数据库</strong></p>
<p>一个常见的设置是使用其它数据库作为主要的数据存储，使用 Elasticsearch 做数据检索</p>
<p><strong>数据同步</strong></p>
<p>这意味着主数据库的所有更改发生时都需要被复制到 Elasticsearch ，如果多个进程负责这一数据同步，你可能遇到类似于之前描述的并发问题。</p>
<p><strong>指定外部版本号</strong></p>
<p>如果你的主数据库已经有了版本号 — 或一个能作为版本号的字段值比如 <code>timestamp</code> — 那么你就可以在 Elasticsearch 中通过增加 <code>version_type=external</code> 到查询字符串的方式重用这些相同的版本号， 版本号必须是大于零的整数， 且小于 <code>9.2E+18</code> — 一个 Java 中 <code>long</code> 类型的正值。</p>
<p><strong>外部版本号的处理</strong></p>
<p>外部版本号的处理方式和我们之前讨论的内部版本号的处理方式有些不同， Elasticsearch 不是检查当前 <code>_version</code> 和请求中指定的版本号是否相同， 而是检查当前 <code>_version</code> 是否 <em>小于</em> 指定的版本号。 如果请求成功，外部的版本号作为文档的新 <code>_version</code> 进行存储。</p>
<p>外部版本号不仅在索引和删除请求是可以指定，而且在 <em>创建</em> 新文档时也可以指定。</p>
<p>例如，要创建一个新的具有外部版本号 <code>5</code> 的博客文章，我们可以按以下方法进行：</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="执行分布式检索"><a class="header" href="#执行分布式检索">执行分布式检索</a></h1>
<p>在继续之前，我们将绕道讨论一下在分布式环境中搜索是怎么执行的。</p>
<p>一个 CRUD 操作只对单个文档进行处理，文档的唯一性由 <code>_index</code>, <code>_type</code>, 和 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/routing-value.html"><code>routing</code> values</a> （通常默认是该文档的 <code>_id</code> ）的组合来确定。 这表示我们确切的知道集群中哪个分片含有此文档。</p>
<p>搜索需要一种更加复杂的执行模型因为我们不知道查询会命中哪些文档:这些文档有可能在集群的任何分片上</p>
<p>但是找到所有的匹配文档仅仅完成事情的一半</p>
<p>在 <code>search</code> 接口返回一个 <code>page</code> 结果之前,多分片中的结果必须组合成单个排序列表</p>
<p>为此，搜索被执行成一个两阶段过程，我们称之为 <em>query then fetch</em> 。</p>
<h2 id="查询阶段"><a class="header" href="#查询阶段">查询阶段</a></h2>
<p>在初始 <em>查询阶段</em> 时， 查询会广播到索引中每一个分片拷贝（主分片或者副本分片）。 每个分片在本地执行搜索并构建一个匹配文档的 <em>优先队列</em>。</p>
<p>一个 <em>优先队列</em> 仅仅是一个存有 <em>top-n</em> 匹配文档的有序列表。优先队列的大小取决于分页参数 <code>from</code> 和 <code>size</code> 。例如，如下搜索请求将需要足够大的优先队列来放入100条文档。</p>
<p>查询阶段包含以下三个步骤:</p>
<ol>
<li>客户端发送一个 搜索请求到 任意某个节点（即协调节点），协调节点 会创建一个大小为 <code>from + size</code> 的空优先队列。</li>
<li>协调节点 将查询请求转发到索引的每个主分片或副本分片中。每个分片在本地执行查询并添加结果到大小为 <code>from + size</code> 的本地有序优先队列中。</li>
<li>每个分片返回各自优先队列中所有文档的 ID 和排序值给协调节点，，它合并这些值到自己的优先队列中来产生一个全局排序后的结果列表。</li>
</ol>
<h2 id="取回阶段"><a class="header" href="#取回阶段">取回阶段</a></h2>
<p>查询阶段标识哪些文档满足搜索请求，但是我们仍然需要取回这些文档。这是取回阶段的任务</p>
<ul>
<li>协调节点辨别出哪些文档需要被取回并向相关的分片提交多个 <code>GET</code> 请求</li>
<li>每个分片加载并 <em>丰富</em> 文档，如果有需要的话，接着返回文档给协调节点。</li>
<li>一旦所有的文档都被取回了，协调节点返回结果给客户端。</li>
</ul>
<p><strong>深分页（Deep Pagination）</strong></p>
<p>先查后取的过程支持用 <code>from</code> 和 <code>size</code> 参数分页，但是这是 <em>有限制的</em> 。 要记住需要传递信息给协调节点的每个分片必须先创建一个 <code>from + size</code> 长度的队列，协调节点需要根据 <code>number_of_shards * (from + size)</code> 排序文档，来找到被包含在 <code>size</code> 里的文档。</p>
<p>取决于你的文档的大小，分片的数量和你使用的硬件，给 10,000 到 50,000 的结果文档深分页（ 1,000 到 5,000 页）是完全可行的。但是使用足够大的 <code>from</code> 值，排序过程可能会变得非常沉重，使用大量的CPU、内存和带宽。因为这个原因，我们强烈建议你不要使用深分页。</p>
<p>实际上， “深分页” 很少符合人的行为。当2到3页过去以后，人会停止翻页，并且改变搜索标准。会不知疲倦地一页一页的获取网页直到你的服务崩溃的罪魁祸首一般是机器人或者web spider。</p>
<p>如果你 <em>确实</em> 需要从你的集群取回大量的文档，你可以通过用 <code>scroll</code> 查询禁用排序使这个取回行为更有效率</p>
<h2 id="搜索选项"><a class="header" href="#搜索选项">搜索选项</a></h2>
<p>有几个 查询参数可以影响搜索过程。</p>
<h2 id="偏好"><a class="header" href="#偏好">偏好</a></h2>
<p>偏好这个参数 <code>preference</code> 允许 用来控制由哪些分片或节点来处理搜索请求。 它接受像 <code>_primary</code>, <code>_primary_first</code>, <code>_local</code>, <code>_only_node:xyz</code>, <code>_prefer_node:xyz</code>, 和 <code>_shards:2,3</code> 这样的值, 这些值在 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/search-request-preference.html">search <code>preference</code></a> 文档页面被详细解释。</p>
<p>但是最有用的值是某些随机字符串，它可以避免 <em>bouncing results</em> 问题。</p>
<p><strong>Bouncing Results</strong></p>
<blockquote>
<p>每次请求有不同的排序</p>
</blockquote>
<p>想象一下有两个文档有同样值的时间戳字段，搜索结果用 <code>timestamp</code> 字段来排序。 由于搜索请求是在所有有效的分片副本间轮询的，那就有可能发生主分片处理请求时，这两个文档是一种顺序， 而副本分片处理请求时又是另一种顺序。</p>
<p>这就是所谓的 <em>bouncing results</em> 问题: 每次用户刷新页面，搜索结果表现是不同的顺序。 让同一个用户始终使用同一个分片，这样可以避免这种问题， 可以设置 <code>preference</code> 参数为一个特定的任意值比如用户会话ID来解决。</p>
<h3 id="超时问题"><a class="header" href="#超时问题">超时问题</a></h3>
<p>通常分片处理完它所有的数据后再把结果返回给协同节点，协同节点把收到的所有结果合并为最终结果。</p>
<p>这意味着花费的时间是最慢分片的处理时间加结果合并的时间。如果有一个节点有问题，就会导致所有的响应缓慢。</p>
<p>参数 <code>timeout</code> 告诉 分片允许处理数据的最大时间。如果没有足够的时间处理所有数据，这个分片的结果可以是部分的，甚至是空数据。</p>
<p>搜索的返回结果会用属性 <code>timed_out</code> 标明分片是否返回的是部分结果：</p>
<h3 id="路由"><a class="header" href="#路由">路由</a></h3>
<p>在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/routing-value.html">路由一个文档到一个分片中</a> 中, 我们解释过如何定制参数 <code>routing</code> ，它能够在索引时提供来确保相关的文档，比如属于某个用户的文档被存储在某个分片上。 在搜索的时候，不用搜索索引的所有分片，而是通过指定几个 <code>routing</code> 值来限定只搜索几个相关的分片：</p>
<pre><code class="language-js">GET /_search?routing=user_1,user2
</code></pre>
<h3 id="搜索类型"><a class="header" href="#搜索类型">搜索类型</a></h3>
<p>缺省的搜索类型是 <code>query_then_fetch</code> 。 在某些情况下，你可能想明确设置 <code>search_type</code> 为 <code>dfs_query_then_fetch</code> 来改善相关性精确度：</p>
<pre><code class="language-js">GET /_search?search_type=dfs_query_then_fetch
</code></pre>
<p>搜索类型 <code>dfs_query_then_fetch</code> 有预查询阶段，这个阶段可以从所有相关分片获取词频来计算全局词频。 我们在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-is-broken.html">被破坏的相关度！</a> 会再讨论它。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="分片内部原理"><a class="header" href="#分片内部原理">分片内部原理</a></h1>
<p>分片是 最小的 <em>工作单元</em> ，但是究竟什么 <em>是</em> 一个分片，它是如何工作的？ 在这个章节，我们回答以下问题:</p>
<ul>
<li>为什么搜索是 NRT （近实时）</li>
<li>为什么 CRUD 是实时的</li>
<li>如何保证断电数据不丢失</li>
<li>optimize、reflush、flush API  都做了什么，在什么情况下使用</li>
</ul>
<h2 id="使文本可搜索"><a class="header" href="#使文本可搜索">使文本可搜索</a></h2>
<blockquote>
<p>第一个要解决的问题是：使文本可搜索</p>
</blockquote>
<p>倒排索引包含一个有序列表，有序列表包含所有文档中出现的不重复的词项，每个词项都包含它所出现文档的位置列表</p>
<pre><code>Term  | Doc 1 | Doc 2 | Doc 3 | ...
------------------------------------
brown |   X   |       |  X    | ...
fox   |   X   |   X   |  X    | ...
quick |   X   |   X   |       | ...
the   |   X   |       |  X    | ...
</code></pre>
<blockquote>
<p>我们讨论倒排索引时，由于历史原因，用来针对整个非结构化文档进行标引，而elasticSearch 是存在结构化的JSON文档，所以，针对elasticSearch 的每一个索引字段 都有自己的倒排索引</p>
</blockquote>
<h3 id="不变性"><a class="header" href="#不变性">不变性</a></h3>
<p>倒排索引写入磁盘后，是不可变的，不变性有价值：</p>
<ol>
<li>无锁、没有数据竞争</li>
<li>增加操作系统缓存性能</li>
<li>利于其他缓存（filter）</li>
<li>存储数据可以压缩</li>
</ol>
<p>当然，一个不变的索引也有不好的地方。主要事实是它是不可变的! 你不能修改它。如果你需要让一个新的文档 可被搜索，你需要重建整个索引。这要么对一个索引所能包含的数据量造成了很大的限制，要么对索引可被更新的频率造成了很大的限制。</p>
<h2 id="动态更新索引"><a class="header" href="#动态更新索引">动态更新索引</a></h2>
<blockquote>
<p>下一个要解决的问题是：如何保证在保证不变性的前提下，更新倒排索引：解决办法是使用更多索引</p>
</blockquote>
<p>通过增加新的倒排索引 来反映 最近的变化。而不是重写整个倒排索引，每个倒排索引都会被查询到，从最早的开始，查询完后对结果集进行合并</p>
<p>Elasticsearch 基于 Lucene, 这个 java 库引入了 <em>按段搜索</em> 的概念：每段本身是一个倒排索引</p>
<p>Luence 还增加了提交点的概念：列出了所有已知段的 文件</p>
<p><strong>一个 Lucene 索引包含一个提交点和三个段</strong></p>
<p><img src="elasticSearch/../../images/elas_1101.png" alt="" /></p>
<p>每个段跟提交点 都是一个文件</p>
<p>逐段搜索会以如下流程进行工作：</p>
<ul>
<li>新文档被收集到内存索引缓存</li>
<li>不时地, 缓存被 <em>提交</em> ：
<ul>
<li>一个新的段—一个追加的倒排索引—被写入磁盘。</li>
<li>一个新的包含新段名字的 <em>提交点</em> 被写入磁盘。</li>
<li>磁盘进行 <em>同步</em> — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件。</li>
</ul>
</li>
<li>新的段被开启，让它包含的文档可见以被搜索。</li>
<li>内存缓存被清空，等待接收新的文档。</li>
</ul>
<p>当一个查询被触发，所有已知的段按顺序被查询。</p>
<p>词项统计会对所有段的结果进行聚合，以保证每个词和每个文档的关联都被准确计算。</p>
<p>这种方式可以用相对较低的成本将新文档添加到索引。</p>
<h3 id="删除和更新"><a class="header" href="#删除和更新">删除和更新</a></h3>
<p>段是不可变的，每个提交点对应一个 <strong>.del</strong>  文件 列出这些被删除的文档的段信息</p>
<p>当一个文件被删除或更新时，会当del文件中标记删除，后续查询仍可以匹配到，但它会咋最终结果被返回前移除</p>
<h2 id="近实时搜索"><a class="header" href="#近实时搜索">近实时搜索</a></h2>
<p><strong>背景</strong></p>
<p>按段搜索 使得一个文档 从 索引到可搜索 的延迟 显著降低，通常需要几分钟，这还不够快。</p>
<p><strong>原因</strong></p>
<p>磁盘在这里成了瓶颈：</p>
<p><em>commit</em> 一个新段到磁盘 需要一次 <em>fsync</em> <em>fsync</em>代价很大，每次索引都 <em>fsync</em>的话会造成性能问题</p>
<p><strong>解决办法</strong></p>
<p>在elasticSearch跟磁盘之间 是 文件系统缓存</p>
<p>在内存索引缓存区中的文档会被先写入到一个新的段中，这里的新段会被先写入到文件系统缓存，这一步代价比较低，稍后再被刷新到磁盘，这一步代价较高</p>
<p>只要文件在缓存中，就可以像其他文件一样被打开读取了</p>
<p><strong>refreshAPI</strong></p>
<p>在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 <em>refresh</em> </p>
<p>默认情况下每个分片会每秒自动刷新一次</p>
<p>这就是为什么我们说 Elasticsearch 是 <em>近</em> 实时搜索</p>
<p>：<strong>文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。</strong></p>
<pre><code class="language-json"># 刷新（Refresh）所有的索引。
POST /_refresh 
# 只刷新（Refresh） `blogs` 索引
POST /blogs/_refresh 
</code></pre>
<p>当你在索引日志上，不关注实时性，但比较关注索引的速度，可以降低 刷新的频率</p>
<pre><code class="language-json">PUT /my_logs
{
  &quot;settings&quot;: {
    &quot;refresh_interval&quot;: &quot;30s&quot; 
  }
}
</code></pre>
<p>也可以在建立一个大型索引时，先关闭自动刷新</p>
<pre><code class="language-json">PUT /my_logs/_settings
{ &quot;refresh_interval&quot;: -1 } 

PUT /my_logs/_settings
{ &quot;refresh_interval&quot;: &quot;1s&quot; } 
</code></pre>
<h2 id="持久化变更"><a class="header" href="#持久化变更">持久化变更</a></h2>
<p>如果没有 <strong>fsync</strong> 将文件系统缓存刷到磁盘， 则不能保证数据在断电甚至是程序正常退出之后仍然存在。</p>
<p>一次完整的提交 会将段刷到磁盘、并写入一个包含所有段列表的提交点，elasticSearch 在启动或重新打开一个索引的过程根据 这个提交点文件 判断哪些段属于当前分片</p>
<p>ElasticSearch 增加了 Translog 的概念，每次操作都会记录TransLog</p>
<p>流程如下：</p>
<ol>
<li>文档被索引、添加到内存缓存区中，并写入 translog</li>
<li>分片每秒刷新（<em>refresh</em>） 
<ol>
<li>内存缓存区的文档写入到新段中，且没有进行 fsync 操作</li>
<li>段被打开、可搜索</li>
<li>内存缓存区被清除</li>
</ol>
</li>
<li>更多的文档被添加到 内存缓冲区和追加到事务日志中</li>
<li>每隔一段时间 索引被 <em>flush</em> 
<ol>
<li>所有内存缓冲区的文档被写入一个新的段</li>
<li>缓冲区被清空</li>
<li>提交点被写入硬盘</li>
<li>文件系统 通过 fsync 刷新</li>
<li>老的 translog 被删除</li>
</ol>
</li>
</ol>
<p>translog 提供所有还没有被刷到磁盘的操作的一个持久化纪录。</p>
<p>当elasticSearch 启动时，会从磁盘中最后一个提交点去恢复已知的段</p>
<p>： 它会重放 translog 中 在这个提交点后发生的变更操作</p>
<h3 id="实时crud"><a class="header" href="#实时crud">实时CRUD</a></h3>
<p>translog也提供 实时CRUD</p>
<p>当你试着通过ID查询、更新、删除一个文档 它会先尝试 检查 translog 任何最近的变更</p>
<h3 id="flushapi"><a class="header" href="#flushapi">flushAPI</a></h3>
<p>执行一次提交、并截断translog 的行为在 elasticsearch 中被称作 一次 <em>flush</em></p>
<p>分片每30分钟自动刷新（flush）</p>
<pre><code class="language-json"># 刷新（flush） blogs 索引。
POST /blogs/_flush 
# 刷新（flush）所有的索引并且并且等待所有刷新在返回前完成。
POST /_flush?wait_for_ongoing 
</code></pre>
<p>你很少需要自己手动执行 <code>flush</code> 操作；通常情况下，自动刷新就足够了。</p>
<p>在重启节点或关闭索引之前执行 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/translog.html#flush-api">flush</a> 有益于你的索引</p>
<p>当 Elasticsearch 尝试恢复或重新打开一个索引， 它需要重放 translog 中所有的操作，<strong>所以如果日志越短，恢复越快。</strong></p>
<h3 id="translog-有多安全"><a class="header" href="#translog-有多安全"><strong>Translog 有多安全?</strong></a></h3>
<p>translog 的目的是保证操作不会丢失。这引出了这个问题： Translog 有多安全？</p>
<p><strong>translog的持久化</strong></p>
<p>默认 translog 是每 5 秒被 <code>fsync</code> 刷新到硬盘， 或者在每次写请求完成之后执行(e.g. index, delete, update, bulk)</p>
<p>这意味着在整个请求被 <code>fsync</code> 到主分片和复制分片的translog之前，你的客户端不会得到一个 200 OK 响应。</p>
<p>在每次请求后都执行一个 fsync 会带来一些性能损失，尽管实践表明这种损失相对较小（特别是bulk导入，它在一次请求中平摊了大量文档的开销）。</p>
<p>但是对于一些大容量的偶尔丢失几秒数据问题也并不严重的集群，使用异步的 fsync 还是比较有益的。比如，写入的数据被缓存到内存中，再每5秒执行一次 <code>fsync</code> 。</p>
<p>这个行为可以通过设置 <code>durability</code> 参数为 <code>async</code> 来启用：</p>
<pre><code class="language-js">PUT /my_index/_settings
{
    &quot;index.translog.durability&quot;: &quot;async&quot;,
    &quot;index.translog.sync_interval&quot;: &quot;5s&quot;
}
</code></pre>
<p>这个选项可以针对索引单独设置，并且可以动态进行修改。如果你决定使用异步 translog 的话，你需要 <em>保证</em> 在发生crash时，丢失掉 <code>sync_interval</code> 时间段的数据也无所谓。请在决定前知晓这个特性。</p>
<p>如果你不确定这个行为的后果，最好是使用默认的参数（ <code>&quot;index.translog.durability&quot;: &quot;request&quot;</code> ）来避免数据丢失。</p>
<h2 id="段合并"><a class="header" href="#段合并">段合并</a></h2>
<p>由于每次 reflush 都会 创建一个新段、这样会导致 短时间内 段数量暴增。段数量暴增会带来较大的麻烦</p>
<ol>
<li>每一个段都会消耗 文件句柄、内存、CPU运行周期</li>
<li>每个搜索请求都必须轮流检查每个段，段越多、搜索也越慢</li>
</ol>
<p>Elasticsearch通过在后台进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。</p>
<p>段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档（或被更新文档的旧版本）不会被拷贝到新的大段中。</p>
<p>合并大的段需要消耗大量的I/O和CPU资源，如果任其发展会影响搜索性能。Elasticsearch在默认情况下会对合并流程进行资源限制</p>
<h3 id="optimize-api"><a class="header" href="#optimize-api">optimize API</a></h3>
<p><code>optimize</code> API大可看做是 <em>强制合并</em> API。它会将一个分片强制合并到 <code>max_num_segments</code> 参数指定大小的段数目。 这样做的意图是减少段的数量（<strong>通常减少到一个</strong>），来提升搜索性能。</p>
<p>在特定情况下，使用 <code>optimize</code> API 颇有益处。例如在日志这种用例下，每天、每周、每月的日志被存储在一个索引中。 老的索引实质上是只读的；它们也并不太可能会发生变化。</p>
<p>在这种情况下，使用optimize优化老的索引，将每一个分片合并为一个单独的段就很有用了；这样既可以节省资源，也可以使搜索更加快速：</p>
<pre><code class="language-json">POST /logstash-2014-10/_optimize?max_num_segments=1 
</code></pre>
<p>请注意，使用 <code>optimize</code> API 触发段合并的操作不会受到任何资源上的限制。这可能会消耗掉你节点上全部的I/O资源, 使其没有余裕来处理搜索请求，从而有可能使集群失去响应。 如果你想要对索引执行 <code>optimize</code>，你需要先使用分片分配（查看 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/retiring-data.html#migrate-indices">迁移旧索引</a>）把索引移到一个安全的节点，再执行。</p>
<p>https://www.jianshu.com/p/cc06f9adbe82</p>
<p>https://cloud.tencent.com/developer/article/1488535</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="倒排索引"><a class="header" href="#倒排索引">倒排索引</a></h1>
<p>Elasticsearch 使用一种称为 <em>倒排索引</em> 的结构，它适用于快速的全文搜索。一个倒排索引由文档中所有不重复词的列表构成，对于其中每个词，有一个包含它的文档列表。</p>
<p>例如，假设我们有两个文档，每个文档的 <code>content</code> 域包含如下内容：</p>
<ol>
<li>The quick brown fox jumped over the lazy dog</li>
<li>Quick brown foxes leap over lazy dogs in summer</li>
</ol>
<p>为了创建倒排索引，我们首先将每个文档的 <code>content</code> 域拆分成单独的 词（我们称它为 <code>词条</code> 或 <code>tokens</code> ），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。结果如下所示：</p>
<pre><code>Term      Doc_1  Doc_2
-------------------------
Quick   |       |  X
The     |   X   |
brown   |   X   |  X
dog     |   X   |
dogs    |       |  X
fox     |   X   |
foxes   |       |  X
in      |       |  X
jumped  |   X   |
lazy    |   X   |  X
leap    |       |  X
over    |   X   |  X
quick   |   X   |
summer  |       |  X
the     |   X   |
------------------------
</code></pre>
<p>现在，如果我们想搜索 <code>quick brown</code> ，我们只需要查找包含每个词条的文档：</p>
<pre><code>Term      Doc_1  Doc_2
-------------------------
brown   |   X   |  X
quick   |   X   |
------------------------
Total   |   2   |  1
</code></pre>
<p>两个文档都匹配，但是第一个文档比第二个匹配度更高。如果我们使用仅计算匹配词条数量的简单 <em>相似性算法</em> ，那么，我们可以说，对于我们查询的相关性来讲，第一个文档比第二个文档更佳。</p>
<p>但是，我们目前的倒排索引有一些问题：</p>
<ul>
<li><code>Quick</code> 和 <code>quick</code> 以独立的词条出现，然而用户可能认为它们是相同的词。</li>
<li><code>fox</code> 和 <code>foxes</code> 非常相似, 就像 <code>dog</code> 和 <code>dogs</code> ；他们有相同的词根。</li>
<li><code>jumped</code> 和 <code>leap</code>, 尽管没有相同的词根，但他们的意思很相近。他们是同义词。</li>
</ul>
<p>使用前面的索引搜索 <code>+Quick +fox</code> 不会得到任何匹配文档。（记住，<code>+</code> 前缀表明这个词必须存在。）只有同时出现 <code>Quick</code> 和 <code>fox</code> 的文档才满足这个查询条件，但是第一个文档包含 <code>quick fox</code> ，第二个文档包含 <code>Quick foxes</code> 。</p>
<p>我们的用户可以合理的期望两个文档与查询匹配。我们可以做的更好。</p>
<p>如果我们将词条规范为标准模式，那么我们可以找到与用户搜索的词条不完全一致，但具有足够相关性的文档。例如：</p>
<ul>
<li><code>Quick</code> 可以小写化为 <code>quick</code> 。</li>
<li><code>foxes</code> 可以 <em>词干提取</em> --变为词根的格式-- 为 <code>fox</code> 。类似的， <code>dogs</code> 可以为提取为 <code>dog</code> 。</li>
<li><code>jumped</code> 和 <code>leap</code> 是同义词，可以索引为相同的单词 <code>jump</code> 。</li>
</ul>
<p>现在索引看上去像这样：</p>
<pre><code>Term      Doc_1  Doc_2
-------------------------
brown   |   X   |  X
dog     |   X   |  X
fox     |   X   |  X
in      |       |  X
jump    |   X   |  X
lazy    |   X   |  X
over    |   X   |  X
quick   |   X   |  X
summer  |       |  X
the     |   X   |  X
------------------------
</code></pre>
<p>这还远远不够。我们搜索 <code>+Quick +fox</code> <em>仍然</em> 会失败，因为在我们的索引中，已经没有 <code>Quick</code> 了。但是，如果我们对搜索的字符串使用与 <code>content</code> 域相同的标准化规则，会变成查询 <code>+quick +fox</code> ，这样两个文档都会匹配！</p>
<p>这非常重要。你只能搜索在索引中出现的词条，所以索引文本和查询字符串必须标准化为相同的格式。</p>
<p>分词和标准化的过程称为 <em>分析</em> ， 我们会在下个章节讨论。</p>
<h1 id="分析"><a class="header" href="#分析">分析</a></h1>
<p><em>分析</em> 包含下面的过程：</p>
<ul>
<li>首先，将一块文本分成适合于倒排索引的独立的 <em>词条</em> ，</li>
<li>之后，将这些词条统一化为标准格式以提高它们的“可搜索性”，或者 <em>recall</em></li>
</ul>
<p>分析器执行上面的工作。 <em>分析器</em> 实际上是将三个功能封装到了一个包里：</p>
<p><strong>字符过滤器</strong></p>
<p>首先，字符串按顺序通过每个 <em>字符过滤器</em> 。他们的任务是在分词前整理字符串。一个字符过滤器可以用来去掉HTML，或者将 <code>&amp;</code> 转化成 <code>and</code>。</p>
<p><strong>分词器</strong></p>
<p>其次，字符串被 <em>分词器</em> 分为单个的词条。一个简单的分词器遇到空格和标点的时候，可能会将文本拆分成词条。</p>
<p><strong>Token 过滤器</strong></p>
<p>最后，词条按顺序通过每个 <em>token 过滤器</em> 。这个过程可能会改变词条（例如，小写化 <code>Quick</code> ），删除词条（例如， 像 <code>a</code>， <code>and</code>， <code>the</code> 等无用词），或者增加词条（例如，像 <code>jump</code> 和 <code>leap</code> 这种同义词）。</p>
<h2 id="内置分析器"><a class="header" href="#内置分析器">内置分析器</a></h2>
<p>但是， Elasticsearch还附带了可以直接使用的预包装的分析器。接下来我们会列出最重要的分析器。为了证明它们的差异，我们看看每个分析器会从下面的字符串得到哪些词条：</p>
<pre><code>&quot;Set the shape to semi-transparent by calling set_trans(5)&quot;
</code></pre>
<ul>
<li>
<p><strong>标准分析器</strong></p>
<p>标准分析器是Elasticsearch默认使用的分析器。它是分析各种语言文本最常用的选择。它根据 <a href="http://www.unicode.org/reports/tr29/">Unicode 联盟</a> 定义的 <em>单词边界</em> 划分文本。删除绝大部分标点。最后，将词条小写。它会产生<code>set, the, shape, to, semi, transparent, by, calling, set_trans, 5</code></p>
</li>
<li>
<p><strong>简单分析器</strong></p>
<p>简单分析器在任何不是字母的地方分隔文本，将词条小写。它会产生<code>set, the, shape, to, semi, transparent, by, calling, set, trans</code></p>
</li>
<li>
<p><strong>空格分析器</strong></p>
<p>空格分析器在空格的地方划分文本。它会产生<code>Set, the, shape, to, semi-transparent, by, calling, set_trans(5)</code></p>
</li>
<li>
<p><strong>语言分析器</strong></p>
<p>特定语言分析器可用于 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-lang-analyzer.html">很多语言</a>。它们可以考虑指定语言的特点。例如， <code>英语</code> 分析器附带了一组英语无用词（常用单词，例如 <code>and</code> 或者 <code>the</code> ，它们对相关性没有多少影响），它们会被删除。 由于理解英语语法的规则，这个分词器可以提取英语单词的 <em>词干</em> 。<code>英语</code> 分词器会产生下面的词条：<code>set, shape, semi, transpar, call, set_tran, 5</code>注意看 <code>transparent</code>、 <code>calling</code> 和 <code>set_trans</code> 已经变为词根格式。</p>
</li>
</ul>
<p><strong>什么时候使用分析器</strong></p>
<ol>
<li>当我们 <em>索引</em> 一个文档，它的全文域被分析成词条以用来创建倒排索引</li>
<li>但是，当我们在全文域 <em>搜索</em> 的时候，我们需要将查询字符串通过 <em>相同的分析过程</em> ，以保证我们搜索的词条格式与索引中的词条格式一致。</li>
</ol>
<p>全文查询，理解每个域是如何定义的，因此它们可以做正确的事：</p>
<ul>
<li>当你查询一个 <em>全文</em> 域时， 会对查询字符串应用相同的分析器，以产生正确的搜索词条列表。</li>
<li>当你查询一个 <em>精确值</em> 域时，不会分析查询字符串，而是搜索你指定的精确值。</li>
</ul>
<p><strong>测试分析器</strong></p>
<pre><code class="language-sense">GET /_analyze
{
  &quot;analyzer&quot;: &quot;standard&quot;,
  &quot;text&quot;: &quot;Text to analyze&quot;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="elasticsearch系列---前缀搜索和模糊搜索"><a class="header" href="#elasticsearch系列---前缀搜索和模糊搜索">Elasticsearch系列---前缀搜索和模糊搜索</a></h2>
<h3 id="前缀搜索"><a class="header" href="#前缀搜索">前缀搜索</a></h3>
<div style="break-before: page; page-break-before: always;"></div><h1 id="分片路由"><a class="header" href="#分片路由">分片路由</a></h1>
<p>当索引一个文档的时候，文档会被存储到一个主分片中</p>
<p><strong>Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？</strong></p>
<pre><code>shard = hash(routing) % number_of_primary_shards
</code></pre>
<p>routing<code>是一个可变值，默认是文档的</code>_id，也可以设置成一个自定义的值</p>
<p><code>routing</code> 通过 hash 函数生成一个数字，然后这个数字再除以 <code>number_of_primary_shards</code> （主分片的数量）后得到 <strong>余数</strong> </p>
<p><strong>主分片的数量不可变</strong></p>
<p>这就解释了为什么我们要在创建索引的时候就确定好主分片的数量 并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。</p>
<p><strong>自定义routing字段</strong></p>
<p>所有的文档 API（ <code>get</code> 、 <code>index</code> 、 <code>delete</code> 、 <code>bulk</code> 、 <code>update</code> 以及 <code>mget</code> ）都接受一个叫做 <code>routing</code> 的路由参数 ，通过这个参数我们可以自定义文档到分片的映射。一个自定义的路由参数可以用来确保所有相关的文档——例如所有属于同一个用户的文档——都被存储到同一个分片中。我们也会在<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/scale.html"><em>扩容设计</em></a>这一章中详细讨论为什么会有这样一种需求。</p>
<h1 id="主分片和副本分片如何交互"><a class="header" href="#主分片和副本分片如何交互">主分片和副本分片如何交互</a></h1>
<h2 id="新建索引和删除单个文档"><a class="header" href="#新建索引和删除单个文档"><strong>新建、索引和删除单个文档</strong></a></h2>
<ol>
<li>客户端向 任意一个节点 发送新建、索引或者删除请求，该节点称为协调节点</li>
<li>节点使用文档的 <code>_id</code> 确定文档属于哪个分片。请求会被转发到 该分片所处的节点</li>
<li>该节点 在该文档的主分片上面执行请求。如果成功了，它将请求并行转发到该索引其他的副本分片上。一旦所有的副本分片都报告成功,将向协调节点报告成功，协调节点向客户端报告成功。</li>
</ol>
<p>在客户端收到成功响应时，文档变更已经在主分片和所有副本分片执行完成，变更是安全的。</p>
<p>有一些可选的请求参数允许您影响这个过程，可能以数据安全为代价提升性能。这些选项很少使用，因为Elasticsearch已经很快，但是为了完整起见，在这里阐述如下：</p>
<p><strong>consistency</strong></p>
<p>一致性：要求 大多数 副本分片处于活跃状态下才 会执行 写操作，是为了避免在发生网络分区故障（network partition）的时候进行_写_操作，进而导致数据不一致</p>
<p>规定数量：</p>
<pre><code>int( (primary + number_of_replicas) / 2 ) + 1
</code></pre>
<p><strong>取值</strong></p>
<p><code>one</code>:只要主分片状态 ok 就允许执行_写_操作</p>
<p><code>all</code>:必须要主分片和所有副本分片的状态没问题才允许执行_写_操作</p>
<p><code>quorum</code>: 即大多数的分片副本状态没问题就允许执行_写_操作。</p>
<p><strong>timeout</strong></p>
<p>如果没有足够的副本分片会发生什么？ Elasticsearch会等待，希望更多的分片出现。默认情况下，它最多等待1分钟。 如果你需要，你可以使用 <code>timeout</code> 参数 使它更早终止： <code>100</code> 100毫秒，<code>30s</code> 是30秒。</p>
<h2 id="取回文档"><a class="header" href="#取回文档">取回文档</a></h2>
<ol>
<li>
<p>客户端向 任意节点 发送获取请求。（即协调节点）</p>
</li>
<li>
<p>节点使用文档的 <code>_id</code> 来确定文档属于分片 <code>0</code> 。由于分片存在副本，轮询到其中一个节点的分片上</p>
</li>
</ol>
<p>3、该节点 将文档返回给 协调节点 ，然后将文档返回给客户端。</p>
<p>在处理读取请求时，协调结点在每次请求的时候都会通过轮询所有的副本分片来达到负载均衡。</p>
<p>在文档被检索时，已经被索引的文档可能已经存在于主分片上但是还没有复制到副本分片。</p>
<p>在这种情况下，副本分片可能会报告文档不存在，但是主分片可能成功返回文档。 一旦索引请求成功返回给用户，文档在主分片和副本分片都是可用的。</p>
<h2 id="局部更新文档"><a class="header" href="#局部更新文档">局部更新文档</a></h2>
<ol>
<li>客户端向 任意节点 发送更新请求。（即协调节点）</li>
<li>它将请求转发到主分片所在的 处理节点 。</li>
<li>处理节点 从主分片检索文档，修改 <code>_source</code> 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 <code>retry_on_conflict</code> 次后放弃。</li>
<li>如果 处理节点 成功地更新文档，它将新版本的文档并行转发到 其他节点上的副本分片，重新建立索引。 一旦所有副本分片都返回成功，处理节点 向协调节点也返回成功，协调节点向客户端返回成功。</li>
</ol>
<p><strong>基于文档的复制</strong></p>
<p>当主分片把更改转发到副本分片时， 它不会转发更新请求。</p>
<p>相反，它转发完整文档的新版本。请记住，这些更改将会异步转发到副本分片，并且不能保证它们以发送它们相同的顺序到达。 如果Elasticsearch仅转发更改请求，则可能以错误的顺序应用更改，导致得到损坏的文档。</p>
<h2 id="多文档模式"><a class="header" href="#多文档模式">多文档模式</a></h2>
<p><code>mget</code> 和 <code>bulk</code> API 的模式类似于单文档模式。区别在于协调节点知道每个文档存在于哪个分片中。 它将整个多文档请求分解成 <strong><em>每个分片</em> 的多文档请求</strong>，并且将这些请求并行转发到每个参与节点。</p>
<p>协调节点一旦收到来自每个节点的应答，就将每个节点的响应收集整理成单个响应，返回给客户端</p>
<h3 id="使用-mget-取回多个文档"><a class="header" href="#使用-mget-取回多个文档"><strong>使用</strong> <code>mget</code> <strong>取回多个文档</strong></a></h3>
<ol>
<li>客户端向 任意某个节点 发送 <code>mget</code> 请求。（即协调节点）</li>
<li>协调节点 分析批量操作中每个请求的所位于的分片，并为每个分片构建多文档获取请求，然后并行转发这些请求到托管在每个所需的主分片或者副本分片的节点上。一旦收到所有答复，协调节点 构建响应并将其返回给客户端。</li>
</ol>
<h3 id="使用-bulk-修改多个文档"><a class="header" href="#使用-bulk-修改多个文档"><strong>使用</strong> <code>bulk</code> <strong>修改多个文档</strong></a></h3>
<ol>
<li>客户端向 任意某个节点 发送 <code>bulk</code> 请求。（即协调节点）</li>
<li>协调节点分析批量操作中每个请求的所位于的分片，并将这些请求并行转发到每个包含主分片的节点主机</li>
<li>主分片一个接一个按顺序执行每个操作。当每个操作成功时，主分片并行转发新文档（或删除）到副本分片，然后执行下一个操作。 一旦所有的副本分片报告所有操作成功，该节点将向协调节点报告成功，协调节点将这些响应收集整理并返回给客户端。</li>
</ol>
<p><code>bulk</code> API 还可以在整个批量请求的最顶层使用 <code>consistency</code> 参数，以及在每个请求中的元数据中使用 <code>routing</code> 参数。</p>
<h3 id="多文档模式中的换行符"><a class="header" href="#多文档模式中的换行符">多文档模式中的换行符</a></h3>
<p>为什么 <code>bulk</code> API 需要有换行符的有趣格式，而不是发送包装在 JSON 数组中的请求，例如 <code>mget</code> API？</p>
<p>在批量请求中引用的每个文档可能属于不同的主分片， 每个文档可能被分配给集群中的任何节点。这意味着批量请求 <code>bulk</code> 中的每个 <em>操作</em> 都需要被转发到正确节点上的正确分片。</p>
<p>如果单个请求被包装在 JSON 数组中，那就意味着我们需要执行以下操作：</p>
<ul>
<li>将 JSON 解析为数组（包括文档数据，可以非常大）</li>
<li>查看每个请求以确定应该去哪个分片</li>
<li>为每个分片创建一个请求数组</li>
<li>将这些数组序列化为内部传输格式</li>
<li>将请求发送到每个分片</li>
</ul>
<p>这是可行的，但需要大量的 RAM 来存储原本相同的数据的副本，并将创建更多的数据结构，Java虚拟机（JVM）将不得不花费时间进行垃圾回收</p>
<p>相反，Elasticsearch可以直接读取被网络缓冲区接收的原始数据。 它使用换行符字符来识别和解析小的 <code>action/metadata</code> 行来决定哪个分片应该处理每个请求。</p>
<p>这些原始请求会被直接转发到正确的分片。没有冗余的数据复制，没有浪费的数据结构。整个请求尽可能在最小的内存中处理。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="映射"><a class="header" href="#映射">映射</a></h1>
<p>为了能够将时间域视为时间，数字域视为数字，字符串域视为全文或精确值字符串， Elasticsearch 需要知道每个域中数据的类型。这个信息包含在映射中。</p>
<h2 id="核心简单域类型"><a class="header" href="#核心简单域类型">核心简单域类型</a></h2>
<p>Elasticsearch 支持如下简单域类型：</p>
<ul>
<li>字符串: <code>string</code></li>
<li>整数 : <code>byte</code>, <code>short</code>, <code>integer</code>, <code>long</code></li>
<li>浮点数: <code>float</code>, <code>double</code></li>
<li>布尔型: <code>boolean</code></li>
<li>日期: <code>date</code></li>
</ul>
<p>如果索引一个新域时，通过JSON中基本数据类型，尝试猜测域类型，使用如下规则：</p>
<table><thead><tr><th><strong>JSON type</strong></th><th><strong>域 type</strong></th></tr></thead><tbody>
<tr><td>布尔型: <code>true</code> 或者 <code>false</code></td><td><code>boolean</code></td></tr>
<tr><td>整数: <code>123</code></td><td><code>long</code></td></tr>
<tr><td>浮点数: <code>123.45</code></td><td><code>double</code></td></tr>
<tr><td>字符串，有效日期: <code>2014-09-15</code></td><td><code>date</code></td></tr>
<tr><td>字符串: <code>foo bar</code></td><td><code>string</code></td></tr>
</tbody></table>
<blockquote>
<p>这意味着如果你通过引号( <code>&quot;123&quot;</code> )索引一个数字，它会被映射为 <code>string</code> 类型，而不是 <code>long</code> 。但是，如果这个域已经映射为 <code>long</code> ，那么 Elasticsearch 会尝试将这个字符串转化为 long ，如果无法转化，则抛出一个异常。</p>
</blockquote>
<h2 id="复杂核心域类型"><a class="header" href="#复杂核心域类型">复杂核心域类型</a></h2>
<h3 id="多值域"><a class="header" href="#多值域">多值域</a></h3>
<p>很有可能，我们希望 <code>tag</code> 域包含多个标签。我们可以以数组的形式索引标签：</p>
<p>对于数组，没有特殊的映射需求。任何域都可以包含0、1或者多个值，就像全文域分析得到多个词条。</p>
<p><strong>数组同类型</strong></p>
<p>这暗示 <em>数组中所有的值必须是相同数据类型的</em> 。你不能将日期和字符串混在一起。如果你通过索引数组来创建新的域，Elasticsearch 会用数组中第一个值的数据类型作为这个域的 <code>类型</code> 。</p>
<p><strong>_source不变形</strong>
当你从 Elasticsearch 得到一个文档，每个数组的顺序和你当初索引文档时一样。你得到的 <code>_source</code> 域，包含与你索引的一模一样的 JSON 文档。</p>
<p><strong>数组的无序搜索</strong></p>
<p>但是，数组是以多值域 <em>索引的</em>—可以搜索，但是无序的。 在搜索的时候，你不能指定 “第一个” 或者 “最后一个”。 更确切的说，把数组想象成 <em>装在袋子里的值</em> 。</p>
<h3 id="空域"><a class="header" href="#空域">空域</a></h3>
<p>当然，数组可以为空。这相当于存在零值。 事实上，在 Lucene 中是不能存储 <code>null</code> 值的，所以我们认为存在 <code>null</code> 值的域为空域。</p>
<p>下面三种域被认为是空的，它们将不会被索引：</p>
<pre><code class="language-js">&quot;null_value&quot;:               null,
&quot;empty_array&quot;:              [],
&quot;array_with_null_value&quot;:    [ null ]
</code></pre>
<h3 id="多层级对象"><a class="header" href="#多层级对象">多层级对象</a></h3>
<p>我们讨论的最后一个 JSON 原生数据类是 <em>对象</em> -- 在其他语言中称为哈希，哈希 map，字典或者关联数组。</p>
<p><em>内部对象</em> 经常用于嵌入一个实体或对象到其它对象中。例如，与其在 <code>tweet</code> 文档中包含 <code>user_name</code> 和 <code>user_id</code> 域，我们也可以这样写：</p>
<pre><code class="language-js">{
    &quot;tweet&quot;:            &quot;Elasticsearch is very flexible&quot;,
    &quot;user&quot;: {
        &quot;id&quot;:           &quot;@johnsmith&quot;,
        &quot;gender&quot;:       &quot;male&quot;,
        &quot;age&quot;:          26,
        &quot;name&quot;: {
            &quot;full&quot;:     &quot;John Smith&quot;,
            &quot;first&quot;:    &quot;John&quot;,
            &quot;last&quot;:     &quot;Smith&quot;
        }
    }
}
</code></pre>
<h4 id="内部对象的映射"><a class="header" href="#内部对象的映射">内部对象的映射</a></h4>
<p>Elasticsearch 会动态监测新的对象域并映射它们为 <code>对象</code> ，在 <code>properties</code> 属性下列出内部域：</p>
<pre><code class="language-js">{
  &quot;gb&quot;: {
    &quot;tweet&quot;: { 
      &quot;properties&quot;: {
        &quot;tweet&quot;:            { &quot;type&quot;: &quot;string&quot; },
        &quot;user&quot;: { 
          &quot;type&quot;:             &quot;object&quot;,
          &quot;properties&quot;: {
            &quot;id&quot;:           { &quot;type&quot;: &quot;string&quot; },
            &quot;gender&quot;:       { &quot;type&quot;: &quot;string&quot; },
            &quot;age&quot;:          { &quot;type&quot;: &quot;long&quot;   },
            &quot;name&quot;:   { 
              &quot;type&quot;:         &quot;object&quot;,
              &quot;properties&quot;: {
                &quot;full&quot;:     { &quot;type&quot;: &quot;string&quot; },
                &quot;first&quot;:    { &quot;type&quot;: &quot;string&quot; },
                &quot;last&quot;:     { &quot;type&quot;: &quot;string&quot; }
              }
            }
          }
        }
      }
    }
  }
}
</code></pre>
<h4 id="内部对象是如何索引的"><a class="header" href="#内部对象是如何索引的">内部对象是如何索引的</a></h4>
<p>Lucene 不理解内部对象。 Lucene 文档是由一组键值对列表组成的。为了能让 Elasticsearch 有效地索引内部类，它把我们的文档转化成这样：</p>
<pre><code class="language-js">{
    &quot;tweet&quot;:            [elasticsearch, flexible, very],
    &quot;user.id&quot;:          [@johnsmith],
    &quot;user.gender&quot;:      [male],
    &quot;user.age&quot;:         [26],
    &quot;user.name.full&quot;:   [john, smith],
    &quot;user.name.first&quot;:  [john],
    &quot;user.name.last&quot;:   [smith]
}
</code></pre>
<p><em>内部域</em> 可以通过名称引用（例如， <code>first</code> ）。为了区分同名的两个域，我们可以使用全 <em>路径</em> （例如， <code>user.name.first</code> ） 或 <code>type</code> 名加路径（ <code>tweet.user.name.first</code> ）。</p>
<p>在前面简单扁平的文档中，没有 <code>user</code> 和 <code>user.name</code> 域。Lucene 索引只有标量和简单值，没有复杂数据结构。</p>
<h4 id="内部对象数组"><a class="header" href="#内部对象数组">内部对象数组</a></h4>
<p>最后，考虑包含内部对象的数组是如何被索引的。 假设我们有个 <code>followers</code> 数组：</p>
<pre><code class="language-js">{
    &quot;followers&quot;: [
        { &quot;age&quot;: 35, &quot;name&quot;: &quot;Mary White&quot;},
        { &quot;age&quot;: 26, &quot;name&quot;: &quot;Alex Jones&quot;},
        { &quot;age&quot;: 19, &quot;name&quot;: &quot;Lisa Smith&quot;}
    ]
}
</code></pre>
<p>这个文档会像我们之前描述的那样被扁平化处理，结果如下所示：</p>
<pre><code class="language-js">{
    &quot;followers.age&quot;:    [19, 26, 35],
    &quot;followers.name&quot;:   [alex, jones, lisa, smith, mary, white]
}
</code></pre>
<p><code>{age: 35}</code> 和 <code>{name: Mary White}</code> 之间的相关性已经丢失了</p>
<p>相关内部对象被称为 <em>nested</em> 对象，可以回答上面的查询</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="es数据导入导出"><a class="header" href="#es数据导入导出">es数据导入导出</a></h1>
<pre><code>elasticdump \
  --input=http://es-test1:9201/sca_recording_aliyun_1634527720352 \
  --output=http://es-test:9200/sca_recording \
  --type=analyzer
  
elasticdump \
  --input=http://es-test1:9201/sca_recording_aliyun_1634527720352 \
  --output=http://es-test:9200/sca_recording \
  --type=mapping
elasticdump \
  --input=http://es-test1:9201/sca_recording_aliyun_1634527720352 \
  --output=http://es-test:9200/sca_recording \
  --type=data
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h3 id="概念定义"><a class="header" href="#概念定义">概念定义</a></h3>
<ul>
<li>集群(cluster):由一个或多个节点组成, 并通过集群名称与其他集群进行区分</li>
<li>节点(node):单个ElasticSearch实例. 通常一个节点运行在一个隔离的容器或虚拟机中</li>
<li>索引(index):数据的 逻辑命名空间，分片的逻辑组合</li>
<li>分片(shard):存储数据的最小物理单元</li>
<li>副本(replica):主分片的一个副本</li>
</ul>
<h3 id="副本"><a class="header" href="#副本">副本</a></h3>
<p>本文中不会对ElasticSearch的副本做详细阐述. 如果想单独了解可参考<a href="https://qbox.io/blog/announcing-replicated-elasticsearch-clusters">这篇文章</a>.</p>
<p>副本对搜索性能非常重要, 同时用户也可在任何时候添加或删除副本. 正如<a href="https://qbox.io/blog/announcing-replicated-elasticsearch-clusters">另篇文章</a>所述, 额外的副本能给你带来更大的容量, 更高的呑吐能力及更强的故障恢复能力.</p>
<h3 id="分片"><a class="header" href="#分片">分片</a></h3>
<p>一个分片 <em>shard</em> 就是 es中的最小工作单元</p>
<p>它只是保存了索引中的所有数据的一部分</p>
<p>每个分片就是一个Lucene实例，并且它本身就是一个完整的搜索引擎</p>
<h3 id="分片是es在进群中分发数据的关键"><a class="header" href="#分片是es在进群中分发数据的关键"><strong>分片是ES在进群中分发数据的关键</strong></a></h3>
<ol>
<li>
<p>可以把分片想想成数据的容器。文档存储在分片中</p>
</li>
<li>
<p>然后分片分配到集群中的节点上。当集群扩容或缩小，ES将会自动在节点间迁移分片，以使集群保持平衡</p>
</li>
<li>
<p>分片可以是主分片，或者是副本分片</p>
</li>
<li>
<p>ES默认为一个索引创建5个主分片, 并分别为其创建一个副本分片. 也就是说每个索引都由5个主分片成本, 而每个主分片都相应的有一个copy.  如果磁盘空间不足 15%，则不分配 replica shard。磁盘空间不足 5%，则不再分配任何的 primary shard。</p>
</li>
<li>
<p>索引的每个文档属于一个单独的主分片，所以主分片的数量决定了索引最多能存储多少数据。</p>
</li>
<li>
<p>复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供请求，比如搜索或者从别的shard取回文档。</p>
</li>
</ol>
<h3 id="主分片数无法修改"><a class="header" href="#主分片数无法修改"><strong>主分片数无法修改</strong></a></h3>
<ol>
<li><strong>当索引创建完成的时候，主分片的数量就固定了</strong>，但是复制分片的数量可以随时调整</li>
<li>在集群运行中你无法调整分片设置. 既便以后你发现需要调整分片数量, 你也只能新建创建并对数据进行重新索引(reindex)(虽然reindex会比较耗时, 但至少能保证你不会停机).</li>
<li>主分片的配置与硬盘分区很类似, 在对一块空的硬盘空间进行分区时, 会要求用户先进行数据备份, 然后配置新的分区, 最后把数据写到新的分区上.</li>
</ol>
<h3 id="过度分配"><a class="header" href="#过度分配"><strong>过度分配</strong></a></h3>
<ol>
<li>稍有富余是好的, 但过度分配分片却是大错特错. 具体定义多少分片很难有定论, 取决于用户的数据量和使用方式</li>
<li>每个分片都是有额外的成本的:
<ol>
<li>每个分片本质上就是一个Lucene索引, 因此会消耗相应的文件句柄, 内存和CPU资源</li>
<li>每个搜索请求会调度到索引的每个分片中. 如果分片分散在不同的节点倒是问题不太. 但当分片开始竞争相同的硬件资源时, 性能便会逐步下降</li>
<li>ES使用<a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html">词频统计来计算相关性</a>. 当然这些统计也会分配到各个分片上. 如果在大量分片上只维护了很少的数据, 则将导致最终的文档相关性较差</li>
<li>尽量保证同类数据 分布到 相同的分片</li>
</ol>
</li>
<li>如果你真的担心数据的快速增长, 我们建议你多关心这条限制 <a href="https://www.elastic.co/guide/en/elasticsearch/guide/current/relevance-intro.html">ElasticSearch推荐的最大JVM堆空间</a>是30~32G,
<ol>
<li>所以把你的分片最大容量限制为30GB, 然后再对分片数量做合理估算. 例如, 你认为你的数据能达到200GB, 我们推荐你最多分配7到8个分片.</li>
</ol>
</li>
<li>总之, 不要现在就为你可能在三年后才能达到的10TB数据做过多分配. 如果真到那一天, 你也会很早感知到性能变化的.</li>
<li>对大数据集, 我们非常鼓励你为索引多分配些分片--当然也要在合理范围内. 上面讲到的每个分片最好不超过30GB的原则依然使用</li>
</ol>
<p>在开始阶段, 一个好的方案是根据你的节点数量按照1.5~3倍的原则来创建分片</p>
<p>例如,如果你有3个节点, 则推荐你创建的分片数最多不超过9(3x3)个</p>
<p>随着数据量的增加,如果你通过[集群状态API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-stats.html?q=cluster stat)发现了问题,或者遭遇了性能退化,则只需要增加额外的节点即可. ES会自动帮你完成分片在不同节点上的分布平衡.</p>
<p>再强调一次, 虽然这里我们暂未涉及副本节点的介绍, 但上面的指导原则依然使用: 是否有必要在每个节点上只分配一个索引的分片. </p>
<p>另外, 如果给每个分片分配1个副本, 你所需的节点数将加倍. 如果需要为每个分片分配2个副本, 则需要3倍的节点数. 更多详情可以参考<a href="http://blog.qbox.io/announcing-replicated-elasticsearch-clusters">基于副本的集群</a>.</p>
<h3 id="logstash"><a class="header" href="#logstash">Logstash</a></h3>
<p>日志场景就是 基于日期的索引需求, 并且对索引数据的搜索场景非常少. </p>
<p>也许这些索引量将达到成百上千, 但每个索引的数据量只有1GB甚至更小.</p>
<p>对于这种类似场景, 我建议你只需要为索引分配1个分片.</p>
<p>如果使用<em>ES的默认配置(5个分片</em>, 并且使用Logstash按天生成索引, 那么6个月下来, 你拥有的分片数将达到890个</p>
<p>再多的话, 你的集群将难以工作--除非你提供了更多(例如15个或更多)的节点.</p>
<p>想一下, 大部分的Logstash用户并不会频繁的进行搜索, 甚至每分钟都不会有一次查询. 所以这种场景, 推荐更为经济使用的设置. 在这种场景下, 搜索性能并不是第一要素, 所以并不需要很多副本. 维护单个副本用于数据冗余已经足够. 不过数据被不断载入到内存的比例相应也会变高.</p>
<p>如果你的索引只需要一个分片, 那么使用Logstash的配置可以在3节点的集群中维持运行6个月. 当然你至少需要使用4GB的内存, 不过建议使用8GB, 因为在多数据云平台中使用8GB内存会有明显的网速以及更少的资源共享.</p>
<p>再次声明, 数据分片也是要有相应资源消耗,并且需要持续投入</p>
<p>当索引拥有较多分片时, 为了组装查询结果, ES必须单独查询每个分片(当然并行的方式)并对结果进行合并. 所以高性能IO设备(SSDs)和多核处理器无疑对分片性能会有巨大帮助. 尽管如此, 你还是要多关心数据本身的大小,更新频率以及未来的状态. 在分片分配上并没有绝对的答案, 只希望你能从本文的讨论中受益.</p>
<p><strong>一个分片只能存放 Integer.MAX_VALUE - 128 = 2,147,483,519 个 docs</strong></p>
<p><a href="https://qbox.io/blog/optimizing-elasticsearch-how-many-shards-per-index">参考链接</a></p>
<h3 id="分片设计推荐"><a class="header" href="#分片设计推荐">分片设计推荐</a></h3>
<ol>
<li>每一个分片数据文件小于30GB</li>
<li>每一个索引中的一个分片对应一个节点</li>
<li>节点数大于等于分片数</li>
</ol>
<h3 id="分片数量计算"><a class="header" href="#分片数量计算">分片数量计算</a></h3>
<p><strong>假如需要300G文件大小</strong></p>
<ol>
<li>
<p>至少需要 10个分片，每个分片位于独立的节点 则至少需要10个节点</p>
<p><strong>SN(分片数) = IS(索引大小) / 30</strong></p>
</li>
<li>
<p><strong>NN(节点数) = SN(分片数) + MNN(主节点数[无数据]) + NNN(负载节点数)</strong></p>
</li>
</ol>
<h3 id="分片查询"><a class="header" href="#分片查询">分片查询</a></h3>
<h4 id="randomizeacross-shards"><a class="header" href="#randomizeacross-shards"><strong>randomizeacross shards</strong></a></h4>
<p>随机选择分片查询数据，es的默认方式</p>
<h4 id="_local"><a class="header" href="#_local"><strong>_local</strong></a></h4>
<p>优先在本地节点上的分片查询数据然后再去其他节点上的分片查询，本地节点没有IO问题但有可能造成负载不均问题。数据量是完整的。</p>
<h4 id="_primary"><a class="header" href="#_primary"><strong>_primary</strong></a></h4>
<p>只在主分片中查询不去副本查，一般数据完整。</p>
<h4 id="_primary_first"><a class="header" href="#_primary_first"><strong>_primary_first</strong></a></h4>
<p>优先在主分片中查，如果主分片挂了则去副本查，一般数据完整。</p>
<h4 id="_only_node"><a class="header" href="#_only_node"><strong>_only_node</strong></a></h4>
<p>只在指定id的节点中的分片中查询，数据可能不完整。</p>
<h4 id="_prefer_node"><a class="header" href="#_prefer_node"><strong>_prefer_node</strong></a></h4>
<p>优先在指定你给节点中查询，一般数据完整。</p>
<h4 id="_shards"><a class="header" href="#_shards"><strong>_shards</strong></a></h4>
<p>在指定分片中查询，数据可能不完整。</p>
<h4 id="_only_nodes"><a class="header" href="#_only_nodes"><strong>_only_nodes</strong></a></h4>
<p>可以自定义去指定的多个节点查询，es不提供此方式需要改源码。</p>
<pre><code class="language-java"> /** 
         * 指定分片 查询 
         */  
        @Test  
        public void testPreference()  
        {  
            SearchResponse searchResponse = transportClient.prepareSearch(index)  
                    .setTypes(&quot;add&quot;)  
                    //.setPreference(&quot;_local&quot;)  
                    //.setPreference(&quot;_primary&quot;)  
                    //.setPreference(&quot;_primary_first&quot;)  
                    //.setPreference(&quot;_only_node:ZYYWXGZCSkSL7QD0bDVxYA&quot;)  
                    //.setPreference(&quot;_prefer_node:ZYYWXGZCSkSL7QD0bDVxYA&quot;)  
                    .setPreference(&quot;_shards:0,1,2&quot;)  
                    .setQuery(QueryBuilders.matchAllQuery()).setExplain(true).get();  

            SearchHits hits = searchResponse.getHits();  
            System.out.println(hits.getTotalHits());  
            SearchHit[] hits2 = hits.getHits();  
            for(SearchHit h : hits2)  
            {  
                System.out.println(h.getSourceAsString());  
            }  
        }  
</code></pre>
<h3 id="分片复制过程"><a class="header" href="#分片复制过程">分片复制过程</a></h3>
<p>我们能够发送请求给集群中任意一个节点。每个节点都有能力处理任意请求。每个节点都知道任意文档所在的节点</p>
<p>新建索引和删除请求都是写操作，它们必须在主分片上成功完成才能赋值到相关的复制分片上</p>
<p>在主分片和复制分片上成功新建、索引或删除一个文档必要的顺序步骤：</p>
<ol>
<li>客户端给Node1 发送新建、索引或删除请求。</li>
<li>节点使用文档的_id确定文档属于分片0.转发请求到Node3，分片0位于这个节点上。</li>
<li>Node3在主分片上执行请求，如果成功，它转发请求到相应的位于Node1和Node2的复制节点上</li>
<li>当所有的复制节点报告成功，Node3报告成功到请求的节点，请求的节点再报告给客户端。</li>
<li>客户端接收到成功响应的时候，文档的修改已经被用于主分片和所有的复制分片，修改生效了。</li>
</ol>
<p><strong>ES分片复制</strong></p>
<ul>
<li>复制默认的值是sync。这将导致主分片得到复制分片的成功响应后才返回。</li>
<li>如果你设置replication为async,请求在主分片上被执行后就会返回给客户端。它依旧会转发给复制节点，但你将不知道复制节点成功与否。</li>
</ul>
<h3 id="节点类型"><a class="header" href="#节点类型">节点类型</a></h3>
<ul>
<li>master 节点： 集群中的一个节点会被选为 master 节点，它将负责管理集群范畴的变更，例如创建或删除索引，添加节点到集群或从集群中删除节点。master 节点无需参与文档层面的变更和搜索，这意味着仅有一个 master 节点并不会因流量增长而成为瓶颈。任意一个节点都可以成为 master 节点。</li>
<li>data 节点： 持有数据和倒排索引。默认情况下，每个节点都可以通过设定配置文件 elasticsearch.yml 中的 node.data 属性为 true (默认) 成为数据节点。如果需要一个专门的主节点 (一个节点既可以是 master 节点，同时也可以是 data 节点)，应将其 node.data 属性设置为 false。</li>
<li>client 节点： 如果将 node.master 属性和 node.data 属性都设置为 false，那么该节点就是一个客户端节点，扮演一个负载均衡的角色，将到来的请求路由到集群中的各个节点。</li>
</ul>
<h3 id="分片与索引实战"><a class="header" href="#分片与索引实战">分片与索引实战</a></h3>
<h4 id="新建索引分片实例"><a class="header" href="#新建索引分片实例">新建索引分片实例</a></h4>
<pre><code>PUT /testIndex
{
	&quot;settings&quot;:{
		&quot;number_of_shards&quot;:12,
		&quot;number_of_replicas&quot;:1
	}
}
</code></pre>
<h4 id="调整分片数"><a class="header" href="#调整分片数">调整分片数</a></h4>
<pre><code>PUT /testIndex/_settings
{
	&quot;number_of_replicas&quot;:2
}
</code></pre>
<h4 id="检查分片信息"><a class="header" href="#检查分片信息">检查分片信息</a></h4>
<pre><code>GET _cat/shards?v
</code></pre>
<h4 id="检查索引信息"><a class="header" href="#检查索引信息">检查索引信息</a></h4>
<pre><code>GET _cat/indices
</code></pre>
<h4 id="设置磁盘水位"><a class="header" href="#设置磁盘水位">设置磁盘水位</a></h4>
<pre><code>PUT _cluster/settings 
{ 
	&quot;transient&quot;: { 
		&quot;cluster.routing.allocation.disk.watermark.low&quot;: &quot;90%&quot;, 
		&quot;cluster.routing.allocation.disk.watermark.high&quot;: &quot;5gb&quot;
	 }
 }

</code></pre>
<ol>
<li>ES 默认当磁盘空间不足 15%时，会禁止分配 replica shard。可以动态调整 ES 对磁盘空间的要求限制；</li>
<li>配置磁盘空间限制的时候，要求low必须比 high 大，可以使用百分比或 gb 的方式设置，且ES要求low至少满足磁盘 95%的容量。</li>
<li>low - 对磁盘空闲容量的最低限制（默认85%）；</li>
<li>high - 对磁盘空闲容量的最高限制（默认90%，极限值95%）</li>
<li>low 为 50gb。high 为 10gb。则当磁盘空闲容量不足 50gb 时停止分配 replica shard。 当磁盘空闲容量不足 10gb 时，停止分配 shard，并将应该在当前结点中分配的 shard 分配 到其他结点中；</li>
</ol>
<h4 id="查看集群健康状态"><a class="header" href="#查看集群健康状态">查看集群健康状态</a></h4>
<pre><code>GET _cluster/health

{
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;status&quot; : &quot;green&quot;,
  &quot;timed_out&quot; : false,
  &quot;number_of_nodes&quot; : 2,
  &quot;number_of_data_nodes&quot; : 2,
  &quot;active_primary_shards&quot; : 8,
  &quot;active_shards&quot; : 16,
  &quot;relocating_shards&quot; : 0,
  &quot;initializing_shards&quot; : 0,
  &quot;unassigned_shards&quot; : 0,
  &quot;delayed_unassigned_shards&quot; : 0,
  &quot;number_of_pending_tasks&quot; : 0,
  &quot;number_of_in_flight_fetch&quot; : 0,
  &quot;task_max_waiting_in_queue_millis&quot; : 0,
  &quot;active_shards_percent_as_number&quot; : 100.0
}
</code></pre>
<h3 id="索引未分配原因"><a class="header" href="#索引未分配原因">索引未分配原因</a></h3>
<ol>
<li>INDEX_CREATED：由于创建索引的API导致未分配。</li>
<li>CLUSTER_RECOVERED：由于完全集群恢复导致未分配。</li>
<li>INDEX_REOPENED ：由于打开open或关闭close一个索引导致未分配。</li>
<li>DANGLING_INDEX_IMPORTED ：由于导入dangling索引的结果导致未分配。</li>
<li>NEW_INDEX_RESTORED ：由于恢复到新索引导致未分配。</li>
<li>EXISTING_INDEX_RESTORED：由于恢复到已关闭的索引导致未分配。</li>
<li>REPLICA_ADDED：由于显式添加副本分片导致未分配。</li>
<li>ALLOCATION_FAILED ：由于分片分配失败导致未分配。</li>
<li>NODE_LEFT ：由于承载该分片的节点离开集群导致未分配。</li>
<li>REINITIALIZED ：由于当分片从开始移动到初始化时导致未分配（例如，使用影子shadow副本分片）。</li>
<li>REROUTE_CANCELLED ：作为显式取消重新路由命令的结果取消分配。</li>
<li>REALLOCATED_REPLICA：确定更好的副本位置被标定使用，导致现有的副本分配被取消，出现未分配。</li>
</ol>
<h4 id="查看具体未分配原因"><a class="header" href="#查看具体未分配原因">查看具体未分配原因</a></h4>
<pre><code>GET /_cluster/allocation/explain

GET _cat/indices?v&amp;health=red


GET /_cat/shards?v&amp;h=n,index,shard,prirep,state,sto,sc,unassigned.reason,unassigned.details
</code></pre>
<h4 id="尝试重新分配失败的分片"><a class="header" href="#尝试重新分配失败的分片">尝试重新分配失败的分片</a></h4>
<pre><code class="language-bash">POST /_cluster/reroute?retry_failed=true
</code></pre>
<p>默认索引的尝试次数为5</p>
<pre><code class="language-bash">PUT /indexname/_settings
{
  &quot;index&quot;: {
    &quot;allocation&quot;: {
      &quot;max_retries&quot;: 20
    }
  }
}
</code></pre>
<h4 id="将副本分片提升为主分片"><a class="header" href="#将副本分片提升为主分片">将副本分片提升为主分片</a></h4>
<p>如果确定了主分片已经损坏，可以尝试将副本分片提升为主(会丢部分数据)：</p>
<pre><code class="language-cpp">POST /_cluster/reroute?pretty
{
  &quot;commands&quot;: [
    {
      &quot;allocate_stale_primary&quot;: {
        &quot;index&quot;: &quot;indexname&quot;,//索引名
        &quot;shard&quot;: 3,//操作的分片id
        &quot;node&quot;: &quot;node1&quot;,//此分片副本位于的节点
        &quot;accept_data_loss&quot;: true//提示数据可能会丢失
      }
    }
  ]
}
</code></pre>
<p>此方案存在一个问题是需要提前知道此分片的副本位于哪个节点用以指定，可以通过如果api获取副本分片位置：</p>
<pre><code class="language-undefined">GET _shard_stores?pretty
GET indexname/_shard_stores?pretty
</code></pre>
<h4 id="手动判断目录"><a class="header" href="#手动判断目录">手动判断目录</a></h4>
<p>判断当前es进程使用的数据目录:通过pid和yml配置的目录去匹配，如data</p>
<pre><code class="language-kotlin">ll /proc/pid/fd |grep data
</code></pre>
<p>如果索引损坏导致api失效，则需要人工去数据目录进行查找副本分片位置,目录结构如下:</p>
<pre><code class="language-skotlin">data/nodes/0/indices/Z60wvPOWSP6Qbk79i757Vg/0
</code></pre>
<p>数据目录下<strong>为节点号 -&gt; 索引文件夹 -&gt; 索引ID -&gt; 分片号</strong></p>
<h4 id="将此分片置为空分片"><a class="header" href="#将此分片置为空分片">将此分片置为空分片</a></h4>
<ol>
<li>
<p>如果此分片的主副都已经损坏，则可将此分片置为空以保留索引其他分片数据：</p>
</li>
<li>
<pre><code class="language-json">{
  &quot;commands&quot;: [
    {
      &quot;allocate_empty_primary&quot;: {
        &quot;index&quot;: &quot;indexname&quot;,//索引名
        &quot;shard&quot;: 3,//操作的分片id
        &quot;node&quot;: &quot;node1&quot;,//空分片要分配的节点
        &quot;accept_data_loss&quot;: true//提示数据可能会丢失
      }
    }
  ]
}
</code></pre>
</li>
<li>
<p>如果集群存在大量索引分片无法恢复，则可以使用脚本将全部分片置空,可以基于下面的脚本修改：</p>
</li>
</ol>
<pre><code class="language-bash">#!/bin/bash
master=$(curl -s 'http://localhost:9200/_cat/master?v' | grep -v ' ip ' | awk '{print $1}')
for index in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | awk '{print $1}' | sort | uniq); do
    for shard in $(curl  -s 'http://localhost:9200/_cat/shards' | grep UNASSIGNED | grep $index | awk '{print $2}' | sort | uniq); do
        echo  $index $shard
        curl -XPOST -H 'Content-Type: application/json'  'http://localhost:9200/_cluster/reroute' -d '{
            &quot;commands&quot; : [ {
                  &quot;allocate_empty_primary&quot; : {
                      &quot;index&quot; : &quot;'$index'&quot;,
                      &quot;shard&quot; : &quot;'$shard'&quot;,
                      &quot;node&quot; : &quot;'$master'&quot;,
                  &quot;accept_data_loss&quot; : true
                  }
                }
            ]
        }'
        sleep 1
    done
done
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="创建索引"><a class="header" href="#创建索引">创建索引</a></h2>
<pre><code class="language-js">PUT /my_index
{
    &quot;settings&quot;: { ... any settings ... },
    &quot;mappings&quot;: {
        &quot;type_one&quot;: { ... any mappings ... },
        &quot;type_two&quot;: { ... any mappings ... },
        ...
    }
}
</code></pre>
<p>如果你想禁止自动创建索引，你 可以通过在 <code>config/elasticsearch.yml</code> 的每个节点下添加下面的配置：</p>
<pre><code class="language-js">action.auto_create_index: false
</code></pre>
<h2 id="删除一个索引"><a class="header" href="#删除一个索引">删除一个索引</a></h2>
<p>用以下的请求来 删除索引:</p>
<pre><code class="language-js">DELETE /my_index
</code></pre>
<p>你也可以这样删除多个索引：</p>
<pre><code class="language-js">DELETE /index_one,index_two
DELETE /index_*
</code></pre>
<p>你甚至可以这样删除 <em>全部</em> 索引：</p>
<pre><code class="language-js">DELETE /_all
DELETE /*
</code></pre>
<p>如果你想要避免意外的大量删除, 你可以在你的 <code>elasticsearch.yml</code> 做如下配置：</p>
<pre><code>action.destructive_requires_name: true
</code></pre>
<p>这个设置使删除只限于特定名称指向的数据, 而不允许通过指定 <code>_all</code> 或通配符来删除指定索引库。</p>
<h2 id="索引设置"><a class="header" href="#索引设置">索引设置</a></h2>
<p>你可以通过修改配置来自定义索引行为，详细配置参照</p>
<p>下面是两个 最重要的设置：</p>
<ul>
<li>
<p><strong><code>number_of_shards</code></strong></p>
<p>每个索引的主分片数，默认值是 <code>5</code> 。这个配置在索引创建后不能修改。</p>
</li>
<li>
<p><strong><code>number_of_replicas</code></strong></p>
<p>每个主分片的副本数，默认值是 <code>1</code> 。对于活动的索引库，这个配置可以随时修改。</p>
</li>
</ul>
<p>例如，我们可以创建只有 一个主分片，没有副本的小索引：</p>
<pre><code class="language-sense">PUT /my_temp_index
{
    &quot;settings&quot;: {
        &quot;number_of_shards&quot; :   1,
        &quot;number_of_replicas&quot; : 0
    }
}
</code></pre>
<p>然后，我们可以用 <code>update-index-settings</code> API 动态修改副本数：</p>
<pre><code class="language-sense">PUT /my_temp_index/_settings
{
    &quot;number_of_replicas&quot;: 1
}
</code></pre>
<h2 id="配置分析器"><a class="header" href="#配置分析器">配置分析器</a></h2>
<p>用来配置已存在的分析器或针对你的索引创建新的自定义分析器。</p>
<p><code>standard</code> 分析器是用于全文字段的默认分析器，对于大部分西方语系来说是一个不错的选择。 它包括了以下几点：</p>
<ul>
<li><code>standard</code> 分词器，通过单词边界分割输入的文本。</li>
<li><code>standard</code> 语汇单元过滤器，目的是整理分词器触发的语汇单元（但是目前什么都没做）。</li>
<li><code>lowercase</code> 语汇单元过滤器，转换所有的语汇单元为小写。</li>
<li><code>stop</code> 语汇单元过滤器，删除停用词—对搜索相关性影响不大的常用词，如 <code>a</code> ， <code>the</code> ， <code>and</code> ， <code>is</code> 。</li>
</ul>
<p>默认情况下，停用词过滤器是被禁用的。如需启用它，你可以通过创建一个基于 <code>standard</code> 分析器的自定义分析器并设置 <code>stopwords</code> 参数。 可以给分析器提供一个停用词列表，或者告知使用一个基于特定语言的预定义停用词列表。</p>
<p>在下面的例子中，我们创建了一个新的分析器，叫做 <code>es_std</code> ， 并使用预定义的西班牙语停用词列表：</p>
<pre><code class="language-sense">PUT /spanish_docs
{
    &quot;settings&quot;: {
        &quot;analysis&quot;: {
            &quot;analyzer&quot;: {
                &quot;es_std&quot;: {
                    &quot;type&quot;:      &quot;standard&quot;,
                    &quot;stopwords&quot;: &quot;_spanish_&quot;
                }
            }
        }
    }
}
</code></pre>
<h2 id="自定义分析器"><a class="header" href="#自定义分析器">自定义分析器</a></h2>
<p>一个 <em>分析器</em> 就是在一个包里面组合了三种函数的一个包装器， 三种函数按照顺序被执行</p>
<h3 id="字符过滤器"><a class="header" href="#字符过滤器"><strong>字符过滤器</strong></a></h3>
<p>字符过滤器 用来 <code>整理</code> 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 <code>&lt;p&gt;</code> 或者 <code>&lt;div&gt;</code> 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-htmlstrip-charfilter.html"><code>html清除</code> 字符过滤器</a> 来移除掉所有的HTML标签，并且像把 <code>Á</code> 转换为相对应的Unicode字符 <code>Á</code> 这样，转换HTML实体。</p>
<p>一个分析器可能有0个或者多个字符过滤器。</p>
<h3 id="分词器"><a class="header" href="#分词器"><strong>分词器</strong></a></h3>
<p>一个分析器 <em>必须</em> 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 <code>标准</code> 分析器里使用的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-standard-tokenizer.html"><code>标准</code> 分词器</a> 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。</p>
<p>例如， <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-keyword-tokenizer.html"><code>关键词</code> 分词器</a> 完整地输出 接收到的同样的字符串，并不做任何分词。 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-whitespace-tokenizer.html"><code>空格</code> 分词器</a> 只根据空格分割文本 。 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-pattern-tokenizer.html"><code>正则</code> 分词器</a> 根据匹配正则表达式来分割文本 。</p>
<h3 id="词单元过滤器"><a class="header" href="#词单元过滤器"><strong>词单元过滤器</strong></a></h3>
<p>经过分词，作为结果的 <em>词单元流</em> 会按照指定的顺序通过指定的词单元过滤器 。</p>
<p>词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 <a href="http://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lowercase-tokenizer.html"><code>lowercase</code> </a>和 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-stop-tokenfilter.html"><code>stop</code> 词过滤器</a> ，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-stemmer-tokenfilter.html">词干过滤器</a> 把单词 <code>遏制</code> 为 词干。 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-asciifolding-tokenfilter.html"><code>ascii_folding</code> 过滤器</a>移除变音符，把一个像 <code>&quot;très&quot;</code> 这样的词转换为 <code>&quot;tres&quot;</code> 。 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-ngram-tokenfilter.html"><code>ngram</code></a> 和 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/analysis-edgengram-tokenfilter.html"><code>edge_ngram</code> 词单元过滤器</a> 可以产生 适合用于部分匹配或者自动补全的词单元。</p>
<h3 id="创建一个自定义分析器"><a class="header" href="#创建一个自定义分析器">创建一个自定义分析器</a></h3>
<p><strong>基本语法</strong></p>
<pre><code class="language-js">PUT /my_index
{
    &quot;settings&quot;: {
        &quot;analysis&quot;: { 
            &quot;char_filter&quot;: { ... custom character filters ... },
            &quot;tokenizer&quot;:   { ...    custom tokenizers     ... },
            &quot;filter&quot;:      { ...   custom token filters   ... },
            &quot;analyzer&quot;:    { ...    custom analyzers      ... }
        }
    }
}
</code></pre>
<p><strong>自定义映射过滤器</strong></p>
<pre><code>&quot;char_filter&quot;: {
    &quot;&amp;_to_and&quot;: {
        &quot;type&quot;:       &quot;mapping&quot;,
        &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;]
    }
}
</code></pre>
<p><strong>自定义token filter</strong></p>
<pre><code class="language-js">&quot;filter&quot;: {
    &quot;my_stopwords&quot;: {
        &quot;type&quot;:        &quot;stop&quot;,
        &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ]
    }
}
</code></pre>
<p><strong>创建分析器</strong></p>
<pre><code class="language-js">&quot;analyzer&quot;: {
    &quot;my_analyzer&quot;: {
        &quot;type&quot;:           &quot;custom&quot;,
        &quot;char_filter&quot;:  [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ],
        &quot;tokenizer&quot;:      &quot;standard&quot;,
        &quot;filter&quot;:       [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ]
    }
}
</code></pre>
<h2 id="类型和映射"><a class="header" href="#类型和映射">类型和映射</a></h2>
<h3 id="lucene-如何处理文档"><a class="header" href="#lucene-如何处理文档"><strong>Lucene 如何处理文档</strong></a></h3>
<ul>
<li>在 Lucene 中，一个文档由一组简单的键值对组成。 每个字段都可以有多个值，但至少要有一个值</li>
<li><strong>一个字符串可以通过分析过程转化为多个值</strong></li>
<li>Lucene 不关心这些值是字符串、数字或日期—所有的值都被当做 <em>不透明字节</em> 。</li>
<li>当我们在 Lucene 中索引一个文档时，每个字段的值都被添加到相关字段的倒排索引中</li>
</ul>
<p>你也可以将未处理的原始数据 <em>存储</em> 起来，以便这些原始数据在之后也可以被检索到。</p>
<h3 id="类型是如何实现的"><a class="header" href="#类型是如何实现的">类型是如何实现的</a></h3>
<p>Elasticsearch 类型是以 Lucene 处理文档的这个方式为基础来实现的</p>
<p>一个索引可以有多个类型，这些类型的文档可以存储在相同的索引中。</p>
<p>Lucene 没有文档类型的概念，每个文档的类型名被存储在一个叫 <code>_type</code> 的元数据字段上。 当我们要检索某个类型的文档时, Elasticsearch 通过在 <code>_type</code> 字段上使用过滤器限制只返回这个类型的文档。</p>
<p>Lucene 也没有映射的概念。 <strong>映射是 Elasticsearch 将复杂 JSON 文档 <em>映射</em> 成 Lucene 需要的扁平化数据的方式</strong>。</p>
<h3 id="根对象"><a class="header" href="#根对象">根对象</a></h3>
<p>映射的最高一层被称为 <em>根对象</em> ，它可能包含下面几项：</p>
<ul>
<li>一个 <em>properties</em> 节点，列出了文档中可能包含的每个字段的映射</li>
<li>各种元数据字段，它们都以一个下划线开头，例如 <code>_type</code> 、 <code>_id</code> 和 <code>_source</code></li>
<li>设置项，控制如何动态处理新的字段，例如 <code>analyzer</code> 、 <code>dynamic_date_formats</code> 和 <code>dynamic_templates</code></li>
<li>其他设置，可以同时应用在根对象和其他 <code>object</code> 类型的字段上，例如 <code>enabled</code> 、 <code>dynamic</code> 和 <code>include_in_all</code></li>
</ul>
<h4 id="属性"><a class="header" href="#属性">属性</a></h4>
<ul>
<li>
<p><strong><code>type</code></strong></p>
<p>字段的数据类型，例如 <code>string</code> 或 <code>date</code></p>
</li>
<li>
<p><strong><code>index</code></strong></p>
<p>字段是否应当被当成全文来搜索（ <code>analyzed</code> ），或被当成一个准确的值（ <code>not_analyzed</code> ），还是完全不可被搜索（ <code>no</code> ）</p>
</li>
<li>
<p><strong><code>analyzer</code></strong></p>
<p>确定在索引和搜索时全文字段使用的 <code>analyzer</code></p>
</li>
</ul>
<p>我们将在本书的后续部分讨论其他字段类型，例如 <code>ip</code> 、 <code>geo_point</code> 和 <code>geo_shape</code> 。</p>
<h4 id="元数据-_source-字段"><a class="header" href="#元数据-_source-字段">元数据: _source 字段</a></h4>
<p>默认地，Elasticsearch 在 <code>_source</code> 字段存储代表文档体的JSON字符串，和所有被存储的字段一样， <code>_source</code> 字段在被写入磁盘之前先会被压缩。</p>
<p>这个字段的存储几乎总是我们想要的，因为它意味着下面的这些：</p>
<ul>
<li>没必要从另一个存储库中拉取源数据</li>
<li><em>update</em> 请求需要该字段</li>
<li>可以部分取出某些字段</li>
<li>可以方便重建索引</li>
</ul>
<p><strong>禁用元数据存储</strong></p>
<pre><code class="language-js">PUT /my_index
{
    &quot;mappings&quot;: {
        &quot;my_type&quot;: {
            &quot;_source&quot;: {
                &quot;enabled&quot;:  false
            }
        }
    }
}
</code></pre>
<h4 id="元数据-_all-字段"><a class="header" href="#元数据-_all-字段">元数据: _all 字段</a></h4>
<p><code>_all</code> 字段在新应用的探索阶段，当你还不清楚文档的最终结构时是比较有用的。你可以使用这个字段来做任何查询，并且有很大可能找到需要的文档：</p>
<p><strong>禁用</strong></p>
<pre><code class="language-js">PUT /my_index/_mapping/my_type
{
    &quot;my_type&quot;: {
        &quot;_all&quot;: { &quot;enabled&quot;: false }
    }
}
</code></pre>
<p>过 <code>include_in_all</code> 设置来逐个控制字段是否要包含在 <code>_all</code> 字段中，默认值是 <code>true</code></p>
<p>在一个对象(或根对象)上设置 <code>include_in_all</code> 可以修改这个对象中的所有字段的默认行为。</p>
<p>记住，<code>_all</code> 字段仅仅是一个 经过分词的 <code>string</code> 字段。它使用默认分词器来分析它的值，不管这个值原本所在字段指定的分词器。就像所有 <code>string</code> 字段，你可以配置 <code>_all</code> 字段使用的分词器：</p>
<h4 id="元数据文档标识"><a class="header" href="#元数据文档标识">元数据：文档标识</a></h4>
<p>文档标识与四个元数据字段相关：</p>
<ul>
<li>
<p><strong><code>_id</code></strong></p>
<p>文档的 ID 字符串</p>
</li>
<li>
<p><strong><code>_type</code></strong></p>
<p>文档的类型名</p>
</li>
<li>
<p><strong><code>_index</code></strong></p>
<p>文档所在的索引</p>
</li>
<li>
<p><strong><code>_uid</code></strong></p>
<p><code>_type</code> 和 <code>_id</code> 连接在一起构造成 <code>type#id</code></p>
</li>
</ul>
<p>默认情况下， <code>_uid</code> 字段是被存储（可取回）和索引（可搜索）的。 <code>_type</code> 字段被索引但是没有存储， <code>_id</code> 和 <code>_index</code> 字段则既没有被索引也没有被存储，这意味着它们并不是真实存在的。</p>
<h2 id="动态映射"><a class="header" href="#动态映射">动态映射</a></h2>
<p>当 Elasticsearch 遇到文档中以前 未遇到的字段，它用 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping-intro.html"><em>dynamic mapping</em></a> 来确定字段的数据类型并自动把新的字段添加到类型映射。</p>
<p>有时这是想要的行为有时又不希望这样。通常没有人知道以后会有什么新字段加到文档，但是又希望这些字段被自动的索引。也许你只想忽略它们。如果Elasticsearch是作为重要的数据存储，可能就会期望遇到新字段就会抛出异常，这样能及时发现问题。</p>
<p>幸运的是可以用 <code>dynamic</code> 配置来控制这种行为 ，可接受的选项如下：</p>
<ul>
<li>
<p><strong><code>true</code></strong></p>
<p>动态添加新的字段—缺省</p>
</li>
<li>
<p><strong><code>false</code></strong></p>
<p>忽略新的字段</p>
</li>
<li>
<p><strong><code>strict</code></strong></p>
<p>如果遇到新字段抛出异常</p>
</li>
</ul>
<pre><code class="language-sense">PUT /my_index
{
    &quot;mappings&quot;: {
        &quot;my_type&quot;: {
            &quot;dynamic&quot;:      &quot;strict&quot;, 
            &quot;properties&quot;: {
                &quot;title&quot;:  { &quot;type&quot;: &quot;string&quot;},
                &quot;stash&quot;:  {
                    &quot;type&quot;:     &quot;object&quot;,
                    &quot;dynamic&quot;:  true 
                }
            }
        }
    }
}
</code></pre>
<ul>
<li>
<p>my_type不允许新增字段</p>
</li>
<li>
<p>stash 可以新增字段</p>
</li>
</ul>
<h2 id="自定义动态映射"><a class="header" href="#自定义动态映射">自定义动态映射</a></h2>
<p>有时候，动态映射 <code>规则</code> 可能不太智能，我们可以通过设置去自定义这些规则</p>
<h3 id="日期检测"><a class="header" href="#日期检测">日期检测</a></h3>
<p>日期检测可以通过在根对象上设置 <code>date_detection</code> 为 <code>false</code> 来关闭：</p>
<pre><code class="language-js">PUT /my_index
{
    &quot;mappings&quot;: {
        &quot;my_type&quot;: {
            &quot;date_detection&quot;: false
        }
    }
}
</code></pre>
<p>使用这个映射，字符串将始终作为 <code>string</code> 类型。如果你需要一个 <code>date</code> 字段，你必须手动添加。</p>
<p>Elasticsearch 判断字符串为日期的规则可以通过 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/dynamic-field-mapping.html#date-detection"><code>dynamic_date_formats</code> setting</a> 来设置。</p>
<h3 id="动态模板"><a class="header" href="#动态模板">动态模板</a></h3>
<p>使用 <code>dynamic_templates</code> ，你可以完全控制新检测生成字段的映射。你甚至可以通过<strong>字段名称</strong>或<strong>数据类型</strong>来应用不同的映射。</p>
<p>每个模板都有一个名称，你可以用来描述这个模板的用途， 一个 <code>mapping</code> 来指定映射应该怎样使用，以及至少一个参数 (如 <code>match</code>) 来定义这个模板适用于哪个字段。</p>
<p>模板按照顺序来检测；第一个匹配的模板会被启用。例如，我们给 <code>string</code> 类型字段定义两个模板：</p>
<pre><code class="language-sense">PUT /my_index
{
    &quot;mappings&quot;: {
        &quot;my_type&quot;: {
            &quot;dynamic_templates&quot;: [
                { &quot;es&quot;: {
                      &quot;match&quot;:              &quot;*_es&quot;, 
                      &quot;match_mapping_type&quot;: &quot;string&quot;,
                      &quot;mapping&quot;: {
                          &quot;type&quot;:           &quot;string&quot;,
                          &quot;analyzer&quot;:       &quot;spanish&quot;
                      }
                }},
                { &quot;en&quot;: {
                      &quot;match&quot;:              &quot;*&quot;, 
                      &quot;match_mapping_type&quot;: &quot;string&quot;,
                      &quot;mapping&quot;: {
                          &quot;type&quot;:           &quot;string&quot;,
                          &quot;analyzer&quot;:       &quot;english&quot;
                      }
                }}
            ]
}}}
</code></pre>
<h2 id="缺省映射"><a class="header" href="#缺省映射">缺省映射</a></h2>
<p>通常，一个索引中的所有类型共享相同的字段和设置。 <code>_default_</code> 映射更加方便地指定通用设置，而不是每次创建新类型时都要重复设置。 <code>_default_</code> 映射是新类型的模板。在设置 <code>_default_</code> 映射之后创建的所有类型都将应用这些缺省的设置，除非类型在自己的映射中明确覆盖这些设置。</p>
<pre><code class="language-sense">PUT /my_index
{
    &quot;mappings&quot;: {
        &quot;_default_&quot;: {
            &quot;_all&quot;: { &quot;enabled&quot;:  false }
        },
        &quot;blog&quot;: {
            &quot;_all&quot;: { &quot;enabled&quot;:  true  }
        }
    }
}
</code></pre>
<h2 id="重新索引数据"><a class="header" href="#重新索引数据">重新索引数据</a></h2>
<p>字段 <code>_source</code> 的一个优点是在Elasticsearch中已经有整个文档。你不必从源数据中重建索引，而且那样通常比较慢。</p>
<p>为了有效的重新索引所有在旧的索引中的文档，用 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/scroll.html"><em>scroll</em></a> 从旧的索引检索批量文档 ， 然后用 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/bulk.html"><code>bulk</code> API</a> 把文档推送到新的索引中。</p>
<p>从Elasticsearch v2.3.0开始， <a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html">Reindex API</a> 被引入。它能够对文档重建索引而不需要任何插件或外部工具。</p>
<pre><code class="language-js">GET /old_index/_search?scroll=1m
{
    &quot;query&quot;: {
        &quot;range&quot;: {
            &quot;date&quot;: {
                &quot;gte&quot;:  &quot;2014-01-01&quot;,
                &quot;lt&quot;:   &quot;2014-02-01&quot;
            }
        }
    },
    &quot;sort&quot;: [&quot;_doc&quot;],
    &quot;size&quot;:  1000
}
</code></pre>
<h2 id="索引别名和零停机"><a class="header" href="#索引别名和零停机">索引别名和零停机</a></h2>
<p>重建索引的问题是必须更新应用中的索引名称。 索引别名就是用来解决这个问题的！</p>
<p>索引 <em>别名</em> 就像一个快捷方式或软连接，可以指向一个或多个索引，也可以给任何一个需要索引名的API来使用。<em>别名</em> 带给我们极大的灵活性，允许我们做下面这些：</p>
<ul>
<li>无缝切换索引</li>
<li>索引分组</li>
<li>当做视图使用</li>
</ul>
<p>有两种方式管理别名： <code>_alias</code> 用于单个操作， <code>_aliases</code> 用于执行多个原子级操作。</p>
<pre><code class="language-sense"># 创建索引
PUT /my_index_v1 
# 创建别名
PUT /my_index_v1/_alias/my_index 
# 这个别名指向哪一个索引：
GET /*/_alias/my_index
# 哪些别名指向这个索引：
GET /my_index_v1/_alias/*

# 重新索引
# 然后我们将数据从 my_index_v1 索引到 my_index_v2 
PUT /my_index_v2
{
    &quot;mappings&quot;: {
        &quot;my_type&quot;: {
            &quot;properties&quot;: {
                &quot;tags&quot;: {
                    &quot;type&quot;:   &quot;string&quot;,
                    &quot;index&quot;:  &quot;not_analyzed&quot;
                }
            }
        }
    }
}
</code></pre>
<p>一个别名可以指向多个索引，所以我们在添加别名到新索引的同时必须从旧的索引中删除它。这个操作需要原子化，这意味着我们需要使用 <code>_aliases</code> 操作：</p>
<pre><code class="language-sense">POST /_aliases
{
    &quot;actions&quot;: [
        { &quot;remove&quot;: { &quot;index&quot;: &quot;my_index_v1&quot;, &quot;alias&quot;: &quot;my_index&quot; }},
        { &quot;add&quot;:    { &quot;index&quot;: &quot;my_index_v2&quot;, &quot;alias&quot;: &quot;my_index&quot; }}
    ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="elasticsearch相似度算法"><a class="header" href="#elasticsearch相似度算法">elasticSearch相似度算法</a></h2>
<p><strong>检索词频率</strong></p>
<ul>
<li>检索词 在 某个字段中 出现的频率，频率越高、相关性越高</li>
</ul>
<p><strong>反向文档频率</strong></p>
<ul>
<li>每个检索词 在 文档库中出现的频率，频率越低 相关性越高，频率越高相关性越低</li>
</ul>
<p><strong>字段长度准则</strong></p>
<ul>
<li>字段的长度越长、相关性越低</li>
</ul>
<h2 id="es的评分标准review"><a class="header" href="#es的评分标准review">es的评分标准review</a></h2>
<pre><code class="language-shell">GET /_search?explain 
{
   &quot;query&quot;   : { &quot;match&quot; : { &quot;tweet&quot; : &quot;honeymoon&quot; }}
}
</code></pre>
<pre><code class="language-json">&quot;_explanation&quot;: { 
   &quot;description&quot;: &quot;weight(tweet:honeymoon in 0)
                  [PerFieldSimilarity], result of:&quot;,
   &quot;value&quot;:       0.076713204, //总结
   &quot;details&quot;: [
      {
         &quot;description&quot;: &quot;fieldWeight in 0, product of:&quot;,
         &quot;value&quot;:       0.076713204,
         &quot;details&quot;: [
            {  
               &quot;description&quot;: &quot;tf(freq=1.0), with freq of:&quot;, //tf
               &quot;value&quot;:       1,
               &quot;details&quot;: [
                  {
                     &quot;description&quot;: &quot;termFreq=1.0&quot;,
                     &quot;value&quot;:       1
                  }
               ]
            },
            { 
               &quot;description&quot;: &quot;idf(docFreq=1, maxDocs=1)&quot;, //idf
               &quot;value&quot;:       0.30685282
            },
            { 
               &quot;description&quot;: &quot;fieldNorm(doc=0)&quot;, //字段长度准则
               &quot;value&quot;:        0.25,
            }
         ]
      }
   ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="集群内的原理"><a class="header" href="#集群内的原理">集群内的原理</a></h1>
<h2 id="主旨"><a class="header" href="#主旨">主旨</a></h2>
<p>ElasticSearch 的主旨是随时可用和按需扩容。 而扩容可以通过购买性能更强大（ <em>垂直扩容</em> ，或 <em>纵向扩容</em> ） 或者数量更多的服务器（ <em>水平扩容</em> ，或 <em>横向扩容</em> ）来实现。</p>
<p><strong>垂直扩容与水平扩容</strong></p>
<p>虽然 Elasticsearch 可以获益于更强大的硬件设备，但是垂直扩容是有极限的。 真正的扩容能力是来自于水平扩容—为集群添加更多的节点，并且将负载压力和稳定性分散到这些节点中。</p>
<h2 id="集群"><a class="header" href="#集群">集群</a></h2>
<p><strong>集群组成</strong></p>
<ul>
<li>
<p>一个运行中的 Elasticsearch 实例称为一个节点</p>
</li>
<li>
<p>而集群是由一个或者多个拥有相同 <code>cluster.name</code> 配置的节点组成， 它们共同承担数据和负载的压力</p>
</li>
<li>
<p>当有节点加入集群中或者从集群中移除节点时，集群将会重新平均分布所有的数据。</p>
</li>
</ul>
<p><strong>主节点</strong></p>
<ul>
<li>当一个节点被选举成为 <em>主</em> 节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引或者增加、删除节点等</li>
<li>而主节点并不需要涉及到文档级别的变更和搜索等操作：</li>
</ul>
<p>所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈</p>
<ul>
<li>任何节点都可以成为主节点</li>
</ul>
<p><strong>集群中的请求转发</strong></p>
<ul>
<li>
<p>作为用户，我们可以将请求发送到 <em>集群中的任何节点</em> ，包括主节点</p>
</li>
<li>
<p>每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点</p>
</li>
<li>
<p>无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端</p>
</li>
</ul>
<h2 id="集群健康"><a class="header" href="#集群健康">集群健康</a></h2>
<p>Elasticsearch 的集群监控信息中包含了许多的统计数据，其中最为重要的一项就是 <em>集群健康</em> </p>
<p>它在 <code>status</code> 字段中展示为 <code>green</code> 、 <code>yellow</code> 或者 <code>red</code> 。</p>
<pre><code class="language-sense">GET /_cluster/health
</code></pre>
<p>在一个不包含任何索引的空集群中，它将会有一个类似于如下所示的返回内容：</p>
<p><code>status</code> 字段指示着当前集群在总体上是否工作正常。它的三种颜色含义如下：</p>
<ul>
<li>
<p><strong><code>green</code></strong></p>
<p>所有的主分片和副本分片都正常运行。</p>
</li>
<li>
<p><strong><code>yellow</code></strong></p>
<p>所有的主分片都正常运行，但不是所有的副本分片都正常运行。</p>
</li>
<li>
<p><strong><code>red</code></strong></p>
<p>有主分片没能正常运行。</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-management"><a class="header" href="#data-management">Data management</a></h1>
<p>您存储在Elasticsearch中的数据通常分为以下两类之一:</p>
<ul>
<li>Content: 您要搜索的项目集合，例如产品目录</li>
<li>Time series data: 连续生成的带时间戳的数据流，例如日志条目</li>
</ul>
<ol>
<li>内容可能会经常更新，但内容的值随着时间的推移保持相对恒定</li>
<li>您希望能够快速检索项目，而不管它们的年龄有多大。</li>
<li>时间序列数据会随着时间的推移而不断累积，因此您需要策略来平衡数据的价值与存储数据的成本</li>
<li>随着年龄的增长，它往往变得不那么重要，并且访问频率较低，因此您可以将其移至价格更低，性能更低的硬件</li>
<li>对于最旧的数据，重要的是您可以访问数据。如果查询需要更长的时间才能完成，这是可以接受的。</li>
</ol>
<p>为了帮助您管理数据，Elasticsearch使您能够:</p>
<ul>
<li>Define <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">multiple tiers</a> of data nodes with different performance characteristics：定义具有不同性能特征的多层数据节点。</li>
<li>根据您的性能需求和保留策略，在数据层中自动转换索引，使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html">index lifecycle management</a> (ILM).</li>
<li>利用存储在远程存储库中的可搜索快照 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html">searchable snapshots</a> ，为您的旧索引提供弹性，同时降低运营成本并保持搜索性能。</li>
<li>存储在性能较差的硬件上的数据执行异步搜索 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/async-search-intro.html">asynchronous searches</a> </li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="data-tiers"><a class="header" href="#data-tiers">Data tiers</a></h2>
<p>数据层是具有相同 data Role 的节点的集合，这些节点通常共享相同的硬件配置文件:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html#content-tier">Content tier</a> 节点  处理  诸如  产品目录之类的内容的索引和查询 负载</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html#hot-tier">Hot tier</a> 节点 处理 时间序列数据 (如日志或指标) 的索引负载，并保存您最近的、最频繁访问的数据。</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html#warm-tier">Warm tier</a> 节点保存访问频率较低且很少需要更新的时间序列数据。</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html#cold-tier">Cold tier</a> 节点保存不经常访问且通常不更新的时间序列数据。</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html#frozen-tier">Frozen tier</a> 节点保存很少访问且从未更新的时间序列数据，并保留在可搜索的快照中。</li>
</ul>
<ol>
<li>当您将文档直接索引到特定索引时，它们会无限期地保留在content tier nodes</li>
<li>当您将文档索引到数据流时，它们最初驻留在热层节点上</li>
<li>可以配置  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html">index lifecycle management</a> (ILM)   策略 使其自动 转化时间序列 到   the hot, warm, and cold tiers </li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node">data role</a>  配置在  <code>elasticsearch.yml</code>   。对于高性能节点配置  data_hot, data_content 角色</li>
</ol>
<pre><code>node.roles: [&quot;data_hot&quot;, &quot;data_content&quot;]
</code></pre>
<h3 id="content-tier"><a class="header" href="#content-tier">Content tier</a></h3>
<ol>
<li>存储在  content tier 通常是  物品或文章的集合</li>
<li>与时间序列数据不同，随着时间的推移，内容的值保持相对恒定，因此随着时间的推移，将其移动到具有不同性能特征的层是没有意义的</li>
<li>content tier  nodes 通常针对查询性能进行了优化-它们优先考虑处理能力而不是IO吞吐量，因此它们可以处理复杂的搜索和聚合并快速返回结果</li>
<li>虽然他们也负责索引，但 content data 的摄取速度通常不会像日志和指标等时间序列数据那样高</li>
<li>从弹性的角度来看，此层中的索引应配置为使用一个或多个副本。</li>
<li>内容层是必需的。系统索引和其他不属于数据流的索引会自动分配给内容层。</li>
</ol>
<h3 id="hot-tier"><a class="header" href="#hot-tier">Hot tier</a></h3>
<ol>
<li>hot层是时间序列数据的Elasticsearch入口点，</li>
<li>保存您最近、最常搜索的时间序列数据。热层中的节点需要快速的读写，这需要更多的硬件资源和更快的存储 (ssd)。为了恢复弹性，应将hot层中的索引配置为使用一个或多个副本。</li>
<li>热层是必需的。作为数据流一部分的新索引会自动分配给热层。</li>
</ol>
<h3 id="warm-tier"><a class="header" href="#warm-tier">Warm tier</a></h3>
<ol>
<li>与热层中的最近索引的数据相比，时间序列数据被查询的频率较低，可以移动到温暖层。</li>
<li>温暖层通常保存最近几周的数据。仍然允许更新，但可能很少。</li>
<li>暖层中的节点通常不需要像热层中的节点那样快。</li>
<li>为了恢复弹性，应将温暖层中的索引配置为使用一个或多个副本。</li>
</ol>
<h3 id="cold-tier"><a class="header" href="#cold-tier">Cold tier</a></h3>
<ol>
<li>一旦数据不再被更新，它就可以从温暖层移动到寒冷层，在那里它停留，而不经常被查询</li>
<li>冷层仍然 响应查询请求 ，但是冷层中的数据通常不会更新。</li>
<li>当数据过渡到冷层时，它可以被压缩和缩小</li>
<li>为了弹性， the cold tier can use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#fully-mounted">fully mounted indices</a> of <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-searchable-snapshot.html">searchable snapshots</a>, 消除了对副本的需求。</li>
</ol>
<h3 id="frozen-tier"><a class="header" href="#frozen-tier">Frozen tier</a></h3>
<ol>
<li>一旦数据不再被查询，或者很少被查询，它可能会从冷层移动到冻结层，在那里它会在其余生中停留。</li>
<li>The frozen tier uses <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#partially-mounted">partially mounted indices</a> to store and load data from a snapshot repository. </li>
<li>这降低了本地存储和运营成本，同时仍然让您搜索冻结的数据. </li>
<li>由于Elasticsearch有时必须从快照存储库中获取冻结的数据，因此冻结层上的搜索通常比冷层上的要慢。</li>
<li>We recommend you use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-frozen-node">dedicated nodes</a> in the frozen tier.</li>
</ol>
<h3 id="data-tier-index-allocation"><a class="header" href="#data-tier-index-allocation">Data tier index allocation</a></h3>
<blockquote>
<p>数据分层时的索引分配</p>
</blockquote>
<ol>
<li>创建索引时，默认情况下，Elasticsearch将  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tier-shard-filtering.html#tier-preference-allocation-filter"><code>index.routing.allocation.include._tier_preference</code></a>  设置为data_content，以自动将索引分片分配给content tier.</li>
<li>创建数据流时， <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tier-shard-filtering.html#tier-preference-allocation-filter"><code>index.routing.allocation.include._tier_preference</code></a>  设置为 <code>data_hot</code> </li>
<li>可以手动指定  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">shard allocation filtering</a> settings   以 覆盖默认</li>
<li>您还可以显式设置<em>index.routing.allocation.include._Tier_preference</em>以选择退出默认的基于层的分配。如果将层首选项设置为null，则Elasticsearch会在分配过程中忽略数据层角色。</li>
</ol>
<h3 id="automatic-data-tier-migration"><a class="header" href="#automatic-data-tier-migration">Automatic data tier migration</a></h3>
<blockquote>
<p>自动data tier 迁移</p>
</blockquote>
<ol>
<li>
<p>ILM使用 “迁移” <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-migrate.html">migrate</a>  操作自动将托管索引转换到可用的数据层中。</p>
</li>
<li>
<p>默认情况下，此操作会在每个阶段自动注入。</p>
</li>
<li>
<p>您可以显式指定迁移操作以覆盖默认行为，或者使用分配操作 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-allocate.html">allocate action</a>  手动指定分配规则。</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ilm-manage-the-index-lifecycle"><a class="header" href="#ilm-manage-the-index-lifecycle">ILM: Manage the index lifecycle</a></h1>
<p>您可以配置索引生命周期管理 (ILM) 策略，根据您的性能、弹性和保留要求自动管理索引。例如，您可以使用ILM来:</p>
<ul>
<li>当索引达到一定大小或文档数量时，启动新索引</li>
<li>Create a new index each day, week, or month and archive previous ones</li>
<li>每天、每周或每月创建一个新索引，并存档以前的索引</li>
<li>删除陈旧索引以强制执行数据保留标准</li>
</ul>
<p>您可以通过<em>Kibana Management</em>或<em>ILM api</em>创建和管理索引生命周期策略。为Beats或Logstash Elasticsearch输出插件启用索引生命周期管理时，会自动配置默认策略。</p>
<p>To automatically back up your indices and manage snapshots, use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/getting-started-snapshot-lifecycle-management.html">snapshot lifecycle policies</a>.</p>
<h2 id="ilm-overview"><a class="header" href="#ilm-overview">ILM overview</a></h2>
<p>您可以创建和应用索引生命周期管理 (ILM) 策略，以根据您的性能、弹性和保留要求自动管理索引。
索引生命周期策略可以触发以下操作:</p>
<ul>
<li><strong>Rollover</strong>: 当当前索引达到一定大小，文档数量或年龄时，创建一个新的写入索引。.</li>
<li><strong>Shrink</strong>: 减少索引中的主分片数量。</li>
<li><strong>Force merge</strong>: 触发 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-forcemerge.html">force merge</a> 减少分片中段的数量.</li>
<li><strong>Freeze</strong>: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/freeze-index-api.html">Freezes</a> an index and makes it read-only.</li>
<li><strong>Delete</strong>: Permanently remove an index, including all of its data and metadata.</li>
</ul>
<p>ILM使用 hot-warm-cold架构管理索引，这在处理日志和指标等时间序列数据时很常见。</p>
<p>您可以指定:</p>
<ul>
<li>要滚动到新索引的最大分片大小，文档数量或年龄。</li>
<li>索引不再更新的点，并且可以减少主分片的数量。</li>
<li>何时强制合并以永久删除标记为删除的文档。</li>
<li>可以将索引移动到性能较低的硬件的点。</li>
<li>可用性不是那么关键，并且可以减少副本数量的点。</li>
<li>当可以安全地删除索引时。</li>
</ul>
<p>例如，如果要将atm机组中的指标数据索引到Elasticsearch中，则可能会定义一个策略，该策略显示:</p>
<ol>
<li>当索引的主分片的总大小达到50gb时，滚动到一个新的索引。</li>
<li>将旧索引移至暖阶段，将其标记为只读，然后将其缩小为单个分片。</li>
<li>7天后，将索引移至冷阶段，然后将其移至较便宜的硬件。</li>
<li>一旦达到所需的30天保留期，请删除索引。</li>
</ol>
<p>要使用ILM，集群中的所有节点都必须运行相同的版本。尽管可能可以在混合版本的集群中创建和应用策略，但不能保证它们能够按预期工作。</p>
<p>尝试使用包含群集中所有节点都不支持的操作的策略将导致错误。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="tutorial-automate-rollover-with-ilm"><a class="header" href="#tutorial-automate-rollover-with-ilm">Tutorial: Automate rollover with ILM</a></h2>
<p>当使用基于时间戳序列时，可以使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-streams.html">data stream</a> ，可以阶段性的 roll over 到下一个阶段</p>
<p>可以让你启用 冷热集群架构 以满足 热数据的 性能要求，控制成本，强制执行保留策略，并且仍然可以充分利用您的数据</p>
<p>Data streams 适用于  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-streams.html#data-streams-append-only">append-only</a>  场景</p>
<p>如果频繁 更新或者 跨索引删除 ，推荐 使用  index alias and index template</p>
<p>我们可以使用 ILM 管理 并 rollover 别名所指向的索引，Skip to <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#manage-time-series-data-without-data-streams">Manage time series data without data streams</a>.</p>
<p>To automate rollover and management of a data stream with ILM, you:</p>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-create-policy">Create a lifecycle policy</a> that defines the appropriate phases and actions.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-apply-policy">Create an index template</a> to create the data stream and apply the ILM policy and the indices settings and mappings configurations for the backing indices.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-check-progress">Verify indices are moving through the lifecycle phases</a> as expected.</li>
</ol>
<p>For an introduction to rolling indices, see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-rollover.html">Rollover</a>.</p>
<p><strong>Beats或者Logstash</strong></p>
<ol>
<li>
<p>使用Beats or the Logstash Elasticsearch output plugin, lifecycle policies are set up automatically. </p>
</li>
<li>
<p>You can modify the default policies through <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/example-using-index-lifecycle-policy.html">Kibana Management</a> or the ILM APIs.</p>
</li>
</ol>
<h3 id="create-a-lifecycle-policy"><a class="header" href="#create-a-lifecycle-policy">Create a lifecycle policy</a></h3>
<p>生命周期策略指定索引生命周期中的阶段以及在每个阶段中要执行的操作。生命周期最多可以有五个阶段:  <code>hot</code>, <code>warm</code>, <code>cold</code>, <code>frozen</code>, and <code>delete</code>.</p>
<p>例如，您可以定义一个具有两个阶段的timeseries_policy:</p>
<ul>
<li>一个热阶段，它定义了一个rollover操作，用于指定索引在达到<em>50 gb</em>的<em>max_primary_shard_size</em>或30天的max_age时滚动。</li>
<li>一个 “删除” 阶段，将min_age设置为在翻转后90天删除索引。</li>
</ul>
<p><em>min_age</em>值是相对于<em>rollover time</em>时间，而不是索引创建时间。</p>
<p><strong>APIOrKibana</strong></p>
<ol>
<li>
<p>You can create the policy through Kibana or with the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-put-lifecycle.html">create or update policy</a> API. </p>
</li>
<li>
<p>To create the policy from Kibana, open the menu and go to <strong>Stack Management &gt; Index Lifecycle Policies</strong>. Click <strong>Create policy</strong>.</p>
</li>
</ol>
<h3 id="使用索引模板创建数据流来应用声明周期策略"><a class="header" href="#使用索引模板创建数据流来应用声明周期策略">使用索引模板创建数据流来应用声明周期策略</a></h3>
<p>To set up a data stream, first create an index template to specify the lifecycle policy. </p>
<p>Because the template is for a data stream, it must also include a <code>data_stream</code> definition.</p>
<p>For example, you might create a <code>timeseries_template</code> to use for a future data stream named <code>timeseries</code>.</p>
<p>To enable the ILM to manage the data stream, the template configures one ILM setting:</p>
<ul>
<li><code>index.lifecycle.name</code> specifies the name of the lifecycle policy to apply to the data stream.</li>
</ul>
<p>You can use the Kibana Create template wizard to add the template. From Kibana, open the menu and go to <strong>Stack Management &gt; Index Management</strong>. In the <strong>Index Templates</strong> tab, click <strong>Create template</strong>.</p>
<p>This wizard invokes the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-put-template.html">create or update index template API</a> to create the index template with the options you specify.</p>
<h3 id="create-the-data-stream"><a class="header" href="#create-the-data-stream">Create the data stream</a></h3>
<p>要开始工作，请将文档索引到索引模板的<em>index_patterns</em>中定义的名称或通配符模式中。只要现有数据流、索引或索引别名尚未使用该名称，索引请求就会自动创建具有单个后备索引的相应数据流。Elasticsearch会自动将请求的文档索引到此支持索引中，该索引也充当流的写入索引。</p>
<p>For example, the following request creates the <code>timeseries</code> data stream and the first generation backing index called <code>.ds-timeseries-2099.03.08-000001</code>.</p>
<p>例如，以下请求创建timeseries数据流和调用的第一代后备索引。ds-timeseries-2099.03.08-000001。</p>
<pre><code class="language-console">POST timeseries/_doc
{
  &quot;message&quot;: &quot;logged the request&quot;,
  &quot;@timestamp&quot;: &quot;1591890611&quot;
}
</code></pre>
<p>当满足生命周期策略中的rollover条件时，rollover操作:</p>
<ul>
<li>创建第二代支持索引，名为。ds-timeseries-2099.03.08-000002。因为它是timeseries数据流的后备索引，所以来自timeseries_template索引模板的配置将应用于新索引。</li>
<li>由于它是timeseries数据流的最新一代索引，因此新创建的支持索引。ds-timeseries-2099.03.08-000002成为数据流的写索引。</li>
</ul>
<p>每次满足翻转条件时，都会重复此过程。您可以使用timeseries数据流名称搜索由timeseries_policy管理的所有数据流的支持索引。写操作被路由到当前写索引。读取操作将由所有支持索引处理。</p>
<h3 id="check-lifecycle-progress"><a class="header" href="#check-lifecycle-progress">Check lifecycle progress</a></h3>
<p>要获取托管索引的状态信息，请使用ILM explain API。这让你找出这样的事情:</p>
<ul>
<li>索引处于什么阶段以及何时进入该阶段。</li>
<li>当前的操作以及正在执行的步骤。</li>
<li>如果发生任何错误或进度被阻止。</li>
</ul>
<p>For example, the following request gets information about the <code>timeseries</code> data stream’s backing indices:</p>
<pre><code class="language-console">GET .ds-timeseries-*/_ilm/explain
</code></pre>
<p>The following response shows the data stream’s first generation backing index is waiting for the <code>hot</code> phase’s <code>rollover</code> action. It remains in this state and ILM continues to call <code>check-rollover-ready</code> until a rollover condition is met.</p>
<pre><code class="language-console-result">{
  &quot;indices&quot;: {
    &quot;.ds-timeseries-2099.03.07-000001&quot;: {
      &quot;index&quot;: &quot;.ds-timeseries-2099.03.07-000001&quot;,
      &quot;managed&quot;: true,
      &quot;policy&quot;: &quot;timeseries_policy&quot;,             
      &quot;lifecycle_date_millis&quot;: 1538475653281,
      &quot;age&quot;: &quot;30s&quot;,                              
      &quot;phase&quot;: &quot;hot&quot;,
      &quot;phase_time_millis&quot;: 1538475653317,
      &quot;action&quot;: &quot;rollover&quot;,
      &quot;action_time_millis&quot;: 1538475653317,
      &quot;step&quot;: &quot;check-rollover-ready&quot;,            
      &quot;step_time_millis&quot;: 1538475653317,
      &quot;phase_execution&quot;: {
        &quot;policy&quot;: &quot;timeseries_policy&quot;,
        &quot;phase_definition&quot;: {                    
          &quot;min_age&quot;: &quot;0ms&quot;,
          &quot;actions&quot;: {
            &quot;rollover&quot;: {
              &quot;max_primary_shard_size&quot;: &quot;50gb&quot;,
              &quot;max_age&quot;: &quot;30d&quot;
            }
          }
        },
        &quot;version&quot;: 1,
        &quot;modified_date_in_millis&quot;: 1539609701576
      }
    }
  }
}
</code></pre>
<h3 id="manage-time-series-data-without-data-streams"><a class="header" href="#manage-time-series-data-without-data-streams">Manage time series data without data streams</a></h3>
<p>尽管数据流是扩展和管理时间序列数据的便捷方法，但它们被设计为仅附加。我们认识到可能存在一些用例，其中需要更新或删除数据，并且数据流不支持直接删除和更新请求，因此索引api需要直接用于数据流的支持索引。</p>
<p>在这些情况下，您可以使用索引别名来管理包含时间序列数据的索引，并定期滚动到新索引。</p>
<p>要使用索引别名自动使用ILM对时间序列索引进行展期和管理，您可以:</p>
<ol>
<li>Create a lifecycle policy that defines the appropriate phases and actions. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-create-policy">Create a lifecycle policy</a> above.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-alias-apply-policy">Create an index template</a> to apply the policy to each new index.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-alias-bootstrap">Bootstrap an index</a> as the initial write index.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-alias-check-progress">Verify indices are moving through the lifecycle phases</a> as expected.</li>
</ol>
<h3 id="create-an-index-template-to-apply-the-lifecycle-policy"><a class="header" href="#create-an-index-template-to-apply-the-lifecycle-policy">Create an index template to apply the lifecycle policy</a></h3>
<p>要在过渡时自动将生命周期策略应用于新的写入索引，请在用于创建新索引的索引模板中指定策略。
例如，您可以创建一个timeseries_template，该模板应用于名称与timeseries-* 索引模式匹配的新索引。
要启用自动翻转，模板将配置两个ILM设置:</p>
<ul>
<li><code>index.lifecycle.name</code> specifies the name of the lifecycle policy to apply to new indices that match the index pattern.</li>
<li><code>index.lifecycle.rollover_alias</code> specifies the index alias to be rolled over when the rollover action is triggered for an index.</li>
</ul>
<p>You can use the Kibana Create template wizard to add the template. To access the wizard, open the menu and go to <strong>Stack Management &gt; Index Management</strong>. In the <strong>Index Templates</strong> tab, click <strong>Create template</strong>.</p>
<h3 id="bootstrap-the-initial-time-series-index-with-a-write-index-alias"><a class="header" href="#bootstrap-the-initial-time-series-index-with-a-write-index-alias">Bootstrap the initial time series index with a write index alias</a></h3>
<p>要开始工作，您需要引导一个初始索引，并将其指定为索引模板中指定的翻转别名的写索引。此索引的名称必须与模板的索引模式匹配，并以数字结尾。在翻转时，此值递增以生成新索引的名称。</p>
<p>For example, the following request creates an index called <code>timeseries-000001</code> and makes it the write index for the <code>timeseries</code> alias.</p>
<pre><code class="language-console">PUT timeseries-000001
{
  &quot;aliases&quot;: {
    &quot;timeseries&quot;: {
      &quot;is_write_index&quot;: true
    }
  }
}
</code></pre>
<p>When the rollover conditions are met, the <code>rollover</code> action:</p>
<ul>
<li>Creates a new index called <code>timeseries-000002</code>. This matches the <code>timeseries-*</code> pattern, so the settings from <code>timeseries_template</code> are applied to the new index.</li>
<li>Designates the new index as the write index and makes the bootstrap index read-only.</li>
</ul>
<p>This process repeats each time rollover conditions are met. You can search across all of the indices managed by the <code>timeseries_policy</code> with the <code>timeseries</code> alias. Write operations are routed to the current write index.</p>
<h3 id="check-lifecycle-progress-1"><a class="header" href="#check-lifecycle-progress-1">Check lifecycle progress</a></h3>
<p>Retrieving the status information for managed indices is very similar to the data stream case. See the data stream <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/getting-started-index-lifecycle-management.html#ilm-gs-check-progress">check progress section</a> for more information. The only difference is the indices namespace, so retrieving the progress will entail the following api call:</p>
<pre><code class="language-console">GET timeseries-*/_ilm/explain
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="frozen-indices"><a class="header" href="#frozen-indices">Frozen Indices</a></h2>
<p>经常搜索的索引保存在内存中，因为需要时间来重建它们并帮助进行有效的搜索。</p>
<p>另一方面，可能有一些我们很少访问的索引。这些索引不需要占用内存，可以在需要时重新构建。这种索引被称为冻结索引。</p>
<p>Elasticsearch在每次搜索该分片时都会构建冻结索引的每个分片的瞬态数据结构，并在搜索完成后立即丢弃这些数据结构。</p>
<p>由于Elasticsearch不会在内存中维护这些瞬态数据结构，因此冻结的索引比正常索引消耗的堆要少得多。</p>
<p>This allows for a much higher disk-to-heap ratio than would otherwise be possible.</p>
<h3 id="example-for-freezing-and-unfreezing"><a class="header" href="#example-for-freezing-and-unfreezing">Example for Freezing and Unfreezing</a></h3>
<pre><code>POST /index_name/_freeze
POST /index_name/_unfreeze
</code></pre>
<p>冻结索引的搜索预计会缓慢执行。</p>
<p>冻结索引不适合高搜索负载。</p>
<p>冻结索引的搜索可能需要几秒钟或几分钟才能完成，即使在索引未冻结的情况下以毫秒完成相同的搜索。</p>
<p>冷冻后，不能写入</p>
<h3 id="searching-a-frozen-index"><a class="header" href="#searching-a-frozen-index">Searching a Frozen Index</a></h3>
<p>每个节点并发加载的冻结索引的数量受search_throttled threadpool中的线程数量的限制，默认情况下为1。要包含冻结索引，必须使用查询参数 − ignore_throttled = false执行搜索请求。</p>
<pre><code>GET /index_name/_search?q=user:tpoint&amp;ignore_throttled=false
</code></pre>
<h3 id="monitoring-frozen-indices"><a class="header" href="#monitoring-frozen-indices">Monitoring Frozen Indices</a></h3>
<pre><code>GET /_cat/indices/index_name?v&amp;h=i,sth
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h3 id="前序"><a class="header" href="#前序">前序</a></h3>
<p>综合上述拆解分析可知：</p>
<ol>
<li>
<p>有了：冷热集群架构，集群的不同节点有了明确的角色之分，冷热数据得以物理隔离，SSD 固态盘使用效率会更高。</p>
</li>
<li>
<p>有了：rollover 滚动索引，索引可以基于文档个数、时间、占用磁盘容量滚动升级，实现了索引的动态变化。</p>
</li>
<li>
<p>有了：Shrink 压缩索引、Frozen 冷冻索引，索引可以物理层面压缩、冷冻，分别释放了磁盘空间和内存空间，提高了集群的可用性。</p>
</li>
<li>
<p>除此之外，还有：Force merge 段合并、Delete 索引数据删除等操作，索引的“生、老、病、死”的全生命周期的</p>
</li>
</ol>
<p>如上指令单个操作，非常麻烦和繁琐，有没有更为快捷的方法呢？</p>
<p>第一：命令行可以 DSL 大综合实现。</p>
<p>第二：可以借助 Kibana 图形化界面实现。</p>
<h3 id="index-lifecycle"><a class="header" href="#index-lifecycle">Index lifecycle</a></h3>
<p>ILM defines five index lifecycle <em>phases</em>:</p>
<h4 id="hot"><a class="header" href="#hot"><strong>Hot</strong></a></h4>
<p>频繁更新与查询</p>
<h4 id="warm"><a class="header" href="#warm"><strong>Warm</strong></a></h4>
<p>索引不再更新，但仍在查询中。</p>
<h4 id="cold"><a class="header" href="#cold"><strong>Cold</strong></a></h4>
<p>不在被更新。查询得不是很频繁</p>
<h4 id="frozen"><a class="header" href="#frozen"><strong>Frozen</strong></a></h4>
<p>该索引不再被更新，并且很少被查询。信息仍然需要搜索，但是如果这些查询非常慢，也可以。</p>
<h4 id="delete"><a class="header" href="#delete"><strong>Delete</strong></a></h4>
<p>该索引不再需要，可以安全地删除。</p>
<p>索引的生命周期策略指定了哪些阶段适用哪些操作，在每个阶段中执行了哪些操作以及何时在阶段之间过渡。</p>
<p>您可以在创建索引时手动应用生命周期策略。</p>
<p>对于时间序列索引，您需要将生命周期策略与用于在序列中创建新索引的索引模板相关联。</p>
<p>当索引滚动时，手动应用的策略不会自动应用于新索引。</p>
<h3 id="phase-transitions"><a class="header" href="#phase-transitions">Phase transitions</a></h3>
<p>ILM根据索引的年龄在整个生命周期中移动索引。</p>
<ol>
<li>
<p>为了控制这些转换的时间，您可以为每个阶段设置一个最小年龄。</p>
</li>
<li>
<p>为了使索引移至下一阶段，当前阶段中的所有操作必须完成，并且索引必须早于下一阶段的最小年龄。配置的最小年龄必须在后续阶段之间增加，</p>
</li>
<li>
<p>例如，最小年龄为10天的 “warm” 阶段之后，cold阶段只能 大于 10天</p>
</li>
<li>
<p>最小年龄默认是 0：这就导致 ILM 完成阶段中的action 之后 就立马 转移到下一阶段</p>
</li>
<li>
<p>如果索引 有未分配的 分片，集群状态 为 黄色，索引仍然能改 转移 到下一个阶段 </p>
</li>
<li>
<p>但是，由于Elasticsearch只能在绿色集群上执行某些清理任务，因此可能会产生意外的副作用。</p>
</li>
<li>
<p>为了避免增加磁盘使用率和可靠性问题，请及时解决任何群集运行状况问题。</p>
</li>
</ol>
<h3 id="phase-execution"><a class="header" href="#phase-execution">Phase execution</a></h3>
<p>ILM控制执行一个阶段中的动作的顺序以及执行哪些步骤来执行每个动作的必要索引操作。</p>
<p>当索引进入某一阶段时，ILM会在索引元数据中缓存阶段定义。</p>
<p>这样可以确保策略更新不会将索引置于永远无法退出阶段的状态</p>
<p>如果可以安全地应用更改，则ILM会更新缓存的阶段定义。如果不能，则使用缓存的定义继续执行阶段。</p>
<ol>
<li>
<p>ILM定期运行，检查索引是否符合策略标准，并执行所需的任何步骤。</p>
</li>
<li>
<p>为了避免竞争条件，ILM可能需要运行多次才能执行完成操作所需的所有步骤。</p>
</li>
<li>
<p>例如，如果ILM确定索引已满足展期标准，则它开始执行完成展期操作所需的步骤。</p>
</li>
<li>
<p>如果它达到了前进到下一步不安全的地步，则执行停止。</p>
</li>
<li>
<p>下次ILM运行时，ILM会从停止的地方开始执行。这意味着，即使将indices.lifecycle.poll_interval设置为10分钟，并且索引满足翻转标准，也可能需要20分钟才能完成翻转。</p>
</li>
</ol>
<h2 id="各生命周期-actions-设定"><a class="header" href="#各生命周期-actions-设定">各生命周期 Actions 设定</a></h2>
<h3 id="hot-阶段"><a class="header" href="#hot-阶段">Hot 阶段</a></h3>
<ul>
<li>基于：max_age=3天、最大文档数为5、最大size为：50gb rollover 滚动索引。</li>
<li>设置优先级为：100（值越大，优先级越高）。</li>
</ul>
<h3 id="warm-阶段"><a class="header" href="#warm-阶段">Warm 阶段 	 	 	 	</a></h3>
<ul>
<li>实现段合并，max_num_segments 设置为1.</li>
<li>副本设置为 0。</li>
<li>数据迁移到：warm 节点。</li>
<li>优先级设置为：50。</li>
</ul>
<h3 id="cold-阶段"><a class="header" href="#cold-阶段">Cold 阶段</a></h3>
<ul>
<li>冷冻索引</li>
<li>数据迁移到冷节点</li>
</ul>
<h3 id="delete-阶段"><a class="header" href="#delete-阶段">Delete 阶段 	</a></h3>
<ul>
<li>删除索引</li>
</ul>
<p>关于触发滚动的条件：</p>
<ul>
<li>Hot 阶段的触发条件：手动创建第一个满足模板要求的索引。</li>
<li>其余阶段触发条件：min_age，索引自创建后的时间。</li>
</ul>
<p>时间类似：业务里面的 热节点保留 3 天，温节点保留 7 天，冷节点保留 30 天的概念。</p>
<h3 id="实战"><a class="header" href="#实战">实战</a></h3>
<h4 id="节点配置"><a class="header" href="#节点配置">节点配置</a></h4>
<ul>
<li>节点 node-022：主节点+数据节点+热节点（Hot）。</li>
<li>节点 node-023：主节点+数据节点+温节点（Warm）。</li>
<li>节点 node-024：主节点+数据节点+冷节点（Cold）。 </li>
</ul>
<p><strong>节点属性配置</strong></p>
<pre><code>- node.attr.box_type: hot
- node.attr.box_type: warm
- node.attr.box_type: cold
</code></pre>
<h4 id="集群刷新频率"><a class="header" href="#集群刷新频率">集群刷新频率</a></h4>
<pre><code>PUT _cluster/settings
{
  &quot;persistent&quot;: {
    &quot;indices.lifecycle.poll_interval&quot;: &quot;1s&quot;
  }
}
</code></pre>
<h4 id="新建ilm-policy"><a class="header" href="#新建ilm-policy">新建ILM Policy</a></h4>
<pre><code>
# step2:测试需要，值调的很小
PUT _ilm/policy/my_custom_policy_filter
{
  &quot;policy&quot;: {
    &quot;phases&quot;: {
      &quot;hot&quot;: {
        &quot;actions&quot;: {
          &quot;rollover&quot;: {
            &quot;max_age&quot;: &quot;3d&quot;,
            &quot;max_docs&quot;: 5,
            &quot;max_size&quot;: &quot;50gb&quot;
          },
          &quot;set_priority&quot;: {
            &quot;priority&quot;: 100
          }
        }
      },
      &quot;warm&quot;: {
        &quot;min_age&quot;: &quot;15s&quot;,
        &quot;actions&quot;: {
          &quot;forcemerge&quot;: {
            &quot;max_num_segments&quot;: 1
          },
          &quot;allocate&quot;: {
            &quot;require&quot;: {
              &quot;box_type&quot;: &quot;warm&quot;
            },
            &quot;number_of_replicas&quot;: 0
          },
          &quot;set_priority&quot;: {
            &quot;priority&quot;: 50
          }
        }
      },
      &quot;cold&quot;: {
        &quot;min_age&quot;: &quot;30s&quot;,
        &quot;actions&quot;: {
          &quot;allocate&quot;: {
            &quot;require&quot;: {
              &quot;box_type&quot;: &quot;cold&quot;
            }
          },
          &quot;freeze&quot;: {}
        }
      },
      &quot;delete&quot;: {
        &quot;min_age&quot;: &quot;45s&quot;,
        &quot;actions&quot;: {
          &quot;delete&quot;: {}
        }
      }
    }
  }
}
</code></pre>
<h4 id="索引模板"><a class="header" href="#索引模板">索引模板</a></h4>
<pre><code>
# step3:创建模板，关联配置的ilm_policy
PUT _index_template/timeseries_template
{
  &quot;index_patterns&quot;: [&quot;timeseries-*&quot;],                 
  &quot;template&quot;: {
    &quot;settings&quot;: {
      &quot;number_of_shards&quot;: 1,
      &quot;number_of_replicas&quot;: 0,
      &quot;index.lifecycle.name&quot;: &quot;my_custom_policy_filter&quot;,      
      &quot;index.lifecycle.rollover_alias&quot;: &quot;timeseries&quot;,
      &quot;index.routing.allocation.require.box_type&quot;: &quot;hot&quot;
    }
  }
}


</code></pre>
<p><strong>新建初始索引</strong></p>
<pre><code>
# step4:创建起始索引（便于滚动）
PUT timeseries-000001
{
  &quot;aliases&quot;: {
    &quot;timeseries&quot;: {
      &quot;is_write_index&quot;: true
    }
  }
}
</code></pre>
<h4 id="插入数据"><a class="header" href="#插入数据">插入数据</a></h4>
<pre><code>
# step5：插入数据
PUT timeseries/_bulk
{&quot;index&quot;:{&quot;_id&quot;:1}}
{&quot;title&quot;:&quot;testing 01&quot;}
{&quot;index&quot;:{&quot;_id&quot;:2}}
{&quot;title&quot;:&quot;testing 02&quot;}
{&quot;index&quot;:{&quot;_id&quot;:3}}
{&quot;title&quot;:&quot;testing 03&quot;}
{&quot;index&quot;:{&quot;_id&quot;:4}}
{&quot;title&quot;:&quot;testing 04&quot;}

# step6：临界值（会滚动）
PUT timeseries/_bulk
{&quot;index&quot;:{&quot;_id&quot;:5}}
{&quot;title&quot;:&quot;testing 05&quot;}

# 下一个索引数据写入
PUT timeseries/_bulk
{&quot;index&quot;:{&quot;_id&quot;:6}}
{&quot;title&quot;:&quot;testing 06&quot;}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="rollover-api"><a class="header" href="#rollover-api">Rollover API</a></h2>
<p>Creates a new index for a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-streams.html">data stream</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-add-alias.html">index alias</a>.</p>
<h3 id="request"><a class="header" href="#request">Request</a></h3>
<pre><code>POST /&lt;rollover-target&gt;/_rollover/
POST /&lt;rollover-target&gt;/_rollover/&lt;target-index&gt;
</code></pre>
<h3 id="description"><a class="header" href="#description">Description</a></h3>
<p>推荐使用 ILM’s <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-rollover.html"><code>rollover</code></a> action to automate rollovers. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-index-lifecycle.html">Index lifecycle</a>.</p>
<p>rollover API为数据流或索引别名创建新索引。API的行为取决于翻转目标。</p>
<p><strong>Roll over a data stream</strong></p>
<p>If you roll over a data stream, the API creates a new write index for the stream. The stream’s previous write index becomes a regular backing index. A rollover also increments the data stream’s generation. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-streams.html#data-streams-rollover">Rollover</a>.</p>
<p><strong>Roll over an index alias with a write index</strong></p>
<p>Prior to Elasticsearch 7.9, you would typically use an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-aliases.html">index alias</a> with a write index to manage time series data. Data streams replace this functionality, require less maintenance, and automatically integrate with <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">data tiers</a>.</p>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/set-up-a-data-stream.html#convert-index-alias-to-data-stream">Convert an index alias to a data stream</a>.</p>
<ol>
<li>如果索引别名指向多个索引。则 其中一个必须是  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-aliases.html#write-index">write index</a>.</li>
<li>rollover API  为 索引别名  创建新索引 。并设置  <code>is_write_index</code> = <code>true</code></li>
<li>同样给之前的 索引 设置  <code>is_write_index</code>  = false</li>
</ol>
<p><strong>Roll over an index alias with one index</strong></p>
<p>如果滚动仅指向一个索引的索引别名，则API会为该别名创建一个新索引，并从该别名中删除原始索引。</p>
<h4 id="increment-index-names-for-an-alias"><a class="header" href="#increment-index-names-for-an-alias">Increment index names for an alias</a></h4>
<ol>
<li>滚动索引时可以执行索引名</li>
<li>如果 没有指定索引名，且当前索引以  <code> -number</code> 结尾 例如  <code>my-index-000001</code> or <code>my-index-3</code>, 则number递增</li>
<li>这个 数字 是填充0的六位数</li>
</ol>
<p><strong>Use date math with index alias rollovers</strong></p>
<ol>
<li>可以在 time series data  中 是会用   <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/date-math-index-names.html">date math</a> </li>
<li><code>&lt;my-index-{now/d}-000001&gt;</code>  see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-rollover-index.html#roll-over-index-alias-with-write-index">Roll over an index alias with a write index</a>.</li>
</ol>
<h4 id="wait-for-active-shards"><a class="header" href="#wait-for-active-shards">Wait for active shards</a></h4>
<p>A rollover creates a new index and is subject to the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-create-index.html#create-index-wait-for-active-shards"><code>wait_for_active_shards</code></a> setting.</p>
<h3 id="path-parameters"><a class="header" href="#path-parameters">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;rollover-target&gt;</code></strong></p>
<p>(Required, string) Name of the data stream or index alias to roll over.</p>
</li>
<li>
<p><strong><code>&lt;target-index&gt;</code></strong></p>
<p>(Optional, string) Name of the index to create. Supports <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/date-math-index-names.html">date math</a>.</p>
<ol>
<li>Data streams 不支持</li>
<li>如果名称没有 以 -number 结尾 则 必须要指定该值</li>
<li>只能小写，不允许  <code>\</code>, <code>/</code>, <code>*</code>, <code>?</code> <code>&quot;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>|</code>,  (space character), <code>,</code>, <code>#</code>:</li>
<li>不能超过255个字节</li>
<li>starting with <code>.</code> are deprecated, except for <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#index-hidden">hidden indices</a> and internal indices managed by plugins</li>
</ol>
</li>
</ul>
<h3 id="query-parameters"><a class="header" href="#query-parameters">Query parameters</a></h3>
<p><strong><code>dry_run</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, checks whether the current index matches one or more specified <code>conditions</code> but does not perform a rollover. Defaults to <code>false</code>.</p>
<p><strong><code>wait_for_active_shards</code></strong></p>
<p>(Optional, string) The number of shard copies that must be active before proceeding with the operation. Set to <code>all</code> or any positive integer up to the total number of shards in the index (<code>number_of_replicas+1</code>). Default: 1, the primary shard.</p>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-index_.html#index-wait-for-active-shards">Active shards</a>.</p>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<h3 id="request-body"><a class="header" href="#request-body">Request body</a></h3>
<h4 id="aliases"><a class="header" href="#aliases"><strong><code>aliases</code></strong></a></h4>
<p>别名对象</p>
<h4 id="properties-of-aliases-objects"><a class="header" href="#properties-of-aliases-objects">Properties of aliases objects</a></h4>
<p><strong><code>filter</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl.html">Query DSL object</a>) Query used to limit documents the alias can access.</p>
<p><strong><code>index_routing</code></strong></p>
<p>(Optional, string) Value used to route indexing operations to a specific shard. If specified, this overwrites the <code>routing</code> value for indexing operations.</p>
<p><strong><code>is_hidden</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, the alias is <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/multi-index.html#hidden">hidden</a>. Defaults to <code>false</code>. All indices for the alias must have the same <code>is_hidden</code> value.</p>
<p><strong><code>is_write_index</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, the index is the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-aliases.html#write-index">write index</a> for the alias. Defaults to <code>false</code>.</p>
<p><strong><code>routing</code></strong></p>
<p>(Optional, string) Value used to route indexing and search operations to a specific shard.</p>
<p><strong><code>search_routing</code></strong></p>
<p>(Optional, string) Value used to route search operations to a specific shard. If specified, this overwrites the <code>routing</code> value for search operations.</p>
<h4 id="conditions"><a class="header" href="#conditions"><strong><code>conditions</code></strong></a></h4>
<ol>
<li>
<p>只当满足 至少一个或者多个条件才 rollover</p>
</li>
<li>
<p>如果未指定此参数，则Elasticsearch会无条件执行翻转。</p>
</li>
<li>
<p>要翻转索引，在请求的时刻当前索引必须满足 条件</p>
</li>
<li>
<p>es不会 监控 索引状态</p>
</li>
<li>
<p>To automate rollover, use ILM’s <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-rollover.html"><code>rollover</code></a> instead.</p>
</li>
</ol>
<h4 id="properties-of-conditions"><a class="header" href="#properties-of-conditions">Properties of conditions</a></h4>
<p><strong><code>max_age</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) </p>
<ol>
<li>至 索引从创建时间以来经历的最大时间</li>
<li>even if the index origination date is configured to a custom date, such as when using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-settings.html#index-lifecycle-parse-origination-date">index.lifecycle.parse_origination_date</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-settings.html#index-lifecycle-origination-date">index.lifecycle.origination_date</a> settings.</li>
</ol>
<p><strong><code>max_docs</code></strong></p>
<p>(Optional, integer) Triggers rollover after the specified maximum number of documents is reached. Documents added since the last refresh are not included in the document count. The document count does <strong>not</strong> include documents in replica shards.</p>
<p><strong><code>max_size</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#byte-units">byte units</a>) Triggers rollover when the index reaches a certain size. This is the total size of all primary shards in the index. Replicas are not counted toward the maximum index size.</p>
<p>To see the current index size, use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cat-indices.html">_cat indices</a> API. The <code>pri.store.size</code> value shows the combined size of all primary shards.</p>
<p><strong><code>max_primary_shard_size</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#byte-units">byte units</a>) Triggers rollover when the largest primary shard in the index reaches a certain size. This is the maximum size of the primary shards in the index. As with <code>max_size</code>, replicas are ignored.</p>
<p>To see the current shard size, use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cat-shards.html">_cat shards</a> API. The <code>store</code> value shows the size each shard, and <code>prirep</code> indicates whether a shard is a primary (<code>p</code>) or a replica (<code>r</code>).</p>
<h4 id="mappings"><a class="header" href="#mappings"><strong><code>mappings</code></strong></a></h4>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping.html">mapping object</a>) Mapping for fields in the index. If specified, this mapping can include:</p>
<ul>
<li>Field names</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-types.html">Field data types</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-params.html">Mapping parameters</a></li>
</ul>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping.html">Mapping</a>.</p>
<p>Data streams do not support this parameter.</p>
<h4 id="settings"><a class="header" href="#settings"><strong><code>settings</code></strong></a></h4>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#index-modules-settings">index setting object</a>) Configuration options for the index. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#index-modules-settings">Index Settings</a>.</p>
<p>Data streams do not support this parameter.</p>
<h3 id="response-body"><a class="header" href="#response-body">Response body</a></h3>
<p><strong><code>acknowledged</code></strong></p>
<p>(Boolean) If <code>true</code>, the request received a response from the master node within the <code>timeout</code> period.</p>
<p><strong><code>shards_acknowledged</code></strong></p>
<p>(Boolean) If <code>true</code>, the request received a response from <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-index_.html#index-wait-for-active-shards">active shards</a> within the <code>master_timeout</code> period.</p>
<p><strong><code>old_index</code></strong></p>
<p>(string) Previous index for the data stream or index alias. For data streams and index aliases with a write index, this is the previous write index.</p>
<p><strong><code>new_index</code></strong></p>
<p>(string) Index created by the rollover. For data streams and index aliases with a write index, this is the current write index.</p>
<p><strong><code>rolled_over</code></strong></p>
<p>(Boolean) If <code>true</code>, the data stream or index alias rolled over.</p>
<p><strong><code>dry_run</code></strong></p>
<p>(Boolean) If <code>true</code>, Elasticsearch did not perform the rollover.</p>
<p><strong><code>condition</code></strong></p>
<p>(object) Result of each condition specified in the request’s <code>conditions</code>. If no conditions were specified, this is an empty object.</p>
<p>(Boolean) The key is each condition. The value is its result. If <code>true</code>, the index met the condition at rollover.</p>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<h4 id="roll-over-a-data-stream"><a class="header" href="#roll-over-a-data-stream">Roll over a data stream</a></h4>
<pre><code class="language-console">POST my-data-stream/_rollover


POST my-data-stream/_rollover
{
  &quot;conditions&quot;: {
    &quot;max_age&quot;: &quot;7d&quot;,
    &quot;max_docs&quot;: 1000,
    &quot;max_primary_shard_size&quot;: &quot;50gb&quot;
  }
}

</code></pre>
<h4 id="按日志rollover-并设置写索引"><a class="header" href="#按日志rollover-并设置写索引"><strong>按日志rollover 并设置写索引</strong></a></h4>
<pre><code class="language-console">PUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E
{
  &quot;aliases&quot;: {
    &quot;my-alias&quot;: {
      &quot;is_write_index&quot;: true
    }
  }
}
</code></pre>
<p>如果别名的索引名称使用date math，并且您定期滚动索引，则可以使用日期数学来缩小搜索范围。例如，以下搜索目标是最近三天创建的索引。</p>
<pre><code class="language-console"># GET /&lt;my-index-{now/d}-*&gt;,&lt;my-index-{now/d-1d}-*&gt;,&lt;my-index-{now/d-2d}-*&gt;/_search
GET /%3Cmy-index-%7Bnow%2Fd%7D-*%3E%2C%3Cmy-index-%7Bnow%2Fd-1d%7D-*%3E%2C%3Cmy-index-%7Bnow%2Fd-2d%7D-*%3E/_search
</code></pre>
<h4 id="roll-over-an-index-alias-with-one-index"><a class="header" href="#roll-over-an-index-alias-with-one-index">Roll over an index alias with one index</a></h4>
<pre><code class="language-console"># PUT &lt;my-index-{now/d}-000001&gt;
PUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E
{
  &quot;aliases&quot;: {
    &quot;my-write-alias&quot;: { }
  }
}
</code></pre>
<h4 id="specify-settings-during-a-rollover"><a class="header" href="#specify-settings-during-a-rollover">Specify settings during a rollover</a></h4>
<pre><code class="language-console">POST my-alias/_rollover
{
  &quot;settings&quot;: {
    &quot;index.number_of_shards&quot;: 2
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="shrink-index-api"><a class="header" href="#shrink-index-api">Shrink index API</a></h2>
<p>缩减现有索引的主分片 到新的索引</p>
<pre><code class="language-console">POST /my-index-000001/_shrink/shrunk-my-index-000001
</code></pre>
<h3 id="request-1"><a class="header" href="#request-1">Request</a></h3>
<pre><code>POST /&lt;index&gt;/_shrink/&lt;target-index&gt;
PUT /&lt;index&gt;/_shrink/&lt;target-index&gt;
</code></pre>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<ul>
<li>If the Elasticsearch security features are enabled, you must have the <code>manage</code> <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-privileges.html#privileges-list-indices">index privilege</a> for the index.</li>
<li>Before you can shrink an index:
<ul>
<li>The index must be read-only</li>
<li>A copy of every shard in the index must reside on the same node.</li>
<li>The index must have a <code>green</code> <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html">health status</a>.</li>
</ul>
</li>
</ul>
<p>简单起见 推荐移除 索引的副本分片，之后在 重新添加副本</p>
<ol>
<li>You can use the following <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-update-settings.html">update index settings API</a> request to remove an index’s replica shards</li>
<li>relocates the index’s remaining shards to the same node, and make the index read-only.</li>
</ol>
<pre><code class="language-console">PUT /my_source_index/_settings
{
  &quot;settings&quot;: {
    &quot;index.number_of_replicas&quot;: 0,//1          
    &quot;index.routing.allocation.require._name&quot;: &quot;shrink_node_name&quot;, //2
    &quot;index.blocks.write&quot;: true //3
  }
}
</code></pre>
<ol>
<li>Removes replica shards for the index.</li>
<li>Relocates the index’s shards to the shrink_node_name node. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">Index-level shard allocation filtering</a>.</li>
<li>Prevents write operations to this index. Metadata changes, such as deleting the index, are still allowed.</li>
</ol>
<p>It can take a while to relocate the source index. Progress can be tracked with the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cat-recovery.html"><code>_cat recovery</code> API</a>, or the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html"><code>cluster health</code> API</a> can be used to wait until all shards have relocated with the <code>wait_for_no_relocating_shards</code> parameter.</p>
<h3 id="description-1"><a class="header" href="#description-1">Description</a></h3>
<ol>
<li>shrink API 允许你 缩减一个现有 的索引 到一个新索引。该索引只有很少的主分片</li>
<li>目标索引中请求的主分片数量必须是源索引中分片数量的一个因子。</li>
<li>例如 8个主分片 只能缩减为 4、2、1，15个主分片 只能缩减为 5、3、1</li>
<li>如果是一个质数：则只能 缩减为 1个主分片</li>
<li>在收缩之前，索引中每个分片的 (主或副本) 副本必须存在于同一节点上。</li>
<li>当前 data stream  的 写索引 不能缩减，必须先要 rollover 才能缩减之前的索引</li>
</ol>
<h4 id="how-shrinking-works"><a class="header" href="#how-shrinking-works">How shrinking works</a></h4>
<ol>
<li>创建一个新的目标索引，其定义与源索引相同，但主分片数量较少。</li>
<li>从源索引到目标索引的硬链接段。(如果文件系统不支持硬链接，那么所有段都将复制到新索引中，这是一个更耗时的过程。如果使用多个数据路径，如果不同数据路径上的分片不在同一磁盘上，则需要段文件的完整副本，因为硬链接无法跨磁盘工作)</li>
<li>Recovers the target index as though it were a closed index which had just been re-opened.</li>
</ol>
<h4 id="shrink-an-index"><a class="header" href="#shrink-an-index">Shrink an index</a></h4>
<p>To shrink <code>my_source_index</code> into a new index called <code>my_target_index</code>, issue the following request:</p>
<pre><code class="language-console">POST /my_source_index/_shrink/my_target_index
{
  &quot;settings&quot;: {
    &quot;index.routing.allocation.require._name&quot;: null, //1
    &quot;index.blocks.write&quot;: null //2
  }
}

</code></pre>
<ol>
<li>Clear the allocation requirement copied from the source index.</li>
<li>Clear the index write block copied from the source index.</li>
</ol>
<p>The above request returns immediately once the target index has been added to the cluster state — it doesn’t wait for the shrink operation to start.</p>
<p>Indices can only be shrunk if they satisfy the following requirements:</p>
<ul>
<li>The target index must not exist.</li>
<li>The source index must have more primary shards than the target index.</li>
<li>The number of primary shards in the target index must be a factor of the number of primary shards in the source index. The source index must have more primary shards than the target index.</li>
<li>The index must not contain more than <code>2,147,483,519</code> documents in total across all shards that will be shrunk into a single shard on the target index as this is the maximum number of docs that can fit into a single shard.</li>
<li>The node handling the shrink process must have sufficient free disk space to accommodate a second copy of the existing index.</li>
</ul>
<p>The <code>_shrink</code> API is similar to the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-create-index.html"><code>create index</code> API</a> and accepts <code>settings</code> and <code>aliases</code> parameters for the target index:</p>
<pre><code class="language-console">POST /my_source_index/_shrink/my_target_index
{
  &quot;settings&quot;: {
    &quot;index.number_of_replicas&quot;: 1,
    &quot;index.number_of_shards&quot;: 1, 
    &quot;index.codec&quot;: &quot;best_compression&quot; 
  },
  &quot;aliases&quot;: {
    &quot;my_search_indices&quot;: {}
  }
}
</code></pre>
<p>Best compression will only take affect when new writes are made to the index, such as when <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-forcemerge.html">force-merging</a> the shard to a single segment.</p>
<p>Mappings may not be specified in the <code>_shrink</code> request.</p>
<h4 id="monitor-the-shrink-process"><a class="header" href="#monitor-the-shrink-process">Monitor the shrink process</a></h4>
<p><strong>监控</strong></p>
<p>The shrink process can be monitored with the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cat-recovery.html"><code>_cat recovery</code> API</a>, or the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html"><code>cluster health</code> API</a> can be used to wait until all primary shards have been allocated by setting the <code>wait_for_status</code> parameter to <code>yellow</code>.</p>
<p><strong>shrinkAPI返回</strong></p>
<ol>
<li>_shrinkAPI 一旦被添加到集群状态上，会马上返回</li>
<li>此时分片还未分配到任务节点上 所有分片 处于 未分配状态</li>
<li>处于某种原因，不能将 索引分配到 shrink node 。主节点会保持 未分配状态 直到 有节点可用</li>
<li>一旦主分片 被分配 了，会成为 initializing 状态。shrink 进程就会开始。</li>
<li>shrink 进程完成后。分片就会 active </li>
<li>在那时，Elasticsearch将尝试分配任何副本，并可能决定将主分片重新定位到另一个节点。</li>
</ol>
<h4 id="wait-for-active-shards-1"><a class="header" href="#wait-for-active-shards-1">Wait for active shards</a></h4>
<p>Because the shrink operation creates a new index to shrink the shards to, the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-create-index.html#create-index-wait-for-active-shards">wait for active shards</a> setting on index creation applies to the shrink index action as well.</p>
<h3 id="path-parameters-1"><a class="header" href="#path-parameters-1">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;index&gt;</code></strong></p>
<p>(Required, string) Name of the source index to shrink.</p>
</li>
<li>
<p><strong><code>&lt;target-index&gt;</code></strong></p>
<p>(Required, string) Name of the target index to create.Index names must meet the following criteria:Lowercase onlyCannot include <code>\</code>, <code>/</code>, <code>*</code>, <code>?</code>, <code>&quot;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>|</code>, <code> </code> (space character), <code>,</code>, <code>#</code>Indices prior to 7.0 could contain a colon (<code>:</code>), but that’s been deprecated and won’t be supported in 7.0+Cannot start with <code>-</code>, <code>_</code>, <code>+</code>Cannot be <code>.</code> or <code>..</code>Cannot be longer than 255 bytes (note it is bytes, so multi-byte characters will count towards the 255 limit faster)Names starting with <code>.</code> are deprecated, except for <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#index-hidden">hidden indices</a> and internal indices managed by plugins</p>
</li>
</ul>
<h3 id="query-parameters-1"><a class="header" href="#query-parameters-1">Query parameters</a></h3>
<p><strong><code>wait_for_active_shards</code></strong></p>
<p>(Optional, string) The number of shard copies that must be active before proceeding with the operation. Set to <code>all</code> or any positive integer up to the total number of shards in the index (<code>number_of_replicas+1</code>). Default: 1, the primary shard.</p>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-index_.html#index-wait-for-active-shards">Active shards</a>.</p>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<h3 id="request-body-1"><a class="header" href="#request-body-1">Request body</a></h3>
<p><strong><code>aliases</code></strong></p>
<p>索引对象</p>
<p><strong><code>settings</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#index-modules-settings">index setting object</a>) Configuration options for the target index. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#index-modules-settings">Index Settings</a>.</p>
<p><strong>max_primary_shard_size</strong></p>
<p>(Optional, byte units) The max primary shard size for the target index. </p>
<ol>
<li>用于查找目标索引的最佳分片数。</li>
<li>设置此参数时，每个分片在目标索引中的存储不会大于该参数</li>
<li>目标索引的分片数仍将是源索引的分片计数的一个因素，但是，如果参数小于源索引中的单个分片大小，则目标索引的分片计数将等于源索引的分片计数。</li>
<li>例如该 参数设置为 50GB。如果源索引有60个主分片，总共100G， 那么会收缩为2个主分片。每个50G，如果源索引 有60个主分片。总共100G。将会收缩到20个主分片。如果是60，4000G。则保持 原样不变 </li>
<li>这个参数跟  number_of_shards settings 冲突。两者只能设置其一</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="ilm-concepts"><a class="header" href="#ilm-concepts">ILM concepts</a></h2>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-index-lifecycle.html">Index lifecycle</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-rollover.html">Rollover</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/update-lifecycle-policy.html">Policy updates</a></li>
</ul>
<h2 id="index-lifecycle-1"><a class="header" href="#index-lifecycle-1">Index lifecycle</a></h2>
<p>ILM defines five index lifecycle <em>phases</em>:</p>
<ul>
<li><strong>Hot</strong>: 该索引正在积极更新和查询。.</li>
<li><strong>Warm</strong>: 索引不再更新，但仍在查询中.</li>
<li><strong>Cold</strong>: 该索引不再被更新，并且很少被查询。信息仍然需要搜索，但是如果这些查询速度较慢，也可以。.</li>
<li><strong>Frozen</strong>: 该索引不再被更新，并且很少被查询。信息仍然需要搜索，但是如果这些查询非常慢也可以.</li>
<li><strong>Delete</strong>: 该索引不再需要，可以安全地删除。</li>
</ul>
<p>索引的生命周期策略指定了哪些阶段适用，在每个阶段中执行了哪些操作以及何时在阶段之间过渡。</p>
<p>您可以在创建索引时手动应用生命周期策略</p>
<p>对于时间序列索引，您需要将生命周期策略与用于在序列中创建新索引的索引模板相关联。</p>
<p>当索引滚动时，手动应用的策略不会自动应用于新索引</p>
<p>如果您使用Elasticsearch的安全功能，ILM仅具有在上次策略更新时分配给用户的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/defining-roles.html">roles</a> 。</p>
<h3 id="phase-transitions-1"><a class="header" href="#phase-transitions-1">Phase transitions</a></h3>
<p>ILM根据索引在整个生命周期中的年龄 移动索引，为了控制这些转换的时间，您可以为每个阶段设置一个最小年龄。</p>
<p>为了使索引移至下一阶段，当前阶段中的所有操作必须完成，并且索引必须早于下一阶段的最小年龄。</p>
<p>配置的最小年龄必须在后续阶段之间增加，例如，最小年龄为10天的 <code>warm</code> 阶段之后只能是最小年龄为未设置或&gt; = 10天的 <code>cold</code> 阶段。</p>
<p>最小年龄默认为零，这会导致ILM在当前阶段的所有操作完成后立即将索引移至下一阶段</p>
<p>如果索引具有未分配的分片，并且群集运行状况( <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/cluster-health.html">cluster health status</a> i)为黄色，则该索引仍可以根据其索引生命周期管理策略过渡到下一阶段</p>
<p>但是，由于Elasticsearch只能在绿色集群上执行某些清理任务，因此可能会产生意外的副作用，为了避免增加磁盘使用率和可靠性问题，请及时解决任何群集运行状况问题。</p>
<h3 id="phase-execution-1"><a class="header" href="#phase-execution-1">Phase execution</a></h3>
<p>ILM控制执行一个阶段中的动作的顺序以及执行哪些步骤来执行每个动作的必要索引操作。</p>
<p>当索引进入阶段时，ILM会在索引元数据中缓存阶段定义，这样可以确保策略更新不会将索引置于永远无法退出阶段的状态。</p>
<p>如果可以安全地应用更改，则ILM会更新缓存的阶段定义。如果不能，则使用缓存的定义继续执行阶段。</p>
<p>ILM定期运行，检查索引是否符合策略标准，并执行所需的任何步骤。</p>
<p>为了避免竞争条件，ILM可能需要运行多次才能执行完成操作所需的所有步骤。</p>
<p>例如，如果ILM确定索引已满足<em>rollover</em>标准，则它开始执行完成展期操作所需的步骤。</p>
<p>如果它达到了  <code>前进到下一步不安全</code> 的地步，则执行停止</p>
<p>下次ILM运行时，ILM在停止执行的地方开始执行，这意味着，即使将indices.lifecycle.poll_interval设置为10分钟，并且索引满足翻转标准，也可能需要20分钟才能完成翻转。</p>
<h3 id="phase-actions"><a class="header" href="#phase-actions">Phase actions</a></h3>
<p>ILM在每个阶段都支持以下操作。ILM按列出的顺序执行操作。</p>
<ul>
<li>Hot
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-set-priority.html">Set Priority</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-unfollow.html">Unfollow</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-rollover.html">Rollover</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-readonly.html">Read-Only</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-shrink.html">Shrink</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-forcemerge.html">Force Merge</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-searchable-snapshot.html">Searchable Snapshot</a></li>
</ul>
</li>
<li>Warm
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-set-priority.html">Set Priority</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-unfollow.html">Unfollow</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-readonly.html">Read-Only</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-allocate.html">Allocate</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-migrate.html">Migrate</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-shrink.html">Shrink</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-forcemerge.html">Force Merge</a></li>
</ul>
</li>
<li>Cold
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-set-priority.html">Set Priority</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-unfollow.html">Unfollow</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-readonly.html">Read-Only</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-searchable-snapshot.html">Searchable Snapshot</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-allocate.html">Allocate</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-migrate.html">Migrate</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-freeze.html">Freeze</a></li>
</ul>
</li>
<li>Frozen
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-searchable-snapshot.html">Searchable Snapshot</a></li>
</ul>
</li>
<li>Delete
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-wait-for-snapshot.html">Wait For Snapshot</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ilm-delete.html">Delete</a></li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="rollover-1"><a class="header" href="#rollover-1">Rollover</a></h2>
<p>When indexing time series data like logs or metrics, you can’t write to a single index indefinitely. To meet your indexing and search performance requirements and manage resource usage, you write to an index until some threshold is met and then create a new index and start writing to it instead. Using rolling indices enables you to:</p>
<p>在索引日志或指标等时间序列数据时，您不能无限期地写入单个索引。为了满足索引和搜索性能要求并管理资源使用情况，您可以写入索引，直到满足某个阈值，然后创建一个新索引并开始写入索引。使用滚动索引使您能够:</p>
<ul>
<li>优化高性能热节点高摄取率的活跃指数</li>
<li>优化暖节点上的搜索性能</li>
<li>将较旧、访问频率较低的数据转移到较便宜的冷节点，</li>
<li>通过删除整个索引，根据您的保留策略删除数据。</li>
</ul>
<p>我们建议使用数据流（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-create-data-stream.html">data streams</a> ）来管理时间序列数据。数据流自动跟踪写索引，同时保持配置最小化</p>
<p>每个数据流都需要一个索引模板（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html">index template</a> ），其中包含:</p>
<ul>
<li>数据流的名称或通配符 (*) 模式。</li>
<li>数据流的时间戳字段。此字段必须映射为 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/date.html"><code>date</code></a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/date_nanos.html"><code>date_nanos</code></a>  数据类型，并且必须包含在索引到数据流的每个文档中。</li>
<li>创建每个备份索引时应用的映射和设置。</li>
</ul>
<p>数据流是为仅追加数据而设计的，其中数据流名称可以用作操作 (读，写，翻转，收缩等) 的目标。如果您的用例需要适当地更新数据，则可以使用索引别名（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/aliases.html">index aliases</a>.）来管理时间序列数据。但是，还有一些配置步骤和概念:</p>
<ul>
<li>一个索引模板，用于指定系列中每个新索引的设置。您可以优化此配置以进行摄取，通常使用与热节点一样多的分片。</li>
<li>引用整个索引集的索引别名。</li>
<li>指定为写入索引的单个索引。这是处理所有写请求的活动索引。在每次rollover时，新索引成为写索引。</li>
</ul>
<h3 id="automatic-rollover"><a class="header" href="#automatic-rollover">Automatic rollover</a></h3>
<p>ILM使您能够根据索引大小、文档计数或使用年限自动滚动到新索引。触发翻转时，将创建一个新索引，更新写别名以指向新索引，并将所有后续更新写入新索引。</p>
<p>根据大小，文档数量或年龄将其滚动到新索引，比基于时间的滚动更可取。在任意时间滚动通常会导致许多小索引，这可能会对性能和资源使用产生负面影响。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="lifecycle-policy-updates"><a class="header" href="#lifecycle-policy-updates">Lifecycle policy updates</a></h2>
<p>您可以通过修改当前策略或切换到其他策略来更改索引或滚动索引集合的生命周期管理方式。</p>
<p>为了确保策略更新不会将索引置于无法退出当前阶段的状态，阶段定义在进入阶段时会缓存在索引元数据中。如果可以安全地应用更改，则ILM会更新缓存的阶段定义。如果不能，则使用缓存的定义继续执行阶段。</p>
<p>当索引前进到下一阶段时，它将使用更新策略中的阶段定义。</p>
<h3 id="how-changes-are-applied"><a class="header" href="#how-changes-are-applied">How changes are applied</a></h3>
<p>当策略最初应用于索引时，索引将获取策略的最新版本。如果更新策略，则策略版本会被碰撞，并且ILM可以检测到索引使用的是需要更新的早期版本。</p>
<p>对<em>min_age</em>的更改不会传播到缓存的定义。更改阶段的<em>min_age</em>不会影响当前正在执行该阶段的索引。</p>
<p>例如，如果您创建的策略具有未指定<em>min_age</em>的热阶段，则在应用该策略时，索引立即进入热阶段。如果然后更新策略以为热阶段指定1天的min_age，则对已经处于热阶段的索引没有影响。策略更新后创建的索引在一天之前不会进入热阶段。</p>
<h3 id="how-new-policies-are-applied"><a class="header" href="#how-new-policies-are-applied">How new policies are applied</a></h3>
<p>当您将不同的策略应用于托管索引时，索引将使用先前策略中的缓存定义完成当前阶段。当索引进入下一阶段时，它开始使用新策略。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="index-modules"><a class="header" href="#index-modules">Index modules</a></h1>
<p>索引模块是每个索引创建的模块，可控制与索引相关的所有方面。</p>
<h2 id="先验知识"><a class="header" href="#先验知识">先验知识</a></h2>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-open-close.html">closed index</a> </li>
<li>索引拆分 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-split-index.html">split</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-routing-field.html">routing field</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-filter-context.html">cached filters</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">allocation filtering</a> </li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-total-shards.html">total shards per node</a></li>
</ol>
<h2 id="index-settings"><a class="header" href="#index-settings">Index Settings</a></h2>
<p>索引级别的设置可能是:</p>
<ul>
<li>
<p>static</p>
<p>它们只能在索引创建时或在 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-open-close.html">closed index</a> 索引上设置。</p>
</li>
<li>
<p>dynamic</p>
<p>可以通过 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-update-settings.html">update-index-settings</a> API 改变的设置</p>
</li>
</ul>
<p>更改已关闭索引上的静态或动态索引设置可能会导致不正确的设置，而如果不删除和重新创建索引，则无法纠正这些设置。</p>
<h3 id="static-index-settings"><a class="header" href="#static-index-settings">Static index settings</a></h3>
<p>以下是索引通用的静态设置的列表:</p>
<ul>
<li>
<p><strong><code>index.number_of_shards</code></strong></p>
<ol>
<li>索引应具有的主分片数量</li>
<li>默认1。只能在索引创建时设置。可以作用于closed index</li>
<li>每个索引的主分片数最多 <code>1024</code> </li>
<li>此限制是一个安全限制，以防止意外创建索引，这些索引会由于资源分配而使群集不稳定。</li>
<li>这个限制可以通过  <code>export ES_JAVA_OPTS=&quot;-Des.index.max_number_of_shards=128&quot;</code> 系统变量修改（每个节点都要修改）</li>
</ol>
</li>
<li>
<p><strong><code>index.number_of_routing_shards</code></strong></p>
<ol>
<li>
<p>用于拆分 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-split-index.html">split</a>  索引的路由分片数</p>
</li>
<li>
<p>五分片的索引，设置  number_of_routing_shards 为 30 （5x2x3）</p>
</li>
<li>
<p>换句话说，</p>
<pre><code>`5` → `10` → `30`

`5` → `15` → `30` 

`5` → `30`
</code></pre>
</li>
<li>
<p>此设置的默认值取决于索引中主分片的数量。</p>
</li>
<li>
<p>默认值旨在允许您按2的因子拆分，最多1024个分片</p>
</li>
<li>
<p>在Elasticsearch 7.0.0及更高版本中，此设置会影响文档在各个分片之间的分布方式</p>
</li>
<li>
<p>使用自定义路由重新索引较旧的索引时，您必须显式设置<em>index.number_of_routing_shards</em>以保持相同的文档分布 See the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.0/breaking-changes-7.0.html#_document_distribution_changes">related breaking change</a>.</p>
</li>
</ol>
</li>
<li>
<p><strong><code>index.shard.check_on_startup</code></strong></p>
<p>Whether or not shards should be checked for corruption before opening.</p>
<ol>
<li>打开分片前 是否检查 分片是否损坏。如果一损坏则不会打开</li>
<li>默认，false。不检查</li>
<li>checksum：检查物理 校验和</li>
<li>true： 既检查物理也检查逻辑损坏。这个很耗CPU跟内存、仅限专家。在大型索引上检查分片可能需要大量时间。</li>
</ol>
</li>
<li>
<p><strong><code>index.codec</code></strong></p>
<ol>
<li>默认值使用LZ4压缩来压缩存储的数据，</li>
<li>但这可以设置为<em>best_compression</em>，它使用 <a href="https://en.wikipedia.org/wiki/DEFLATE">DEFLATE</a> 来获得更高的压缩比，但以较慢的存储字段性能为代价。</li>
<li>如果要更新compression type，则在合并段后将应用新的压缩类型</li>
<li>Segment merging can be forced using <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-forcemerge.html">force merge</a>.</li>
</ol>
</li>
<li>
<p><strong><code>index.routing_partition_size</code></strong></p>
<p>The number of shards a custom <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-routing-field.html">routing</a> value can go to. </p>
<ol>
<li>自定义路由值。可以 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-routing-field.html">routing</a>  的分片数。</li>
<li>默认为1，只能在索引创建时设置。</li>
<li>必须小于  <code>index.number_of_shards</code> 。除非  <code>index.number_of_shards</code>设置为1 </li>
<li>详见： <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping-routing-field.html#routing-index-partition">Routing to an index partition</a> </li>
</ol>
</li>
<li>
<p><strong><code>index.soft_deletes.enabled</code></strong></p>
<ol>
<li>[7.6.0] Deprecated in 7.6.0. </li>
<li>不建议使用禁用软删除的创建索引，并且将在以后的Elasticsearch版本中删除。</li>
<li>指示是否在索引上启用了软删除</li>
<li>软删除只能在索引创建时配置，并且只能在Elasticsearch 6.5.0上或之后创建的索引上配置。默认为true。</li>
</ol>
</li>
<li>
<p><strong><code>index.soft_deletes.retention_lease.period</code></strong></p>
<ol>
<li>保留分片历史记录的最长时间</li>
<li>碎片历史记录保留租约确保在合并Lucene索引期间保留软删除。</li>
<li>如果软删除在可以复制到flollower 之前被合并了</li>
<li>以下过程将由于 leader 的历史记录不完整而失败. </li>
<li>Defaults to <code>12h</code></li>
</ol>
</li>
<li>
<p><strong><code>index.load_fixed_bitset_filters_eagerly</code></strong></p>
<ol>
<li>load_fixed_bitset_filters_eagerly</li>
<li>是否对 嵌套查询 预加载  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-filter-context.html">cached filters</a></li>
<li>默认FALSE</li>
</ol>
</li>
<li>
<p><strong><code>index.hidden</code></strong></p>
<ol>
<li>指示默认情况下是否应该隐藏索引</li>
<li>使用通配符表达式时，默认情况下不返回隐藏索引。</li>
<li>通过使用<em>expand_wildcards</em>参数来控制此行为。</li>
<li>默认FALSE</li>
</ol>
</li>
</ul>
<h3 id="dynamic-index-settings"><a class="header" href="#dynamic-index-settings">Dynamic index settings</a></h3>
<p>以下是与任何特定索引模块不关联的所有动态索引设置</p>
<ul>
<li>
<p><strong><code>index.number_of_replicas</code></strong></p>
<ol>
<li>每个主分片的副本数。默认为1。</li>
</ol>
</li>
<li>
<p><strong><code>index.auto_expand_replicas</code></strong></p>
<ol>
<li>根据集群中的数据节点数自动扩展副本数</li>
<li>设置为限定下限和上限的破折号 (例如. <code>0-5</code>) or use <code>all</code> for the upper bound (e.g. <code>0-all</code>). </li>
<li>默认FALSE禁用。</li>
<li>分片自动扩展数量 只 会考虑  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">allocation filtering</a>  会忽略  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-total-shards.html">total shards per node</a>, 如果适用的规则阻止分配所有副本，则这可能导致群集运行状况变为黄色。</li>
<li>If the upper bound is <code>all</code> then <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#shard-allocation-awareness">shard allocation awareness</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#cluster-routing-allocation-same-shard-host"><code>cluster.routing.allocation.same_shard.host</code></a> are ignored for this index.</li>
</ol>
</li>
<li>
<p><strong><code>index.search.idle.after</code></strong></p>
<ol>
<li>在被认为搜索空闲之前，分片无法接收搜索或获取请求多长时间。(默认为30秒)</li>
</ol>
</li>
<li>
<p><strong><code>index.refresh_interval</code></strong></p>
<ol>
<li>执行刷新操作的频率，这使索引的最新更改对搜索可见</li>
<li>默认1s。-1 则是禁止刷新</li>
<li>如果未明确设置此设置，则至少在<em>index.search.idle</em>秒前。没有看到搜索流量的分片，在收到搜索请求之前，它们将不会收到后台刷新。</li>
<li>命中空闲且刷新被阻塞的分片的搜索请求。将会在下次后台刷新（1s）</li>
<li>此行为旨在在不执行搜索的默认情况下自动优化批量索引。</li>
<li>为了选择退出此行为，应将此值显示的设置为 1</li>
</ol>
</li>
<li>
<p><strong><code>index.max_result_window</code></strong></p>
<ol>
<li>from+size 搜索的最大大小</li>
<li>默认1w</li>
<li>搜索请求 消耗大量堆、跟时间</li>
<li>详见： <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/paginate-search-results.html#scroll-search-results">Scroll</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/paginate-search-results.html#search-after">Search After</a> </li>
</ol>
</li>
<li>
<p><strong><code>index.max_inner_result_window</code></strong></p>
<ol>
<li>The maximum value of <code>from + size</code> for inner hits definition and top hits aggregations to this index. </li>
<li>分片内部返回的最大值</li>
<li>默认100。</li>
</ol>
</li>
<li>
<p><strong><code>index.max_rescore_window</code></strong></p>
<ol>
<li>The maximum value of <code>window_size</code> for <code>rescore</code> requests in searches of this index.</li>
<li>. Defaults to <code>index.max_result_window</code> which defaults to <code>10000</code>. </li>
<li>Search requests take heap memory and time proportional to <code>max(window_size, from + size)</code> and this limits that memory.</li>
</ol>
</li>
<li>
<p><strong><code>index.max_docvalue_fields_search</code></strong></p>
<ol>
<li>The maximum number of <code>docvalue_fields</code> that are allowed in a query. </li>
<li>Defaults to <code>100</code>. Doc-value fields are costly since they might incur a per-field per-document seek.</li>
</ol>
</li>
<li>
<p><strong><code>index.max_script_fields</code></strong></p>
<p>The maximum number of <code>script_fields</code> that are allowed in a query. Defaults to <code>32</code>.</p>
</li>
<li>
<p><strong><code>index.max_ngram_diff</code></strong></p>
<p>对于NGramTokenizer和NGramTokenFilter，min_gram和max_gram之间的最大允许差。默认为1。</p>
</li>
<li>
<p><strong><code>index.max_shingle_diff</code></strong></p>
<p>The maximum allowed difference between max_shingle_size and min_shingle_size for the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/analysis-shingle-tokenfilter.html"><code>shingle</code> token filter</a>. Defaults to <code>3</code>.</p>
</li>
<li>
<p><strong><code>index.max_refresh_listeners</code></strong></p>
<p>Maximum number of refresh listeners available on each shard of the index. These listeners are used to implement <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-refresh.html"><code>refresh=wait_for</code></a>.</p>
</li>
<li>
<p><strong><code>index.analyze.max_token_count</code></strong></p>
<p>The maximum number of tokens that can be produced using _analyze API. Defaults to <code>10000</code>.</p>
</li>
<li>
<p><strong><code>index.highlight.max_analyzed_offset</code></strong></p>
<p>The maximum number of characters that will be analyzed for a highlight request. This setting is only applicable when highlighting is requested on a text that was indexed without offsets or term vectors. Defaults to <code>1000000</code>.</p>
</li>
<li>
<p><strong><code>index.max_terms_count</code></strong></p>
<p>The maximum number of terms that can be used in Terms Query. Defaults to <code>65536</code>.</p>
</li>
<li>
<p><strong><code>index.max_regex_length</code></strong></p>
<p>The maximum length of regex that can be used in Regexp Query. Defaults to <code>1000</code>.</p>
</li>
<li>
<p><strong><code>index.query.default_field</code></strong></p>
<p>(string or array of strings) Wildcard (<code>*</code>) patterns matching one or more fields. The following query types search these matching fields by default:<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-mlt-query.html">More like this</a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-multi-match-query.html">Multi-match</a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-query-string-query.html">Query string</a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-simple-query-string-query.html">Simple query string</a>Defaults to <code>*</code>, which matches all fields eligible for <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/term-level-queries.html">term-level queries</a>, excluding metadata fields.</p>
</li>
<li>
<p><strong><code>index.routing.allocation.enable</code></strong></p>
<ol>
<li>控制此索引的分片分配。</li>
<li>It can be set to:<code>all</code> (default) - 允许所有分片的分片分配。</li>
<li><code>primaries</code> -仅允许主分片分配.</li>
<li><code>new_primaries</code> - 只允许新创建的主分片分配.</li>
<li><code>none</code> - 不允许分片分配。</li>
</ol>
</li>
<li>
<p><strong><code>index.routing.rebalance.enable</code></strong></p>
<ol>
<li>启用此索引的分片再平衡</li>
<li>It can be set to:<code>all</code> (default) - Allows shard rebalancing for all shards.</li>
<li><code>primaries</code> - Allows shard rebalancing only for primary shards.</li>
<li><code>replicas</code> - Allows shard rebalancing only for replica shards.</li>
<li><code>none</code> - No shard rebalancing is allowed.</li>
</ol>
</li>
<li>
<p><strong><code>index.gc_deletes</code></strong></p>
<p>The length of time that a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-delete.html#delete-versioning">deleted document’s version number</a> remains available for <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-index_.html#index-versioning">further versioned operations</a>. Defaults to <code>60s</code>.</p>
</li>
<li>
<p><strong><code>index.default_pipeline</code></strong></p>
<ol>
<li>The default <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ingest.html">ingest node</a> pipeline for this index. </li>
<li>Index requests will fail if the default pipeline is set and the pipeline does not exist. </li>
<li>The default may be overridden using the <code>pipeline</code> parameter. </li>
<li>The special pipeline name <code>_none</code> indicates no ingest pipeline should be run.</li>
</ol>
</li>
<li>
<p><strong><code>index.final_pipeline</code></strong></p>
<ol>
<li>The final <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ingest.html">ingest node</a> pipeline for this index. </li>
<li>Indexing requests will fail if the final pipeline is set and the pipeline does not exist. </li>
<li>The final pipeline always runs after the request pipeline (if specified) and the default pipeline (if it exists). </li>
<li>The special pipeline name <code>_none</code> indicates no ingest pipeline will run.</li>
<li>You can’t use a final pipelines to change the <code>_index</code> field. If the pipeline attempts to change the <code>_index</code> field, the indexing request will fail.</li>
</ol>
</li>
</ul>
<h3 id="settings-in-other-index-modules"><a class="header" href="#settings-in-other-index-modules">Settings in other index modules</a></h3>
<p>Other index settings are available in index modules:</p>
<ul>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/analysis.html">Analysis</a></strong></p>
<p>Settings to define analyzers, tokenizers, token filters and character filters.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-allocation.html">Index shard allocation</a></strong></p>
<p>Control over where, when, and how shards are allocated to nodes.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-mapper.html">Mapping</a></strong></p>
<p>Enable or disable dynamic mapping for an index.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-merge.html">Merging</a></strong></p>
<p>Control over how shards are merged by the background merge process.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-similarity.html">Similarities</a></strong></p>
<p>Configure custom similarity settings to customize how search results are scored.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-slowlog.html">Slowlog</a></strong></p>
<p>Control over how slow queries and fetch requests are logged.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-store.html">Store</a></strong></p>
<p>Configure the type of filesystem used to access shard data.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-translog.html">Translog</a></strong></p>
<p>Control over the transaction log and background flush operations.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-history-retention.html">History retention</a></strong></p>
<p>Control over the retention of a history of operations in the index.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-indexing-pressure.html">Indexing pressure</a></strong></p>
<p>Configure indexing back pressure limits.</p>
</li>
</ul>
<h3 id="x-pack-index-settings"><a class="header" href="#x-pack-index-settings">X-Pack index settings</a></h3>
<ul>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-settings.html">Index lifecycle management</a></strong></p>
<p>Specify the lifecycle policy and rollover alias for an index.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="analysis"><a class="header" href="#analysis">Analysis</a></h2>
<p>索引分析模块充当分析器的可配置注册表，可用于将字符串字段转换为 独立的 terms:</p>
<ul>
<li>添加到倒排索引中，以使文档可搜索</li>
<li>used by high level queries such as the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/query-dsl-match-query.html"><code>match</code> query</a> to generate search terms.</li>
</ul>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/analysis.html">Text analysis</a> for configuration details.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-shard-allocation"><a class="header" href="#index-shard-allocation">Index Shard Allocation</a></h2>
<p>This module provides per-index settings to control the allocation of shards to nodes:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">Shard allocation filtering</a>: Controlling which shards are allocated to which nodes.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/delayed-allocation.html">Delayed allocation</a>: Delaying allocation of unassigned shards caused by a node leaving.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-total-shards.html">Total shards per node</a>: A hard limit on the number of shards from the same index per node.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tier-shard-filtering.html">Data tier allocation</a>: Controls the allocation of indices to <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">data tiers</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-blocks"><a class="header" href="#index-blocks">Index blocks</a></h2>
<ol>
<li>index block 限制了某个索引上可用的操作类型</li>
<li>这些块有不同的风格，允许阻止写入、读取或元数据操作</li>
<li>可以使用动态索引设置来设置/删除 block，或者可以使用专用的API来添加block，这也可以确保写入块一旦成功返回给用户，</li>
<li>索引的所有分片都正确地考虑了block，例如，添加写入块后，对 所有的 in-flight 的 写入都已完成后。才会添加 write block</li>
</ol>
<h3 id="prerequisite"><a class="header" href="#prerequisite">Prerequisite</a></h3>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#disk-based-shard-allocation">disk-based shard allocator</a> </li>
<li></li>
</ol>
<h3 id="index-block-settings"><a class="header" href="#index-block-settings">Index block settings</a></h3>
<p>以下动态索引设置确定索引上存在的块:</p>
<ul>
<li>
<p><strong><code>index.blocks.read_only</code></strong></p>
<ol>
<li>Set to <code>true</code> to make the index and index metadata read only, </li>
<li><code>false</code> to allow writes and metadata changes.</li>
</ol>
</li>
<li>
<p><strong><code>index.blocks.read_only_allow_delete</code></strong></p>
<ol>
<li>Similar to <code>index.blocks.read_only</code>,但也允许删除索引 以使更多资源可用。</li>
<li>The <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#disk-based-shard-allocation">disk-based shard allocator</a> may add and remove this block automatically.</li>
<li>从索引中删除文档以释放资源-而不是删除索引本身-可以随时间增加索引大小. </li>
<li>When <code>index.blocks.read_only_allow_delete</code> is set to <code>true</code>, 不允许删除文档. </li>
<li>但是，删除索引本身会释放只读索引块，并使资源几乎立即可用</li>
<li>当磁盘利用率低于高水位时，Elasticsearch会自动添加和删除只读索引块，由 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#cluster-routing-flood-stage">cluster.routing.allocation.disk.watermark.flood_stage</a>.控制</li>
</ol>
</li>
<li>
<p><strong><code>index.blocks.read</code></strong></p>
<ol>
<li>设置为true以禁用对索引的读取操作。</li>
</ol>
</li>
<li>
<p><strong><code>index.blocks.write</code></strong></p>
<ol>
<li>设置为true以禁用针对索引的数据写入操作。</li>
<li>Unlike <code>read_only</code>, this setting does not affect metadata. </li>
<li>例如，您可以用写Block 关闭索引，但是不能用read_only block 关闭索引。</li>
</ol>
</li>
<li>
<p><strong><code>index.blocks.metadata</code></strong></p>
<p>Set to <code>true</code> to disable index metadata reads and writes.</p>
</li>
</ul>
<h3 id="add-index-block-api"><a class="header" href="#add-index-block-api">Add index block API</a></h3>
<p>Adds an index block to an index.</p>
<pre><code class="language-console">PUT /my-index-000001/_block/write
</code></pre>
<h4 id="request-2"><a class="header" href="#request-2">Request</a></h4>
<pre><code>PUT /&lt;index&gt;/_block/&lt;block&gt;
</code></pre>
<h4 id="path-parameters-2"><a class="header" href="#path-parameters-2">Path parameters</a></h4>
<ul>
<li><strong><code>&lt;index&gt;</code></strong>
<ol>
<li>逗号分割的索引名或者GLOB模式</li>
<li><code>_all</code> or <code>*</code>添加所有索引</li>
<li>要禁止将BLOCK 添加到具有 _all或通配符表达式的索引中，请将<em>action.destructive_requires_name</em> 群集设置更改为 'true'</li>
<li>You can update this setting in the <code>elasticsearch.yml</code> file or using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-update-settings.html">cluster update settings</a> API.</li>
</ol>
</li>
<li><strong><code>&lt;block&gt;</code></strong>
<ol>
<li>(必填，字符串) 要添加到索引中的BLOCK类型。</li>
</ol>
</li>
</ul>
<h4 id="query-parameters-2"><a class="header" href="#query-parameters-2">Query parameters</a></h4>
<ul>
<li>
<p><strong><code>allow_no_indices</code></strong></p>
<p>(Optional, Boolean) If <code>false</code>, the request returns an error if any wildcard expression, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-aliases.html">index alias</a>, or <code>_all</code> value targets only missing or closed indices. </p>
<ol>
<li>如果设置为FALSE 通配符表达式或者 _all 、索引别名等 没有命中 目标索引 则 返回异常。</li>
<li>即使请求针对其他开放索引，此行为也适用。例如，<code>foo*,bar*</code> ，如果索引以foo开头，但没有索引以bar开头，则返回错误。默认为true。</li>
</ol>
</li>
<li>
<p><strong><code>expand_wildcards</code></strong></p>
<ol>
<li>(可选，字符串) 通配符表达式可以匹配的索引类型。</li>
<li>如果请求可以针对数据流，则此参数确定通配符表达式是否与隐藏数据流匹配。</li>
<li>支持以逗号分隔的值，如open、hidden。</li>
<li><code>all</code> 匹配任何数据流或索引 including <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/multi-index.html#hidden">hidden</a> ones</li>
<li><code>open</code> Match open, non-hidden indices 也匹配任何非隐藏数据流</li>
<li><code>closed</code> Match closed, non-hidden indices，也匹配任何非隐藏数据流。数据流无法关闭</li>
<li><code>hidden</code> 匹配隐藏数据流和隐藏索引</li>
<li>必须与<code>open</code>，<code>closed</code>或两者结合使用。不接受none Wildcard表达式。默认为open。</li>
</ol>
</li>
<li>
<p><strong><code>ignore_unavailable</code></strong></p>
<p>(Optional, Boolean) If <code>false</code>, the request returns an error if it targets a missing or closed index. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
</ul>
<h4 id="examples-1"><a class="header" href="#examples-1">Examples</a></h4>
<p>The following example shows how to add an index block:</p>
<pre><code class="language-console">PUT /my-index-000001/_block/write
</code></pre>
<p>The API returns following response:</p>
<pre><code class="language-console-result">{
  &quot;acknowledged&quot; : true,
  &quot;shards_acknowledged&quot; : true,
  &quot;indices&quot; : [ {
    &quot;name&quot; : &quot;my-index-000001&quot;,
    &quot;blocked&quot; : true
  } ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="mapper"><a class="header" href="#mapper">Mapper</a></h2>
<ol>
<li>mapper module 在创建索引时或使用 update mapping API 充当添加到索引的类型映射定义的注册表</li>
<li>它还处理对没有预先定义的显式映射的类型的动态映射支持</li>
<li>check out the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/mapping.html">mapping section</a>.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="merge"><a class="header" href="#merge">Merge</a></h2>
<ol>
<li>Elasticsearch中的一个分片是Lucene索引，Lucene 索引 被分解成若干段</li>
<li>段是索引中存储索引数据的内部存储元素，并且是不可变的</li>
<li>较小的段会定期合并到较大的段中，以保持索引大小不变并删除删除内容。</li>
<li>合并过程使用自动节流（auto-throttling）来平衡合并和其他活动 (如搜索) 之间硬件资源的使用。</li>
</ol>
<h3 id="merge-scheduling"><a class="header" href="#merge-scheduling">Merge scheduling</a></h3>
<ol>
<li>合并调度程序 (ConcurrentMergeScheduler) 在需要时控制合并操作的执行。</li>
<li>Merges在单独的线程中运行，当达到最大线程数时，进一步的merges将等待，直到合并线程变得可用。</li>
</ol>
<p>合并调度程序支持以下动态设置:</p>
<ul>
<li><strong><code>index.merge.scheduler.max_thread_count</code></strong>
<ol>
<li>单个分片上可能一次合并的最大线程数</li>
<li>Defaults to <code>Math.max(1, Math.min(4, &lt;&lt;node.processors, node.processors&gt;&gt; / 2))</code> 这对于一个好的固态磁盘 (SSD) 来说效果很好。</li>
<li>如果您的索引位于旋转盘片驱动器上，请将其减小为1。</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="similarity-module"><a class="header" href="#similarity-module">Similarity module</a></h2>
<ol>
<li>
<p>相似性 (评分/排名模型) 定义了匹配文档的评分方式。</p>
</li>
<li>
<p>相似性是每个字段，这意味着通过 <code>mapping</code> 可以定义每个字段的不同相似性。</p>
</li>
<li>
<p>配置自定义相似性是 expert feature  ，并且内置相似性很可能就足够了，如 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/similarity.html"><code>similarity</code></a>. 中所述。</p>
</li>
</ol>
<h3 id="configuring-a-similarity"><a class="header" href="#configuring-a-similarity">Configuring a similarity</a></h3>
<ol>
<li>
<p>大多数现有或自定义相似性都具有配置选项，可以通过索引设置进行配置，如下所示</p>
</li>
<li>
<p>创建索引或更新索引设置时可以提供索引选项。</p>
</li>
</ol>
<pre><code class="language-console">PUT /index
{
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;similarity&quot;: {
        &quot;my_similarity&quot;: {
          &quot;type&quot;: &quot;DFR&quot;,
          &quot;basic_model&quot;: &quot;g&quot;,
          &quot;after_effect&quot;: &quot;l&quot;,
          &quot;normalization&quot;: &quot;h2&quot;,
          &quot;normalization.h2.c&quot;: &quot;3.0&quot;
        }
      }
    }
  }
}
</code></pre>
<p>在这里，我们配置DFR相似性，以便可以在映射中引用为my_simility，如下面的示例所示:</p>
<pre><code class="language-console">PUT /index/_mapping
{
  &quot;properties&quot; : {
    &quot;title&quot; : { &quot;type&quot; : &quot;text&quot;, &quot;similarity&quot; : &quot;my_similarity&quot; }
  }
}
</code></pre>
<h3 id="available-similarities"><a class="header" href="#available-similarities">Available similarities</a></h3>
<h4 id="bm25-similarity-default"><a class="header" href="#bm25-similarity-default">BM25 similarity (<strong>default</strong>)</a></h4>
<p>基于TF/IDF的相似性，具有内置的tf归一化，并且对短字段 (如名称) 更好地工作。See <a href="https://en.wikipedia.org/wiki/Okapi_BM25">Okapi_BM25</a> for more details.</p>
<p>这种相似性有以下选项:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><code>k1</code></td><td>Controls non-linear term frequency normalization (saturation). The default value is <code>1.2</code>.</td></tr>
<tr><td><code>b</code></td><td>Controls to what degree document length normalizes tf values. The default value is <code>0.75</code>.</td></tr>
<tr><td><code>discount_overlaps</code></td><td>Determines whether overlap tokens (Tokens with 0 position increment) are ignored when computing norm. By default this is true, meaning overlap tokens do not count when computing norms.</td></tr>
</tbody></table>
<h4 id="dfr-similarity"><a class="header" href="#dfr-similarity">DFR similarity</a></h4>
<p>Similarity that implements the <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/DFRSimilarity.html">divergence from randomness</a> framework. This similarity has the following options:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><code>basic_model</code></td><td>Possible values: <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/BasicModelG.html"><code>g</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/BasicModelIF.html"><code>if</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/BasicModelIn.html"><code>in</code></a> and <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/BasicModelIne.html"><code>ine</code></a>.</td></tr>
<tr><td><code>after_effect</code></td><td>Possible values: <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/AfterEffectB.html"><code>b</code></a> and <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/AfterEffectL.html"><code>l</code></a>.</td></tr>
<tr><td><code>normalization</code></td><td>Possible values: <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/Normalization.NoNormalization.html"><code>no</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/NormalizationH1.html"><code>h1</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/NormalizationH2.html"><code>h2</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/NormalizationH3.html"><code>h3</code></a> and <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/NormalizationZ.html"><code>z</code></a>.</td></tr>
</tbody></table>
<p>All options but the first option need a normalization value.</p>
<p>Type name: <code>DFR</code></p>
<h4 id="dfi-similarity"><a class="header" href="#dfi-similarity">DFI similarity</a></h4>
<p>Similarity that implements the <a href="https://trec.nist.gov/pubs/trec21/papers/irra.web.nb.pdf">divergence from independence</a> model. This similarity has the following options:</p>
<p><code>independence_measure</code>  Possible values <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/IndependenceStandardized.html"><code>standardized</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/IndependenceSaturated.html"><code>saturated</code></a>, <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/IndependenceChiSquared.html"><code>chisquared</code></a>.</p>
<p>When using this similarity, it is highly recommended <strong>not</strong> to remove stop words to get good relevance. Also beware that terms whose frequency is less than the expected frequency will get a score equal to 0.</p>
<p>Type name: <code>DFI</code></p>
<h4 id="ib-similarity"><a class="header" href="#ib-similarity">IB similarity.</a></h4>
<p><a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/IBSimilarity.html">Information based model</a> . The algorithm is based on the concept that the information content in any symbolic <em>distribution</em> sequence is primarily determined by the repetitive usage of its basic elements. For written texts this challenge would correspond to comparing the writing styles of different authors. This similarity has the following options:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><code>distribution</code></td><td>Possible values: <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/DistributionLL.html"><code>ll</code></a> and <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/DistributionSPL.html"><code>spl</code></a>.</td></tr>
<tr><td><code>lambda</code></td><td>Possible values: <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/LambdaDF.html"><code>df</code></a> and <a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/LambdaTTF.html"><code>ttf</code></a>.</td></tr>
<tr><td><code>normalization</code></td><td>Same as in <code>DFR</code> similarity.</td></tr>
</tbody></table>
<p>Type name: <code>IB</code></p>
<h4 id="lm-dirichlet-similarity"><a class="header" href="#lm-dirichlet-similarity">LM Dirichlet similarity.</a></h4>
<p><a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/LMDirichletSimilarity.html">LM Dirichlet similarity</a> . This similarity has the following options:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><code>mu</code></td><td>Default to <code>2000</code>.</td></tr>
</tbody></table>
<p>The scoring formula in the paper assigns negative scores to terms that have fewer occurrences than predicted by the language model, which is illegal to Lucene, so such terms get a score of 0.</p>
<p>Type name: <code>LMDirichlet</code></p>
<h4 id="lm-jelinek-mercer-similarity"><a class="header" href="#lm-jelinek-mercer-similarity">LM Jelinek Mercer similarity.</a></h4>
<p><a href="https://lucene.apache.org/core/8_8_2/core/org/apache/lucene/search/similarities/LMJelinekMercerSimilarity.html">LM Jelinek Mercer similarity</a> . The algorithm attempts to capture important patterns in the text, while leaving out noise. This similarity has the following options:</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><code>lambda</code></td><td>The optimal value depends on both the collection and the query. The optimal value is around <code>0.1</code> for title queries and <code>0.7</code> for long queries. Default to <code>0.1</code>. When value approaches <code>0</code>, documents that match more query terms will be ranked higher than those that match fewer terms.</td></tr>
</tbody></table>
<p>Type name: <code>LMJelinekMercer</code></p>
<h4 id="scripted-similarity"><a class="header" href="#scripted-similarity">Scripted similarity</a></h4>
<p>一种相似性，允许您使用脚本来指定应如何计算分数。</p>
<p>例如，下面的示例显示了如何重新实现tf-idf:</p>
<pre><code class="language-console">PUT /index
{
  &quot;settings&quot;: {
    &quot;number_of_shards&quot;: 1,
    &quot;similarity&quot;: {
      &quot;scripted_tfidf&quot;: {
        &quot;type&quot;: &quot;scripted&quot;,
        &quot;script&quot;: {
          &quot;source&quot;: &quot;double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;&quot;
        }
      }
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;field&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;similarity&quot;: &quot;scripted_tfidf&quot;
      }
    }
  }
}

PUT /index/_doc/1
{
  &quot;field&quot;: &quot;foo bar foo&quot;
}

PUT /index/_doc/2
{
  &quot;field&quot;: &quot;bar baz&quot;
}

POST /index/_refresh

GET /index/_search?explain=true
{
  &quot;query&quot;: {
    &quot;query_string&quot;: {
      &quot;query&quot;: &quot;foo^1.7&quot;,
      &quot;default_field&quot;: &quot;field&quot;
    }
  }
}
</code></pre>
<pre><code class="language-console-result">{
  &quot;took&quot;: 12,
  &quot;timed_out&quot;: false,
  &quot;_shards&quot;: {
    &quot;total&quot;: 1,
    &quot;successful&quot;: 1,
    &quot;skipped&quot;: 0,
    &quot;failed&quot;: 0
  },
  &quot;hits&quot;: {
    &quot;total&quot;: {
        &quot;value&quot;: 1,
        &quot;relation&quot;: &quot;eq&quot;
    },
    &quot;max_score&quot;: 1.9508477,
    &quot;hits&quot;: [
      {
        &quot;_shard&quot;: &quot;[index][0]&quot;,
        &quot;_node&quot;: &quot;OzrdjxNtQGaqs4DmioFw9A&quot;,
        &quot;_index&quot;: &quot;index&quot;,
        &quot;_type&quot;: &quot;_doc&quot;,
        &quot;_id&quot;: &quot;1&quot;,
        &quot;_score&quot;: 1.9508477,
        &quot;_source&quot;: {
          &quot;field&quot;: &quot;foo bar foo&quot;
        },
        &quot;_explanation&quot;: {
          &quot;value&quot;: 1.9508477,
          &quot;description&quot;: &quot;weight(field:foo in 0) [PerFieldSimilarity], result of:&quot;,
          &quot;details&quot;: [
            {
              &quot;value&quot;: 1.9508477,
              &quot;description&quot;: &quot;score from ScriptedSimilarity(weightScript=[null], script=[Script{type=inline, lang='painless', idOrCode='double tf = Math.sqrt(doc.freq); double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; double norm = 1/Math.sqrt(doc.length); return query.boost * tf * idf * norm;', options={}, params={}}]) computed from:&quot;,
              &quot;details&quot;: [
                {
                  &quot;value&quot;: 1.0,
                  &quot;description&quot;: &quot;weight&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 1.7,
                  &quot;description&quot;: &quot;query.boost&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 2,
                  &quot;description&quot;: &quot;field.docCount&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 4,
                  &quot;description&quot;: &quot;field.sumDocFreq&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 5,
                  &quot;description&quot;: &quot;field.sumTotalTermFreq&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 1,
                  &quot;description&quot;: &quot;term.docFreq&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 2,
                  &quot;description&quot;: &quot;term.totalTermFreq&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 2.0,
                  &quot;description&quot;: &quot;doc.freq&quot;,
                  &quot;details&quot;: []
                },
                {
                  &quot;value&quot;: 3,
                  &quot;description&quot;: &quot;doc.length&quot;,
                  &quot;details&quot;: []
                }
              ]
            }
          ]
        }
      }
    ]
  }
}
</code></pre>
<p>While scripted similarities provide a lot of flexibility, there is a set of rules that they need to satisfy. Failing to do so could make Elasticsearch silently return wrong top hits or fail with internal errors at search time:</p>
<ul>
<li>Returned scores must be positive.</li>
<li>All other variables remaining equal, scores must not decrease when <code>doc.freq</code> increases.</li>
<li>All other variables remaining equal, scores must not increase when <code>doc.length</code> increases.</li>
</ul>
<p>It is possible to make the above slightly more efficient by providing an <code>weight_script</code> which will compute the document-independent part of the score and will be available under the <code>weight</code> variable. When no <code>weight_script</code> is provided, <code>weight</code> is equal to <code>1</code>. The <code>weight_script</code> has access to the same variables as the <code>script</code> except <code>doc</code> since it is supposed to compute a document-independent contribution to the score.</p>
<p>The below configuration will give the same tf-idf scores but is slightly more efficient:</p>
<pre><code>PUT /index
{
  &quot;settings&quot;: {
    &quot;number_of_shards&quot;: 1,
    &quot;similarity&quot;: {
      &quot;scripted_tfidf&quot;: {
        &quot;type&quot;: &quot;scripted&quot;,
        &quot;weight_script&quot;: {
          &quot;source&quot;: &quot;double idf = Math.log((field.docCount+1.0)/(term.docFreq+1.0)) + 1.0; return query.boost * idf;&quot;
        },
        &quot;script&quot;: {
          &quot;source&quot;: &quot;double tf = Math.sqrt(doc.freq); double norm = 1/Math.sqrt(doc.length); return weight * tf * norm;&quot;
        }
      }
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;field&quot;: {
        &quot;type&quot;: &quot;text&quot;,
        &quot;similarity&quot;: &quot;scripted_tfidf&quot;
      }
    }
  }
}
</code></pre>
<h4 id="default-similarity"><a class="header" href="#default-similarity">Default Similarity</a></h4>
<p>By default, Elasticsearch will use whatever similarity is configured as <code>default</code>.</p>
<p>You can change the default similarity for all fields in an index when it is <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-create-index.html">created</a>:</p>
<pre><code class="language-console">PUT /index
{
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;similarity&quot;: {
        &quot;default&quot;: {
          &quot;type&quot;: &quot;boolean&quot;
        }
      }
    }
  }
}
</code></pre>
<p>If you want to change the default similarity after creating the index you must <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-open-close.html">close</a> your index, send the following request and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-open-close.html">open</a> it again afterwards:</p>
<pre><code class="language-console">POST /index/_close?wait_for_active_shards=0

PUT /index/_settings
{
  &quot;index&quot;: {
    &quot;similarity&quot;: {
      &quot;default&quot;: {
        &quot;type&quot;: &quot;boolean&quot;
      }
    }
  }
}

POST /index/_open
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="slow-log"><a class="header" href="#slow-log">Slow Log</a></h2>
<h3 id="search-slow-log"><a class="header" href="#search-slow-log">Search Slow Log</a></h3>
<p>分片级慢搜索日志允许将慢速搜索 (查询和获取阶段) 记录到专用日志文件中。</p>
<p>可以为执行的查询阶段和fetch阶段设置阈值，这里是一个示例:</p>
<pre><code class="language-yaml">index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.trace: 500ms

index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.trace: 200ms

index.search.slowlog.level: info
</code></pre>
<p>All of the above settings are <em>dynamic</em> and can be set for each index using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-update-settings.html">update indices settings</a> API. For example:</p>
<pre><code class="language-console">PUT /my-index-000001/_settings
{
  &quot;index.search.slowlog.threshold.query.warn&quot;: &quot;10s&quot;,
  &quot;index.search.slowlog.threshold.query.info&quot;: &quot;5s&quot;,
  &quot;index.search.slowlog.threshold.query.debug&quot;: &quot;2s&quot;,
  &quot;index.search.slowlog.threshold.query.trace&quot;: &quot;500ms&quot;,
  &quot;index.search.slowlog.threshold.fetch.warn&quot;: &quot;1s&quot;,
  &quot;index.search.slowlog.threshold.fetch.info&quot;: &quot;800ms&quot;,
  &quot;index.search.slowlog.threshold.fetch.debug&quot;: &quot;500ms&quot;,
  &quot;index.search.slowlog.threshold.fetch.trace&quot;: &quot;200ms&quot;,
  &quot;index.search.slowlog.level&quot;: &quot;info&quot;
}
</code></pre>
<p>默认情况下，没有启用 (设置为-1)。级别(<code>warn</code>, <code>info</code>, <code>debug</code>, <code>trace</code>) 允许控制将日志记录在哪个日志记录级别下。</p>
<p>并非所有都需要配置 (例如，只能设置warn阈值)。几个级别的好处是能够针对违反的特定阈值快速 “grep”。</p>
<p>The logging is done on the shard level scope, meaning the execution of a search request within a specific shard.</p>
<p>日志记录是在分片级别范围上完成的，这意味着在特定分片中执行搜索请求。</p>
<p>它不包含整个搜索请求，可以将其广播到多个分片以执行</p>
<p>与请求级别相比，分片级别日志记录的一些好处是在特定计算机上实际执行的关联。</p>
<p>默认情况下，日志记录文件使用以下配置 (在log4j2.properties中找到):</p>
<pre><code class="language-properties">appender.index_search_slowlog_rolling.type = RollingFile
appender.index_search_slowlog_rolling.name = index_search_slowlog_rolling
appender.index_search_slowlog_rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_search_slowlog.log
appender.index_search_slowlog_rolling.layout.type = PatternLayout
appender.index_search_slowlog_rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] [%node_name]%marker %.-10000m%n
appender.index_search_slowlog_rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_search_slowlog-%i.log.gz
appender.index_search_slowlog_rolling.policies.type = Policies
appender.index_search_slowlog_rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.index_search_slowlog_rolling.policies.size.size = 1GB
appender.index_search_slowlog_rolling.strategy.type = DefaultRolloverStrategy
appender.index_search_slowlog_rolling.strategy.max = 4

logger.index_search_slowlog_rolling.name = index.search.slowlog
logger.index_search_slowlog_rolling.level = trace
logger.index_search_slowlog_rolling.appenderRef.index_search_slowlog_rolling.ref = index_search_slowlog_rolling
logger.index_search_slowlog_rolling.additivity = false
</code></pre>
<h4 id="identifying-search-slow-log-origin"><a class="header" href="#identifying-search-slow-log-origin">Identifying search slow log origin</a></h4>
<p>It is often useful to identify what triggered a slow running query. If a call was initiated with an <code>X-Opaque-ID</code> header, then the user ID is included in Search Slow logs as an additional <strong>id</strong> field (scroll to the right).</p>
<p>识别是什么触发了运行缓慢的查询通常很有用。如果使用X-Opaque-ID标头启动了调用，则该用户id将作为附加ID字段包含在 “搜索慢日志” 中。</p>
<pre><code class="language-txt">[2030-08-30T11:59:37,786][WARN ][i.s.s.query              ] [node-0] [index6][0] took[78.4micros], took_millis[0], total_hits[0 hits], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{&quot;query&quot;:{&quot;match_all&quot;:{&quot;boost&quot;:1.0}}}], id[MY_USER_ID],
</code></pre>
<p>用户ID也包含在JSON日志中。</p>
<pre><code class="language-js">{
  &quot;type&quot;: &quot;index_search_slowlog&quot;,
  &quot;timestamp&quot;: &quot;2030-08-30T11:59:37,786+02:00&quot;,
  &quot;level&quot;: &quot;WARN&quot;,
  &quot;component&quot;: &quot;i.s.s.query&quot;,
  &quot;cluster.name&quot;: &quot;distribution_run&quot;,
  &quot;node.name&quot;: &quot;node-0&quot;,
  &quot;message&quot;: &quot;[index6][0]&quot;,
  &quot;took&quot;: &quot;78.4micros&quot;,
  &quot;took_millis&quot;: &quot;0&quot;,
  &quot;total_hits&quot;: &quot;0 hits&quot;,
  &quot;stats&quot;: &quot;[]&quot;,
  &quot;search_type&quot;: &quot;QUERY_THEN_FETCH&quot;,
  &quot;total_shards&quot;: &quot;1&quot;,
  &quot;source&quot;: &quot;{\&quot;query\&quot;:{\&quot;match_all\&quot;:{\&quot;boost\&quot;:1.0}}}&quot;,
  &quot;id&quot;: &quot;MY_USER_ID&quot;,
  &quot;cluster.uuid&quot;: &quot;Aq-c-PAeQiK3tfBYtig9Bw&quot;,
  &quot;node.id&quot;: &quot;D7fUYfnfTLa2D7y-xw6tZg&quot;
}
</code></pre>
<h3 id="index-slow-log"><a class="header" href="#index-slow-log">Index Slow log</a></h3>
<p>索引慢日志，功能类似于搜索慢日志。日志文件名以 _ index_indexing_slowlog.log结尾。</p>
<p>日志和阈值的配置方式与搜索慢日志相同。索引慢日志样本:</p>
<pre><code class="language-yaml">index.indexing.slowlog.threshold.index.warn: 10s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.debug: 2s
index.indexing.slowlog.threshold.index.trace: 500ms
index.indexing.slowlog.level: info
index.indexing.slowlog.source: 1000
</code></pre>
<p>All of the above settings are <em>dynamic</em> and can be set for each index using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-update-settings.html">update indices settings</a> API. For example:</p>
<pre><code class="language-console">PUT /my-index-000001/_settings
{
  &quot;index.indexing.slowlog.threshold.index.warn&quot;: &quot;10s&quot;,
  &quot;index.indexing.slowlog.threshold.index.info&quot;: &quot;5s&quot;,
  &quot;index.indexing.slowlog.threshold.index.debug&quot;: &quot;2s&quot;,
  &quot;index.indexing.slowlog.threshold.index.trace&quot;: &quot;500ms&quot;,
  &quot;index.indexing.slowlog.level&quot;: &quot;info&quot;,
  &quot;index.indexing.slowlog.source&quot;: &quot;1000&quot;
}
</code></pre>
<ol>
<li>默认情况下，Elasticsearch将记录慢日志中 _source的前1000个字符。</li>
<li>You can change that with <code>index.indexing.slowlog.source</code>. </li>
<li>Setting it to <code>false</code> or <code>0</code> will skip logging the source entirely, </li>
<li>while setting it to <code>true</code> will log the entire source regardless of size. </li>
<li>默认情况下，原始 _source会重新格式化，以确保它适合单个日志行。</li>
<li>If preserving the original document format is important, you can turn off reformatting by setting <code>index.indexing.slowlog.reformat</code> to <code>false</code>, which will cause the source to be logged &quot;as is&quot; and can potentially span multiple log lines.</li>
<li>如果保留原始文档格式很重要，则可以通过将<em>index.indexing.slowlog.reformat</em>设置为false来关闭重新格式化，这将导致源 “按原样” 记录，并且可能跨越多个日志行。</li>
<li>索引慢日志文件默认配置在log4j2.properties文件中:</li>
</ol>
<pre><code class="language-properties">appender.index_indexing_slowlog_rolling.type = RollingFile
appender.index_indexing_slowlog_rolling.name = index_indexing_slowlog_rolling
appender.index_indexing_slowlog_rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_indexing_slowlog.log
appender.index_indexing_slowlog_rolling.layout.type = PatternLayout
appender.index_indexing_slowlog_rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] [%node_name]%marker %.-10000m%n
appender.index_indexing_slowlog_rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_index_indexing_slowlog-%i.log.gz
appender.index_indexing_slowlog_rolling.policies.type = Policies
appender.index_indexing_slowlog_rolling.policies.size.type = SizeBasedTriggeringPolicy
appender.index_indexing_slowlog_rolling.policies.size.size = 1GB
appender.index_indexing_slowlog_rolling.strategy.type = DefaultRolloverStrategy
appender.index_indexing_slowlog_rolling.strategy.max = 4

logger.index_indexing_slowlog.name = index.indexing.slowlog.index
logger.index_indexing_slowlog.level = trace
logger.index_indexing_slowlog.appenderRef.index_indexing_slowlog_rolling.ref = index_indexing_slowlog_rolling
logger.index_indexing_slowlog.additivity = false
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="store"><a class="header" href="#store">Store</a></h2>
<p>存储模块允许您控制索引数据在磁盘上的存储和访问方式。</p>
<p>这是一个低级的设置。某些存储实现的并发性较差，或者禁用了对堆内存使用的优化。我们建议坚持使用默认值。</p>
<h3 id="file-system-storage-types"><a class="header" href="#file-system-storage-types">File system storage types</a></h3>
<p>有不同的文件系统实现或存储类型。默认情况下，Elasticsearch会根据操作环境选择最佳实现。</p>
<p>还可以通过在<em>config/elasticsearch.yml</em>文件中配置存储类型，为所有索引显式设置存储类型:</p>
<pre><code class="language-yaml">index.store.type: hybridfs
</code></pre>
<p>它是一个静态设置，可以在索引创建时按索引设置:</p>
<pre><code class="language-console">PUT /my-index-000001
{
  &quot;settings&quot;: {
    &quot;index.store.type&quot;: &quot;hybridfs&quot;
  }
}
</code></pre>
<p>这是一个仅限专家的设置，将来可能会删除。</p>
<p>以下部分列出了支持的所有不同存储类型。</p>
<ul>
<li>
<p><strong><code>fs</code></strong></p>
<p>Default file system implementation. This will pick the best implementation depending on the operating environment, which is currently <code>hybridfs</code> on all supported systems but is subject to change.</p>
</li>
<li>
<p><strong><code>simplefs</code></strong></p>
<p>The Simple FS type is a straightforward implementation of file system storage (maps to Lucene <code>SimpleFsDirectory</code>) using a random access file. This implementation has poor concurrent performance (multiple threads will bottleneck) and disables some optimizations for heap memory usage.</p>
</li>
<li>
<p><strong><code>niofs</code></strong></p>
<p>The NIO FS type stores the shard index on the file system (maps to Lucene <code>NIOFSDirectory</code>) using NIO. It allows multiple threads to read from the same file concurrently. It is not recommended on Windows because of a bug in the SUN Java implementation and disables some optimizations for heap memory usage.</p>
</li>
<li>
<p><strong><code>mmapfs</code></strong></p>
<p>The MMap FS type stores the shard index on the file system (maps to Lucene <code>MMapDirectory</code>) by mapping a file into memory (mmap). Memory mapping uses up a portion of the virtual memory address space in your process equal to the size of the file being mapped. Before using this class, be sure you have allowed plenty of <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/vm-max-map-count.html">virtual address space</a>.</p>
</li>
<li>
<p><strong><code>hybridfs</code></strong></p>
<p>The <code>hybridfs</code> type is a hybrid of <code>niofs</code> and <code>mmapfs</code>, which chooses the best file system type for each type of file based on the read access pattern. Currently only the Lucene term dictionary, norms and doc values files are memory mapped. All other files are opened using Lucene <code>NIOFSDirectory</code>. Similarly to <code>mmapfs</code> be sure you have allowed plenty of <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/vm-max-map-count.html">virtual address space</a>.</p>
</li>
</ul>
<ol>
<li>You can restrict the use of the <code>mmapfs</code> and the related <code>hybridfs</code> store type via the setting <code>node.store.allow_mmap</code>. </li>
<li>This is a boolean setting indicating whether or not memory-mapping is allowed. </li>
<li>The default is to allow it. This setting is useful, for example, if you are in an environment where you can not control the ability to create a lot of memory maps so you need disable the ability to use memory-mapping.</li>
</ol>
<h3 id="preloading-data-into-the-file-system-cache"><a class="header" href="#preloading-data-into-the-file-system-cache">Preloading data into the file system cache</a></h3>
<blockquote>
<p>This is an expert setting, the details of which may change in the future.</p>
</blockquote>
<ol>
<li>默认情况下，Elasticsearch完全依赖操作系统文件系统缓存来缓存I/O操作</li>
<li>It is possible to set <code>index.store.preload</code> in order to tell the operating system to load the content of hot index files into memory upon opening.</li>
<li>可以设置<code>index.store.preload</code>，以便告诉操作系统在打开时将热索引文件的内容加载到内存中</li>
<li>此设置接受以逗号分隔的文件扩展名列表: 所有扩展名在列表中的文件将在打开时预加载</li>
<li>这对于提高索引的搜索性能非常有用，尤其是在重新启动主机操作系统时，因为这会导致文件系统缓存被丢弃</li>
<li>但是请注意，这可能会减慢索引的打开速度，因为它们仅在数据已加载到物理内存后才可用。</li>
</ol>
<p>此设置仅是尽力而为，根据store 类型和主机操作系统的不同，可能根本不起作用。</p>
<p>The <code>index.store.preload</code> is a static setting that can either be set in the <code>config/elasticsearch.yml</code>:</p>
<pre><code class="language-yaml">index.store.preload: [&quot;nvd&quot;, &quot;dvd&quot;]
</code></pre>
<p>or in the index settings at index creation time:</p>
<pre><code class="language-console">PUT /my-index-000001
{
  &quot;settings&quot;: {
    &quot;index.store.preload&quot;: [&quot;nvd&quot;, &quot;dvd&quot;]
  }
}
</code></pre>
<ol>
<li>默认值是空数组，这意味着什么都不会被急切地加载到文件系统缓存中。</li>
<li>对于主动搜索的索引，您可能希望将其设置为 [“nvd”，“dvd”]，这将导致 norms 和doc values 急切地加载到物理内存中</li>
<li>These are the two first extensions to look at since Elasticsearch performs random access on them.</li>
<li>这是两个 首要的扩展名。因为 Elasticsearch 执行 在它们身上在执行 random access </li>
<li>可以使用通配符来指示应预加载所有文件 <code>index.store.preload: [&quot;*&quot;]</code>. </li>
<li>但是请注意，将所有文件加载到内存中通常没有用，尤其是存储字段和term vectors 的文件。</li>
<li>所以更好的选择可能是将其设置为 <code>[&quot;nvd&quot;, &quot;dvd&quot;, &quot;tim&quot;, &quot;doc&quot;, &quot;dim&quot;]</code>, which will preload norms, doc values, terms dictionaries, postings lists and points,这些是索引中搜索和聚合的最重要部分。</li>
<li>请注意，此设置对于大于主机主内存大小的索引可能是危险的，因为它会导致文件系统缓存在大合并后重新打开时被丢弃，这将使索引和搜索速度变慢。</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="translog"><a class="header" href="#translog">Translog</a></h2>
<ol>
<li>对Lucene的更改仅在Lucene提交期间保留到磁盘，这是相对昂贵的操作，因此无法在每次索引或删除操作后执行</li>
<li>在进程退出或硬件故障的情况下，Lucene将在一次提交之后和另一次提交之前发生的更改从索引中删除。</li>
<li>Lucene提交过于昂贵，无法对每个单独的更改执行，因此每个分片副本还将操作写入其称为<em>translog</em>的事务日志中。</li>
<li>在内部Lucene索引处理后但在确认提交之前，所有索引和删除操作都将写入translog</li>
<li>如果发生崩溃，则在分片恢复时，将从translog中恢复已确认但尚未包含在上次Lucene提交中的最近操作。</li>
<li>Elasticsearch刷新 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-flush.html">flush</a> 是执行Lucene提交并开始新的translog生成的过程。</li>
<li>刷新在后台自动执行，以确保translog不会增长太大，如果太大这将使重放其操作在恢复过程中花费大量时间</li>
<li>手动执行刷新的能力也通过API公开，尽管这很少需要。</li>
</ol>
<h3 id="translog-settings"><a class="header" href="#translog-settings">Translog settings</a></h3>
<ol>
<li>translog中的数据只有在fsynced和committed translog时才会持久化到磁盘</li>
<li>如果发生硬件故障或操作系统崩溃或JVM崩溃或分片故障，则自上一次 的translog 提交以来写入的任何数据都将丢失。</li>
<li>默认情况下， <code>index.translog.durability</code> is set to <code>request</code> ，这意味着Elasticsearch仅在成功fsyned并在主副本和每个已分配副本上提交translog后，才会向客户端报告索引、删除、更新或批量请求的成功。</li>
<li>如果设置成 <code>index.translog.durability</code> is set to <code>async</code> ，然后Elasticsearch fsyncs并提交translog，在每个<em>index.translog.sync_interval</em> 期间这意味着在崩溃之前执行的任何操作都可能在节点恢复时丢失。</li>
</ol>
<p>以下可动态更新的每个索引设置控制translog的行为:</p>
<ul>
<li>
<p><strong><code>index.translog.sync_interval</code></strong></p>
<p>How often the translog is <code>fsync</code>ed to disk and committed, regardless of write operations. Defaults to <code>5s</code>. Values less than <code>100ms</code> are not allowed.</p>
<p>无论写操作如何，translog多久被同步到磁盘并提交一次。默认为5s。小于100ms的值是不允许的。</p>
</li>
<li>
<p><strong><code>index.translog.durability</code></strong></p>
<ol>
<li>是否在每个索引、删除、更新或批量请求后fsync和提交translog。</li>
<li>此设置接受以下参数</li>
<li><strong><code>request</code></strong>(default) <code>Fsync</code>在每次请求后提交。如果发生硬件故障，所有确认的写入都将已提交到磁盘</li>
<li><code>async</code>   <code>fsync</code> and commit in the background every <code>sync_interval</code>. 如果发生故障，自上次自动提交以来的所有已确认写入都将被丢弃。</li>
</ol>
</li>
<li>
<p><strong><code>index.translog.flush_threshold_size</code></strong></p>
<ul>
<li>translog存储尚未安全地保存在Lucene中的所有操作 (即，不是Lucene提交点的一部分)</li>
<li>尽管这些操作可用于读取，但如果分片已停止并必须恢复，则需要对其进行重放。</li>
<li>此设置控制这些操作的最大总大小，以防止恢复时间过长。</li>
<li>一旦达到最大大小，将发生刷新，生成新的Lucene提交点。默认为512mb。</li>
</ul>
</li>
</ul>
<h4 id="translog-retention"><a class="header" href="#translog-retention">Translog retention</a></h4>
<h3 id="deprecated-in-740"><a class="header" href="#deprecated-in-740">Deprecated in 7.4.0.</a></h3>
<p>Translog retention settings are deprecated in favor of <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-history-retention.html">soft deletes</a>. These settings are effectively ignored since 7.4 and will be removed in a future version.</p>
<p>If an index is not using <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-history-retention.html">soft deletes</a> to retain historical operations then Elasticsearch recovers each replica shard by replaying operations from the primary’s translog. This means it is important for the primary to preserve extra operations in its translog in case it needs to rebuild a replica. Moreover it is important for each replica to preserve extra operations in its translog in case it is promoted to primary and then needs to rebuild its own replicas in turn. The following settings control how much translog is retained for peer recoveries.</p>
<ul>
<li>
<p><strong><code>index.translog.retention.size</code></strong></p>
<p>This controls the total size of translog files to keep for each shard. Keeping more translog files increases the chance of performing an operation based sync when recovering a replica. If the translog files are not sufficient, replica recovery will fall back to a file based sync. Defaults to <code>512mb</code>. This setting is ignored, and should not be set, if soft deletes are enabled. Soft deletes are enabled by default in indices created in Elasticsearch versions 7.0.0 and later.</p>
</li>
<li>
<p><strong><code>index.translog.retention.age</code></strong></p>
<p>This controls the maximum duration for which translog files are kept by each shard. Keeping more translog files increases the chance of performing an operation based sync when recovering replicas. If the translog files are not sufficient, replica recovery will fall back to a file based sync. Defaults to <code>12h</code>. This setting is ignored, and should not be set, if soft deletes are enabled. Soft deletes are enabled by default in indices created in Elasticsearch versions 7.0.0 and later.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="history-retention"><a class="header" href="#history-retention">History retention</a></h2>
<p>Elasticsearch有时需要重放在分片上执行的一些操作</p>
<p>例如，如果副本短暂处于脱机状态，则重播脱机时错过的一些操作可能比从头开始重建要有效得多</p>
<p>同样，跨集群复制的工作原理是在leader集群上执行操作，然后在follower集群上重放这些操作</p>
<p>在Lucene级别，实际上Elasticsearch对索引执行的写操作只有两个</p>
<ol>
<li>
<p>新文档可能会被索引</p>
</li>
<li>
<p>现有文档可能会被删除。</p>
</li>
<li>
<p>通过原子删除旧文档，然后为新文档建立索引来实现更新。</p>
</li>
<li>
<p>索引到Lucene的文档已经包含重放该索引操作所需的所有信息，但是文档删除并非如此。</p>
</li>
<li>
<p>为了解决这个问题，Elasticsearch使用一种称为软删除的功能来保留Lucene索引中最近的删除，以便可以重放它们。</p>
</li>
<li>
<p>Elasticsearch仅在索引中保留某些最近删除的文档，因为软删除的文档仍然会占用一些空间</p>
</li>
<li>
<p>最终，Elasticsearch将完全丢弃这些软删除的文档，以释放该空间，从而使索引不会随着时间的推移而变得越来越大。</p>
</li>
<li>
<p>幸运的是，Elasticsearch不需要能够重放曾经在分片上执行过的每个操作，因为始终可以在远程节点上制作分片的完整副本。</p>
</li>
<li>
<p>但是，复制整个分片可能比重放一些缺少的操作要花费更长的时间，因此Elasticsearch尝试保留其将来需要重放的所有操作。</p>
</li>
</ol>
<p>Elasticsearch使用称为<em>shard history retention leases</em> 的机制来跟踪将来需要重播的操作。</p>
<p>每个可能需要重放操作的分片副本必须首先为自己创建一个历史保留租约</p>
<p>例如，使用跨集群复制时，此分片副本可能是分片的副本，也可能是跟随索引的分片</p>
<p>每个保留租约都会跟踪相应分片副本尚未收到的第一个操作的序列号</p>
<p>当分片副本接收到新的操作时，它会增加其保留租约中包含的序列号，以表明将来不需要重播这些操作</p>
<p>一旦软删除的操作没有被任何保留租约持有，Elasticsearch就会丢弃它们。</p>
<ol>
<li>
<p>如果分片副本失败，则它将停止更新其分片历史记录保留租约，这意味着Elasticsearch将保留所有新操作，以便在失败的分片副本恢复时可以重放它们</p>
</li>
<li>
<p>然而，保留租约只能持续有限的时间</p>
</li>
<li>
<p>如果分片副本恢复得不够快，则其保留租约可能会到期。</p>
</li>
<li>
<p>如果分片副本永久失败，这可以保护Elasticsearch永远保留历史记录，</p>
</li>
<li>
<p>因为一旦保留租约到期，Elasticsearch可以再次开始丢弃历史记录</p>
</li>
<li>
<p>如果分片副本在保留租约到期后在恢复，则Elasticsearch将退回到复制整个索引，因为它不再可以简单地重播丢失的历史记录</p>
</li>
<li>
<p>保留租约的到期时间默认为12小时，对于大多数合理的恢复情况，该时间应足够长。</p>
</li>
<li>
<p>默认情况下，在最新版本中创建的索引上启用软删除，但可以在索引创建时显式启用或禁用它们</p>
</li>
<li>
<p>如果禁用了软删除，则有时仍然可以通过仅从<em>translog</em>中复制丢失的操作来进行对等恢复，只要这些操作保留在那里即可（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules-translog.html#index-modules-translog-retention">as long as those operations are retained there</a>.）。如果禁用软删除，跨集群复制将不起作用。</p>
</li>
</ol>
<h3 id="history-retention-settings"><a class="header" href="#history-retention-settings">History retention settings</a></h3>
<ul>
<li>
<p><strong><code>index.soft_deletes.enabled</code></strong></p>
<p>[7.6.0] Deprecated in 7.6.0. Creating indices with soft-deletes disabled is deprecated and will be removed in future Elasticsearch versions.Indicates whether soft deletes are enabled on the index. Soft deletes can only be configured at index creation and only on indices created on or after Elasticsearch 6.5.0. Defaults to <code>true</code>.</p>
</li>
<li>
<p><strong><code>index.soft_deletes.retention_lease.period</code></strong></p>
<p>The maximum period to retain a shard history retention lease before it is considered expired. Shard history retention leases ensure that soft deletes are retained during merges on the Lucene index. If a soft delete is merged away before it can be replicated to a follower the following process will fail due to incomplete history on the leader. Defaults to <code>12h</code>.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-sorting"><a class="header" href="#index-sorting">Index Sorting</a></h2>
<ol>
<li>
<p>在Elasticsearch中创建新索引时，可以配置如何对每个分片中的段进行排序。</p>
</li>
<li>
<p>默认情况下，Lucene不应用任何排序</p>
</li>
<li>
<p><code>index.sort.*</code> 设置定义了应使用哪些字段对每个段内的文档进行排序。</p>
</li>
<li>
<p>嵌套字段与索引排序不兼容，因为它们依赖于嵌套文档存储在连续的doc id中的假设，这些id可以通过索引排序来破坏。如果对包含嵌套字段的索引激活了索引排序，则会引发错误。</p>
</li>
</ol>
<pre><code class="language-console">PUT my-index-000001
{
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;sort.field&quot;: &quot;date&quot;, 
      &quot;sort.order&quot;: &quot;desc&quot;  
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;date&quot;: {
        &quot;type&quot;: &quot;date&quot;
      }
    }
  }
}
</code></pre>
<ol>
<li>
<p>This index is sorted by the date field</p>
</li>
<li>
<p>… in descending order.</p>
</li>
</ol>
<p>也可以按多个字段对索引进行排序:</p>
<pre><code class="language-console">PUT my-index-000001
{
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;sort.field&quot;: [ &quot;username&quot;, &quot;date&quot; ], 
      &quot;sort.order&quot;: [ &quot;asc&quot;, &quot;desc&quot; ]       
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;username&quot;: {
        &quot;type&quot;: &quot;keyword&quot;,
        &quot;doc_values&quot;: true
      },
      &quot;date&quot;: {
        &quot;type&quot;: &quot;date&quot;
      }
    }
  }
}
</code></pre>
<ol>
<li>This index is sorted by username first then by date</li>
<li>… in ascending order for the <code>username</code> field and in descending order for the <code>date</code> field.</li>
</ol>
<p><strong>索引排序支持以下设置:</strong></p>
<p><strong><code>index.sort.field</code></strong></p>
<p>The list of fields used to sort the index. Only <code>boolean</code>, <code>numeric</code>, <code>date</code> and <code>keyword</code> fields with <code>doc_values</code> are allowed here.</p>
<p><strong><code>index.sort.order</code></strong></p>
<p>The sort order to use for each field. The order option can have the following values:</p>
<ul>
<li><code>asc</code>: For ascending order</li>
<li><code>desc</code>: For descending order.</li>
</ul>
<p><strong><code>index.sort.mode</code></strong></p>
<p>Elasticsearch支持按多值字段排序。模式选项控制选择什么值来对文档进行排序。</p>
<p>模式选项可以具有以下值:</p>
<ul>
<li><code>min</code>: Pick the lowest value.</li>
<li><code>max</code>: Pick the highest value.</li>
</ul>
<p><strong><code>index.sort.missing</code></strong></p>
<p>缺少参数指定应如何处理缺少字段的文档。缺失值可以有以下值:</p>
<ul>
<li><code>_last</code>: Documents without value for the field are sorted last.</li>
<li><code>_first</code>: Documents without value for the field are sorted first.</li>
</ul>
<p><strong>注意</strong></p>
<ol>
<li>
<p>索引排序只能在索引创建时定义一次。</p>
</li>
<li>
<p>不允许在现有索引上添加或更新排序</p>
</li>
<li>
<p>索引排序在索引吞吐量方面也有成本，因为文档必须在刷新和合并时间进行排序。</p>
</li>
<li>
<p>在激活此功能之前，您应该测试对应用程序的影响。</p>
</li>
</ol>
<h3 id="early-termination-of-search-request"><a class="header" href="#early-termination-of-search-request">Early termination of search request</a></h3>
<p>默认情况下，在Elasticsearch中，搜索请求必须访问与查询匹配的每个文档，以检索按指定排序排序的 TOP 文档。</p>
<p>但是当索引排序和搜索排序相同时，可以限制每个段应访问的文档数量，以在全局范围内检索N个排名最高的文档。</p>
<p>例如，假设我们有一个包含按时间戳字段排序的事件的索引:</p>
<pre><code class="language-console">PUT events
{
  &quot;settings&quot;: {
    &quot;index&quot;: {
      &quot;sort.field&quot;: &quot;timestamp&quot;,
      &quot;sort.order&quot;: &quot;desc&quot; 
    }
  },
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;timestamp&quot;: {
        &quot;type&quot;: &quot;date&quot;
      }
    }
  }
}
</code></pre>
<p>此索引按时间戳按降序排序 (最近的第一个)</p>
<pre><code class="language-console">GET /events/_search
{
  &quot;size&quot;: 10,
  &quot;sort&quot;: [
    { &quot;timestamp&quot;: &quot;desc&quot; }
  ]
}
</code></pre>
<ol>
<li>
<p>Elasticsearch将检测到每个段的顶部文档已经在索引中排序，并且将仅比较每个段的前N个文档。</p>
</li>
<li>
<p>匹配查询的剩余的文档 被收集，以计算结果总数并构建聚合。</p>
</li>
</ol>
<p>如果您仅查找最后10个事件，并且对与查询匹配的文档总数不感兴趣，则可以将track_total_hits设置为false:</p>
<pre><code class="language-console">GET /events/_search
{
  &quot;size&quot;: 10,
  &quot;sort&quot;: [ 
      { &quot;timestamp&quot;: &quot;desc&quot; }
  ],
  &quot;track_total_hits&quot;: false
}
</code></pre>
<p>索引排序将用于对顶级文档进行排名，并且每个段将在前10个匹配之后提前终止集合。</p>
<p>这一次，Elasticsearch将不会尝试计算文档的数量，并且一旦每个段收集了N个文档，就能够终止查询。</p>
<pre><code class="language-console-result">{
  &quot;_shards&quot;: ...
   &quot;hits&quot; : {  
      &quot;max_score&quot; : null,
      &quot;hits&quot; : []
  },
  &quot;took&quot;: 20,
  &quot;timed_out&quot;: false
}
</code></pre>
<p>由于提前终止，匹配查询的命中总数未知。</p>
<p>聚合将收集与查询匹配的所有文档，而不管 “<em>track_total_hits</em>” 的值如何</p>
<h2 id="use-index-sorting-to-speed-up-conjunctions"><a class="header" href="#use-index-sorting-to-speed-up-conjunctions">Use index sorting to speed up conjunctions</a></h2>
<ol>
<li>索引排序对于组织Lucene doc id (不与 _id合并) 以使 conjunctions  (a和b和…) 更有效的方式很有用。</li>
<li>为了高效，conjunctions 依赖于这样一个事实，即如果任何子句不匹配，那么整个conjunction都不匹配。</li>
<li>通过使用索引排序，我们可以将不匹配的文档放在一起，这将有助于有效地跳过与连接不匹配的大范围doc id。</li>
<li>此技巧仅适用于低基数字段。</li>
<li>经验法则是，您应该首先对基数较低且经常用于过滤的字段进行排序。</li>
<li>排序顺序 (asc或desc) 并不重要，因为我们只关心将匹配相同子句的值彼此靠近。</li>
</ol>
<p>例如，如果您要索引要出售的汽车，则按燃料类型，车身类型，品牌，注册年份和最终里程进行分类可能会很有趣。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="indexing-pressure"><a class="header" href="#indexing-pressure">Indexing pressure</a></h2>
<ol>
<li>将文档索引到Elasticsearch以内存和CPU负载的形式引入系统负载</li>
<li>每个索引操作包括协调阶段、primary 阶段和复制阶段。</li>
<li>这些阶段可以跨集群中的多个节点执行。</li>
<li>索引压力可以通过外部操作 (例如索引请求) 或内部机制 (例如恢复和跨集群复制) 来建立</li>
<li>如果在系统中引入过多的索引工作，则群集可能会变得饱和</li>
<li>这可能会对其他操作产生不利影响，例如搜索，群集协调和后台处理。</li>
<li>为了防止这些问题，Elasticsearch内部监控索引负载。当负载超过一定限度时，新的索引工作被拒绝</li>
</ol>
<h3 id="indexing-stages"><a class="header" href="#indexing-stages">Indexing stages</a></h3>
<p>External indexing operations go through three stages: coordinating, primary, and replica. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-replication.html#basic-write-model">Basic write model</a>.</p>
<h3 id="memory-limits"><a class="header" href="#memory-limits">Memory limits</a></h3>
<ol>
<li>
<p><code>Indexing_presssure.memory.limit</code>节点设置限制了可用于未完成索引请求的字节数。</p>
</li>
<li>
<p>此设置默认为堆的10%。</p>
</li>
<li>
<p>在每个索引阶段开始时，Elasticsearch会占用索引请求消耗的字节,此统计仅在索引阶段结束时发布。这意味着上游阶段将会被统计请求负载，直到所有下游阶段都完成</p>
</li>
<li>
<p>例如，在完成  primary阶段和replica阶段之前，协调请求将保持统计。在每个同步副本响应（以在必要时启用副本重试之前），primary请求将会被保留</p>
</li>
<li>
<p>当未完成的协调，主要和副本索引字节的数量超过配置的限制时，节点将在协调或primary 阶段开始拒绝新的索引工作。</p>
</li>
<li>
<p>当未完成的副本索引字节数超过配置限制的1.5倍时，节点将在副本阶段开始拒绝新的索引工作。</p>
</li>
<li>
<p>这种设计意味着，随着在节点上 索引压力的建立，它们自然会停止接受协调和主要工作，而倾向于未完成的副本工作。</p>
</li>
<li>
<p>The <code>indexing_pressure.memory.limit</code> setting’s 10% default limit is generously sized. </p>
</li>
<li>
<p>你应该在仔细考虑后才改变它。只有索引请求才受限于此限制 </p>
</li>
<li>
<p>这意味着还有额外的索引开销 (缓冲区，侦听器等)，这也需要堆空间。Elasticsearch的其他组件也需要内存。将此限制设置得太高可能会拒绝其他操作和组件的操作内存。</p>
</li>
</ol>
<h3 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h3>
<p>您可以使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-nodes-stats.html#cluster-nodes-stats-api-response-body-indexing-pressure">node stats API</a>  检索索引压力指标。</p>
<h3 id="indexing-pressure-settings"><a class="header" href="#indexing-pressure-settings">Indexing pressure settings</a></h3>
<ul>
<li><code>indexing_pressure.memory.limit</code>
<ol>
<li>索引请求可能消耗的未完成字节数。</li>
<li>当达到或超过此限制时，节点将拒绝新的协调和Primary操作</li>
<li>当副本操作消耗此限制的1.5倍时，节点将拒绝新的副本操作。</li>
<li>默认为堆的10%。</li>
</ol>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="ingest-pipelines"><a class="header" href="#ingest-pipelines">Ingest pipelines</a></h2>
<p>摄取管道(Ingest pipelines)允许您在索引之前对数据执行常见的转换。</p>
<p>例如，您可以使用管道删除字段，从文本中提取值并丰富数据。</p>
<p>管道由一系列称为处理器( <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/processors.html">processors</a>)的可配置任务组成。每个处理器按顺序运行，对传入文档进行特定更改。处理器运行后，Elasticsearch将转换后的文档添加到数据流或索引中。</p>
<p>您可以使用Kibana的 “摄取管道” 功能或摄取api创建和管理摄取管道。Elasticsearch将管道存储在集群状态。</p>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<ul>
<li>具有ingest节点角色的节点处理管道处理。要使用  ingest pipelines，您的群集必须至少有一个具有 ingest pipelines, 角色的节点。对于繁重的摄取负载，我们建议创建专用的摄取节点。</li>
<li>如果启用了Elasticsearch安全功能，则必须具有<em>manage_pipeline</em>群集权限才能管理 pipelines。要使用Kibana的摄取管道功能，您还需要群集:  <code>cluster:monitor/nodes/info</code>  群集权限。</li>
<li>包括enrich处理器在内的管道需要额外的设置。 See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-enriching-data.html"><em>Enrich your data</em></a>.</li>
</ul>
<h2 id="create-and-manage-pipelines"><a class="header" href="#create-and-manage-pipelines">Create and manage pipelines</a></h2>
<p>在Kibana中，打开主菜单，然后<strong>Stack Management &gt; Ingest Pipelines</strong>。从列表视图中，您可以:</p>
<ul>
<li>View a list of your pipelines and drill down into details</li>
<li>Edit or clone existing pipelines</li>
<li>Delete pipelines</li>
</ul>
<p>To create a pipeline, click <strong>Create pipeline &gt; New pipeline</strong>. For an example tutorial, see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/common-log-format-example.html"><em>Example: Parse logs</em></a>.</p>
<p>The <strong>New pipeline from CSV</strong> option lets you use a CSV to create an ingest pipeline that maps custom data to the <a href="https://www.elastic.co/guide/en/ecs/8.1">Elastic Common Schema (ECS)</a>. Mapping your custom data to ECS makes the data easier to search and lets you reuse visualizations from other datasets. To get started, check <a href="https://www.elastic.co/guide/en/ecs/8.1/ecs-converting.html">Map custom data to ECS</a>.</p>
<p>You can also use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest-apis.html">ingest APIs</a> to create and manage pipelines. The following <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html">create pipeline API</a> request creates a pipeline containing two <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/set-processor.html"><code>set</code></a> processors followed by a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/lowercase-processor.html"><code>lowercase</code></a> processor. The processors run sequentially in the order specified.</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;description&quot;: &quot;My optional pipeline description&quot;,
  &quot;processors&quot;: [
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;My optional processor description&quot;,
        &quot;field&quot;: &quot;my-long-field&quot;,
        &quot;value&quot;: 10
      }
    },
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;Set 'my-boolean-field' to true&quot;,
        &quot;field&quot;: &quot;my-boolean-field&quot;,
        &quot;value&quot;: true
      }
    },
    {
      &quot;lowercase&quot;: {
        &quot;field&quot;: &quot;my-keyword-field&quot;
      }
    }
  ]
}
</code></pre>
<h2 id="manage-pipeline-versions"><a class="header" href="#manage-pipeline-versions">Manage pipeline versions</a></h2>
<p>When you create or update a pipeline, you can specify an optional <code>version</code> integer</p>
<p>创建或更新管道时，可以指定可选的版本整数。</p>
<p>。您可以将此版本号与  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/put-pipeline-api.html#put-pipeline-api-query-params"><code>if_version</code></a> parameter 一起使用，以有条件地更新管道。当指定<em>if_version</em>参数时，成功的更新会增加管道的版本</p>
<pre><code>PUT _ingest/pipeline/my-pipeline-id
{
  &quot;version&quot;: 1,
  &quot;processors&quot;: [ ... ]
}
</code></pre>
<p>要使用API取消设置版本号，请在不指定版本参数的情况下替换或更新管道。</p>
<h2 id="test-a-pipeline"><a class="header" href="#test-a-pipeline">Test a pipeline</a></h2>
<p>在生产中使用管道之前，我们建议您使用示例文档对其进行测试。在Kibana中创建或编辑管道时，请单击 “添加文档”。在文档页签中，提供示例文档，然后单击运行管道。</p>
<p>You can also test pipelines using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/simulate-pipeline-api.html">simulate pipeline API</a>. You can specify a configured pipeline in the request path. For example, the following request tests <code>my-pipeline</code>.</p>
<pre><code>POST _ingest/pipeline/my-pipeline/_simulate
{
  &quot;docs&quot;: [
    {
      &quot;_source&quot;: {
        &quot;my-keyword-field&quot;: &quot;FOO&quot;
      }
    },
    {
      &quot;_source&quot;: {
        &quot;my-keyword-field&quot;: &quot;BAR&quot;
      }
    }
  ]
}
</code></pre>
<p>Alternatively, you can specify a pipeline and its processors in the request body.</p>
<pre><code class="language-console">POST _ingest/pipeline/_simulate
{
  &quot;pipeline&quot;: {
    &quot;processors&quot;: [
      {
        &quot;lowercase&quot;: {
          &quot;field&quot;: &quot;my-keyword-field&quot;
        }
      }
    ]
  },
  &quot;docs&quot;: [
    {
      &quot;_source&quot;: {
        &quot;my-keyword-field&quot;: &quot;FOO&quot;
      }
    },
    {
      &quot;_source&quot;: {
        &quot;my-keyword-field&quot;: &quot;BAR&quot;
      }
    }
  ]
}
</code></pre>
<p>The API returns transformed documents:</p>
<pre><code class="language-console-result">{
  &quot;docs&quot;: [
    {
      &quot;doc&quot;: {
        &quot;_index&quot;: &quot;_index&quot;,
        &quot;_type&quot;: &quot;_doc&quot;,
        &quot;_id&quot;: &quot;_id&quot;,
        &quot;_source&quot;: {
          &quot;my-keyword-field&quot;: &quot;foo&quot;
        },
        &quot;_ingest&quot;: {
          &quot;timestamp&quot;: &quot;2099-03-07T11:04:03.000Z&quot;
        }
      }
    },
    {
      &quot;doc&quot;: {
        &quot;_index&quot;: &quot;_index&quot;,
        &quot;_type&quot;: &quot;_doc&quot;,
        &quot;_id&quot;: &quot;_id&quot;,
        &quot;_source&quot;: {
          &quot;my-keyword-field&quot;: &quot;bar&quot;
        },
        &quot;_ingest&quot;: {
          &quot;timestamp&quot;: &quot;2099-03-07T11:04:04.000Z&quot;
        }
      }
    }
  ]
}
</code></pre>
<h2 id="add-a-pipeline-to-an-indexing-request"><a class="header" href="#add-a-pipeline-to-an-indexing-request">Add a pipeline to an indexing request</a></h2>
<p>Use the <code>pipeline</code> query parameter to apply a pipeline to documents in <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-index_.html">individual</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-bulk.html">bulk</a> indexing requests.</p>
<pre><code class="language-console">POST my-data-stream/_doc?pipeline=my-pipeline
{
  &quot;@timestamp&quot;: &quot;2099-03-07T11:04:05.000Z&quot;,
  &quot;my-keyword-field&quot;: &quot;foo&quot;
}

PUT my-data-stream/_bulk?pipeline=my-pipeline
{ &quot;create&quot;:{ } }
{ &quot;@timestamp&quot;: &quot;2099-03-07T11:04:06.000Z&quot;, &quot;my-keyword-field&quot;: &quot;foo&quot; }
{ &quot;create&quot;:{ } }
{ &quot;@timestamp&quot;: &quot;2099-03-07T11:04:07.000Z&quot;, &quot;my-keyword-field&quot;: &quot;bar&quot; }
</code></pre>
<p>You can also use the <code>pipeline</code> parameter with the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-update-by-query.html">update by query</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/docs-reindex.html">reindex</a> APIs.</p>
<pre><code class="language-console">POST my-data-stream/_update_by_query?pipeline=my-pipeline

POST _reindex
{
  &quot;source&quot;: {
    &quot;index&quot;: &quot;my-data-stream&quot;
  },
  &quot;dest&quot;: {
    &quot;index&quot;: &quot;my-new-data-stream&quot;,
    &quot;op_type&quot;: &quot;create&quot;,
    &quot;pipeline&quot;: &quot;my-pipeline&quot;
  }
}
</code></pre>
<h2 id="set-a-default-pipeline"><a class="header" href="#set-a-default-pipeline">Set a default pipeline</a></h2>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/index-modules.html#index-default-pipeline"><code>index.default_pipeline</code></a> index setting to set a default pipeline. Elasticsearch applies this pipeline to indexing requests if no <code>pipeline</code> parameter is specified.</p>
<h2 id="set-a-final-pipeline"><a class="header" href="#set-a-final-pipeline">Set a final pipeline</a></h2>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/index-modules.html#index-final-pipeline"><code>index.final_pipeline</code></a> index setting to set a final pipeline. Elasticsearch applies this pipeline after the request or default pipeline, even if neither is specified.</p>
<h2 id="pipelines-for-beats"><a class="header" href="#pipelines-for-beats">Pipelines for Beats</a></h2>
<p>To add an ingest pipeline to an Elastic Beat, specify the <code>pipeline</code> parameter under <code>output.elasticsearch</code> in <code>&lt;BEAT_NAME&gt;.yml</code>. For example, for Filebeat, you’d specify <code>pipeline</code> in <code>filebeat.yml</code>.</p>
<p>若要在Elastic Beat中添加一个Elastic pipeline，请在 &lt;BEAT_NAME&gt;.yml中指定output.elasticsearch下的pipeline参数。例如，对于Filebeat，您可以在filebeat.yml中指定管道。</p>
<pre><code class="language-yaml">output.elasticsearch:
  hosts: [&quot;localhost:9200&quot;]
  pipeline: my-pipeline
</code></pre>
<h2 id="access-source-fields-in-a-processor"><a class="header" href="#access-source-fields-in-a-processor">Access source fields in a processor</a></h2>
<p>Processors have read and write access to an incoming document’s source fields. To access a field key in a processor, use its field name. The following <code>set</code> processor accesses <code>my-long-field</code>.</p>
<p>处理器具有对传入文档的源字段的读写权限。要访问处理器中的 field key，请使用其字段名称。下面的set处理器访问my-long-field。</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;set&quot;: {
        &quot;field&quot;: &quot;my-long-field&quot;,
        &quot;value&quot;: 10
      }
    }
  ]
}
</code></pre>
<p>You can also prepend the <code>_source</code> prefix.</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;set&quot;: {
        &quot;field&quot;: &quot;_source.my-long-field&quot;,
        &quot;value&quot;: 10
      }
    }
  ]
}
</code></pre>
<p>使用点表示法访问对象字段。</p>
<p>注意：</p>
<p>If your document contains flattened objects, use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/dot-expand-processor.html"><code>dot_expander</code></a> processor to expand them first. Other ingest processors cannot access flattened objects.</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;dot_expander&quot;: {
        &quot;description&quot;: &quot;Expand 'my-object-field.my-property'&quot;,
        &quot;field&quot;: &quot;my-object-field.my-property&quot;
      }
    },
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;Set 'my-object-field.my-property' to 10&quot;,
        &quot;field&quot;: &quot;my-object-field.my-property&quot;,
        &quot;value&quot;: 10
      }
    }
  ]
}
</code></pre>
<p>Several processor parameters support <a href="https://mustache.github.io/">Mustache</a> template snippets.</p>
<p>支持大胡子模板引擎</p>
<p>To access field values in a template snippet, enclose the field name in triple curly brackets:<code>{{{field-name}}}</code>. You can use template snippets to dynamically set field names.</p>
<h2 id="access-metadata-fields-in-a-processor"><a class="header" href="#access-metadata-fields-in-a-processor">Access metadata fields in a processor</a></h2>
<p>Processors can access the following metadata fields by name:</p>
<ul>
<li><code>_index</code></li>
<li><code>_id</code></li>
<li><code>_routing</code></li>
<li><code>_dynamic_templates</code></li>
</ul>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;Set '_routing' to 'geoip.country_iso_code' value&quot;,
        &quot;field&quot;: &quot;_routing&quot;,
        &quot;value&quot;: &quot;{{{geoip.country_iso_code}}}&quot;
      }
    }
  ]
}
</code></pre>
<p>使用大胡子模板片段访问元数据字段值。例如 {{{_routing }}} 检索文档的路由值。</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;Use geo_point dynamic template for address field&quot;,
        &quot;field&quot;: &quot;_dynamic_templates&quot;,
        &quot;value&quot;: {
          &quot;address&quot;: &quot;geo_point&quot;
        }
      }
    }
  ]
}
</code></pre>
<p>The set processor above tells ES to use the dynamic template named <code>geo_point</code> for the field <code>address</code> if this field is not defined in the mapping of the index yet. This processor overrides the dynamic template for the field <code>address</code> if already defined in the bulk request, but has no effect on other dynamic templates defined in the bulk request.</p>
<p>如果索引的映射中尚未定义此字段，则上面的set处理器会告诉ES将名为geo_point的动态模板用于字段地址。如果已在批量请求中定义，则此处理器将覆盖字段地址的动态模板，但对批量请求中定义的其他动态模板没有影响。</p>
<h2 id="handling-pipeline-failures"><a class="header" href="#handling-pipeline-failures">Handling pipeline failures</a></h2>
<p>管道的处理器按顺序运行。默认情况下，当这些处理器之一出现故障或遇到错误时，管道处理将停止。
要忽略处理器故障并运行管道的其余处理器，请将ignore_failure设置为true。</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;rename&quot;: {
        &quot;description&quot;: &quot;Rename 'provider' to 'cloud.provider'&quot;,
        &quot;field&quot;: &quot;provider&quot;,
        &quot;target_field&quot;: &quot;cloud.provider&quot;,
        &quot;ignore_failure&quot;: true
      }
    }
  ]
}
</code></pre>
<p>使用<em>on_failure</em>参数指定处理器故障后立即运行的处理器列表。如果指定了on_failure，则即使<em>on_failure</em>配置为空，Elasticsearch也会随后运行管道的其余处理器。</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;rename&quot;: {
        &quot;description&quot;: &quot;Rename 'provider' to 'cloud.provider'&quot;,
        &quot;field&quot;: &quot;provider&quot;,
        &quot;target_field&quot;: &quot;cloud.provider&quot;,
        &quot;on_failure&quot;: [
          {
            &quot;set&quot;: {
              &quot;description&quot;: &quot;Set 'error.message'&quot;,
              &quot;field&quot;: &quot;error.message&quot;,
              &quot;value&quot;: &quot;Field 'provider' does not exist. Cannot rename to 'cloud.provider'&quot;,
              &quot;override&quot;: false,
              &quot;on_failure&quot;: [
                {
                  &quot;set&quot;: {
                    &quot;description&quot;: &quot;Set 'error.message.multi'&quot;,
                    &quot;field&quot;: &quot;error.message.multi&quot;,
                    &quot;value&quot;: &quot;Document encountered multiple ingest errors&quot;,
                    &quot;override&quot;: true
                  }
                }
              ]
            }
          }
        ]
      }
    }
  ]
}
</code></pre>
<p>You can also specify <code>on_failure</code> for a pipeline. If a processor without an <code>on_failure</code> value fails, Elasticsearch uses this pipeline-level parameter as a fallback. Elasticsearch will not attempt to run the pipeline’s remaining processors.</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [ ... ],
  &quot;on_failure&quot;: [
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;Index document to 'failed-&lt;index&gt;'&quot;,
        &quot;field&quot;: &quot;_index&quot;,
        &quot;value&quot;: &quot;failed-{{{ _index }}}&quot;
      }
    }
  ]
}
</code></pre>
<p>Additional information about the pipeline failure may be available in the document metadata fields <code>on_failure_message</code>, <code>on_failure_processor_type</code>, <code>on_failure_processor_tag</code>, and <code>on_failure_pipeline</code>. These fields are accessible only from within an <code>on_failure</code> block.</p>
<p>以下示例使用元数据字段在文档中包含有关管道故障的信息。</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [ ... ],
  &quot;on_failure&quot;: [
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;Record error information&quot;,
        &quot;field&quot;: &quot;error_information&quot;,
        &quot;value&quot;: &quot;Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}&quot;
      }
    }
  ]
}
</code></pre>
<h2 id="conditionally-run-a-processor"><a class="header" href="#conditionally-run-a-processor">Conditionally run a processor</a></h2>
<p>Each processor supports an optional <code>if</code> condition, written as a <a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.16/painless-guide.html">Painless script</a>. If provided, the processor only runs when the <code>if</code> condition is <code>true</code>.</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;drop&quot;: {
        &quot;description&quot;: &quot;Drop documents with 'network.name' of 'Guest'&quot;,
        &quot;if&quot;: &quot;ctx?.network?.name == 'Guest'&quot;
      }
    }
  ]
}
</code></pre>
<p>If the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.16/circuit-breaker.html#script-painless-regex-enabled"><code>script.painless.regex.enabled</code></a> cluster setting is enabled, you can use regular expressions in your <code>if</code> condition scripts. For supported syntax, see <a href="https://www.elastic.co/guide/en/elasticsearch/painless/7.16/painless-regexes.html">Painless regular expressions</a>.</p>
<p>If possible, avoid using regular expressions. Expensive regular expressions can slow indexing speeds.</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;set&quot;: {
        &quot;description&quot;: &quot;If 'url.scheme' is 'http', set 'url.insecure' to true&quot;,
        &quot;if&quot;: &quot;ctx.url?.scheme =~ /^http[^s]/&quot;,
        &quot;field&quot;: &quot;url.insecure&quot;,
        &quot;value&quot;: true
      }
    }
  ]
}
</code></pre>
<p>您必须将if条件指定为单行上的有效JSON。但是，您可以使用Kibana控制台的三引号语法来编写和调试更大的脚本。</p>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;drop&quot;: {
        &quot;description&quot;: &quot;Drop documents that don't contain 'prod' tag&quot;,
        &quot;if&quot;: &quot;&quot;&quot;
            Collection tags = ctx.tags;
            if(tags != null){
              for (String tag : tags) {
                if (tag.toLowerCase().contains('prod')) {
                  return false;
                }
              }
            }
            return true;
        &quot;&quot;&quot;
      }
    }
  ]
}
</code></pre>
<p>You can also specify a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/modules-scripting-stored-scripts.html">stored script</a> as the <code>if</code> condition.</p>
<pre><code class="language-console">PUT _scripts/my-prod-tag-script
{
  &quot;script&quot;: {
    &quot;lang&quot;: &quot;painless&quot;,
    &quot;source&quot;: &quot;&quot;&quot;
      Collection tags = ctx.tags;
      if(tags != null){
        for (String tag : tags) {
          if (tag.toLowerCase().contains('prod')) {
            return false;
          }
        }
      }
      return true;
    &quot;&quot;&quot;
  }
}

PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;drop&quot;: {
        &quot;description&quot;: &quot;Drop documents that don't contain 'prod' tag&quot;,
        &quot;if&quot;: { &quot;id&quot;: &quot;my-prod-tag-script&quot; }
      }
    }
  ]
}
</code></pre>
<p>传入文档通常包含对象字段。如果处理器脚本试图访问父对象不存在的字段</p>
<p>如果处理器脚本尝试访问父对象不存在的字段，则Elasticsearch将返回NullPointerException。</p>
<p>要避免这些异常，请使用null safe运算符，例如 <code>?.</code>，并将脚本编写为null safe。</p>
<pre><code> `ctx.network?.name.equalsIgnoreCase('Guest')` is not null safe. `ctx.network?.name` can return null. 

Rewrite the script as `'Guest'.equalsIgnoreCase(ctx.network?.name)`, which is null safe because `Guest` is always non-null.

</code></pre>
<pre><code class="language-console">PUT _ingest/pipeline/my-pipeline
{
  &quot;processors&quot;: [
    {
      &quot;drop&quot;: {
        &quot;description&quot;: &quot;Drop documents that contain 'network.name' of 'Guest'&quot;,
        &quot;if&quot;: &quot;ctx.network?.name != null &amp;&amp; ctx.network.name.contains('Guest')&quot;
      }
    }
  ]
}
</code></pre>
<h2 id="conditionally-apply-pipelines"><a class="header" href="#conditionally-apply-pipelines">Conditionally apply pipelines</a></h2>
<p>将if条件与管道处理器结合使用，以根据您的条件将其他管道应用于文档。您可以将此管道用作用于配置多个数据流或索引的索引模板中的默认管道。</p>
<pre><code class="language-console">PUT _ingest/pipeline/one-pipeline-to-rule-them-all
{
  &quot;processors&quot;: [
    {
      &quot;pipeline&quot;: {
        &quot;description&quot;: &quot;If 'service.name' is 'apache_httpd', use 'httpd_pipeline'&quot;,
        &quot;if&quot;: &quot;ctx.service?.name == 'apache_httpd'&quot;,
        &quot;name&quot;: &quot;httpd_pipeline&quot;
      }
    },
    {
      &quot;pipeline&quot;: {
        &quot;description&quot;: &quot;If 'service.name' is 'syslog', use 'syslog_pipeline'&quot;,
        &quot;if&quot;: &quot;ctx.service?.name == 'syslog'&quot;,
        &quot;name&quot;: &quot;syslog_pipeline&quot;
      }
    },
    {
      &quot;fail&quot;: {
        &quot;description&quot;: &quot;If 'service.name' is not 'apache_httpd' or 'syslog', return a failure message&quot;,
        &quot;if&quot;: &quot;ctx.service?.name != 'apache_httpd' &amp;&amp; ctx.service?.name != 'syslog'&quot;,
        &quot;message&quot;: &quot;This pipeline requires service.name to be either `syslog` or `apache_httpd`&quot;
      }
    }
  ]
}
</code></pre>
<h3 id="get-pipeline-usage-statistics"><a class="header" href="#get-pipeline-usage-statistics">Get pipeline usage statistics</a></h3>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/cluster-nodes-stats.html">node stats</a> API to get global and per-pipeline ingest statistics. </p>
<p>Use these stats to determine which pipelines run most frequently or spend the most time processing.</p>
<pre><code class="language-console">GET _nodes/stats/ingest?filter_path=nodes.*.ingest
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="pipelineexample"><a class="header" href="#pipelineexample">PipeLineExample</a></h2>
<p>In this example tutorial, you’ll use an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html">ingest pipeline</a> to parse server logs in the <a href="https://en.wikipedia.org/wiki/Common_Log_Format">Common Log Format</a> before indexing. </p>
<p>The logs you want to parse look similar to this:</p>
<pre><code class="language-log">212.87.37.154 - - [30/May/2099:16:21:15 +0000] \&quot;GET /favicon.ico HTTP/1.1\&quot;
200 3638 \&quot;-\&quot; \&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\&quot;
</code></pre>
<p>这些日志包含时间戳、ip地址和用户代理。您希望在Elasticsearch中赋予这三个项目自己的字段，以实现更快的搜索和可视化。您还想知道请求是从哪里来的。</p>
<ol>
<li>
<p>In Kibana, open the main menu and click <strong>Stack Management</strong> &gt; <strong>Ingest Pipelines</strong>.</p>
</li>
<li>
<p>Click <strong>Create pipeline</strong>.</p>
</li>
<li>
<p>Provide a name and description for the pipeline.</p>
</li>
<li>
<p>Add a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/grok-processor.html">grok processor</a> to parse the log message:</p>
<ol>
<li>
<p>Click <strong>Add a processor</strong> and select the <strong>Grok</strong> processor type.</p>
</li>
<li>
<p>Set <strong>Field</strong> to <code>message</code> and <strong>Patterns</strong> to the following <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/grok-basics.html">grok pattern</a>:</p>
</li>
<li>
<pre><code class="language-grok">%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\[%{HTTPDATE:@timestamp}\\] \&quot;%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\&quot; %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}
</code></pre>
</li>
<li>
<p>Click <strong>Add</strong> to save the processor.</p>
</li>
<li>
<p>Set the processor description to <code>Extract fields from 'message'</code>.</p>
</li>
</ol>
</li>
<li>
<p>Add processors for the timestamp, IP address, and user agent fields. Configure the processors as follows:</p>
</li>
</ol>
<table><thead><tr><th>Processor type</th><th>Field</th><th>Additional options</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/date-processor.html"><strong>Date</strong></a></td><td><code>@timestamp</code></td><td><strong>Formats</strong>: <code>dd/MMM/yyyy:HH:mm:ss Z</code></td><td><code>Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'</code></td></tr>
<tr><td><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/geoip-processor.html"><strong>GeoIP</strong></a></td><td><code>source.ip</code></td><td><strong>Target field</strong>: <code>source.geo</code></td><td><code>Add 'source.geo' GeoIP data for 'source.ip'</code></td></tr>
<tr><td><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/user-agent-processor.html"><strong>User agent</strong></a></td><td><code>user_agent</code></td><td></td><td><code>Extract fields from 'user_agent'</code></td></tr>
</tbody></table>
<p>Your form should look similar to this:</p>
<p>The four processors will run sequentially:
Grok &gt; Date &gt; GeoIP &gt; User agent
You can reorder processors using the arrow icons.</p>
<p>Alternatively, you can click the <strong>Import processors</strong> link and define the processors as JSON:</p>
<pre><code class="language-js">{
  &quot;processors&quot;: [
    {
      &quot;grok&quot;: {
        &quot;description&quot;: &quot;Extract fields from 'message'&quot;,
        &quot;field&quot;: &quot;message&quot;,
        &quot;patterns&quot;: [&quot;%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\[%{HTTPDATE:@timestamp}\\] \&quot;%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\&quot; %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}&quot;]
      }
    },
    {
      &quot;date&quot;: {
        &quot;description&quot;: &quot;Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'&quot;,
        &quot;field&quot;: &quot;@timestamp&quot;,
        &quot;formats&quot;: [ &quot;dd/MMM/yyyy:HH:mm:ss Z&quot; ]
      }
    },
    {
      &quot;geoip&quot;: {
        &quot;description&quot;: &quot;Add 'source.geo' GeoIP data for 'source.ip'&quot;,
        &quot;field&quot;: &quot;source.ip&quot;,
        &quot;target_field&quot;: &quot;source.geo&quot;
      }
    },
    {
      &quot;user_agent&quot;: {
        &quot;description&quot;: &quot;Extract fields from 'user_agent'&quot;,
        &quot;field&quot;: &quot;user_agent&quot;
      }
    }
  ]

}
</code></pre>
<ol start="6">
<li>To test the pipeline, click <strong>Add documents</strong>.</li>
<li>In the <strong>Documents</strong> tab, provide a sample document for testing:</li>
</ol>
<pre><code class="language-js">[
  {
    &quot;_source&quot;: {
      &quot;message&quot;: &quot;212.87.37.154 - - [05/May/2099:16:21:15 +0000] \&quot;GET /favicon.ico HTTP/1.1\&quot; 200 3638 \&quot;-\&quot; \&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\&quot;&quot;
    }
  }
]
</code></pre>
<ol start="8">
<li>Click <strong>Run the pipeline</strong> and verify the pipeline worked as expected.</li>
<li>If everything looks correct, close the panel, and then click <strong>Create pipeline</strong>.
<ol>
<li>You’re now ready to index the logs data to a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/data-streams.html">data stream</a>.</li>
</ol>
</li>
<li>Create an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-templates.html">index template</a> with <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/set-up-a-data-stream.html#create-index-template">data stream enabled</a>.</li>
</ol>
<pre><code class="language-console">PUT _index_template/my-data-stream-template
{
  &quot;index_patterns&quot;: [ &quot;my-data-stream*&quot; ],
  &quot;data_stream&quot;: { },
  &quot;priority&quot;: 500
}
</code></pre>
<ol start="11">
<li>Index a document with the pipeline you created.</li>
</ol>
<pre><code class="language-console">POST my-data-stream/_doc?pipeline=my-pipeline
{
  &quot;message&quot;: &quot;212.87.37.154 - - [05/May/2099:16:21:15 +0000] \&quot;GET /favicon.ico HTTP/1.1\&quot; 200 3638 \&quot;-\&quot; \&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\&quot;&quot;
}
</code></pre>
<ol start="12">
<li>To verify, search the data stream to retrieve the document. The following search uses <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/common-options.html#common-options-response-filtering"><code>filter_path</code></a> to return only the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/mapping-source-field.html">document source</a>.</li>
</ol>
<pre><code class="language-console">GET my-data-stream/_search?filter_path=hits.hits._source
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="enrich-your-data"><a class="header" href="#enrich-your-data">Enrich your data</a></h2>
<p>You can use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-processor.html">enrich processor</a> to add data from your existing indices to incoming documents during ingest.</p>
<p>For example, you can use the enrich processor to:</p>
<ul>
<li>根据已知的ip地址识别web服务或供应商</li>
<li>根据产品id向零售订单添加产品信息</li>
<li>根据电子邮件地址补充联系信息</li>
<li>根据用户坐标添加邮政编码</li>
</ul>
<h3 id="how-the-enrich-processor-works"><a class="header" href="#how-the-enrich-processor-works">How the enrich processor works</a></h3>
<p>大多数处理器都是自包含的，并且仅更改传入文档中的现有数据。</p>
<p>enrich处理器将新数据添加到传入文档中，并且需要一些特殊组件:</p>
<p><strong>enrich policy</strong></p>
<p>一组配置选项，用于将正确的enrich数据添加到正确的传入文档中。</p>
<p>An enrich policy contains:</p>
<ul>
<li>A list of one or more <em>source indices</em> which store enrich data as documents</li>
<li>The <em>policy type</em> which determines how the processor matches the enrich data to incoming documents</li>
<li>A <em>match field</em> from the source indices used to match incoming documents</li>
<li><em>Enrich fields</em> containing enrich data from the source indices you want to add to incoming documents</li>
</ul>
<p>在将其与enrich处理器一起使用之前，必须执行enrich策略。执行时，enrich策略使用策略的源索引中的enrich数据来创建称为 “enrich索引” 的简化系统索引。处理器使用此索引来匹配和丰富传入的文档。</p>
<p><strong>source index</strong></p>
<p>An index which stores enrich data you’d like to add to incoming documents. You can create and manage these indices just like a regular Elasticsearch index. You can use multiple source indices in an enrich policy. You also can use the same source index in multiple enrich policies.</p>
<p>一个索引，用于存储要添加到传入文档中的enrich data。您可以像常规Elasticsearch索引一样创建和管理这些索引。您可以在enrich策略中使用多个源索引。您还可以在多个enrich策略中使用相同的源索引。</p>
<p><strong>enrich index</strong></p>
<p>与特定的enrich策略相关的特殊系统索引。将传入文档与源索引中的文档直接匹配可能会很慢且资源密集。为了加快速度，enrich处理器使用了enrich索引。Enrich索引包含来自源索引的enrich数据，但具有一些特殊属性来帮助简化它们: 它们是系统索引，这意味着它们由Elasticsearch内部管理，仅适用于enrich处理器。它们总是以<em>enrich-</em> 开始。它们是只读的，这意味着你不能直接更改它们。它们是强制合并的（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-forcemerge.html">force merged</a> ），以便快速检索。</p>
<h2 id="set-up-an-enrich-processor"><a class="header" href="#set-up-an-enrich-processor">Set up an enrich processor</a></h2>
<p>To set up an enrich processor, follow these steps:</p>
<ol>
<li>Check the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#enrich-prereqs">prerequisites</a>.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#create-enrich-source-index">Add enrich data</a>.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#create-enrich-policy">Create an enrich policy</a>.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#execute-enrich-policy">Execute the enrich policy</a>.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#add-enrich-processor">Add an enrich processor to an ingest pipeline</a>.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#ingest-enrich-docs">Ingest and enrich documents</a>.</li>
</ol>
<p>Once you have an enrich processor set up, you can <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#update-enrich-data">update your enrich data</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#update-enrich-policies">update your enrich policies</a>.</p>
<p>enrich处理器执行多个操作，可能会影响您的 ingest pipeline的速度。</p>
<p>我们强烈建议在将enrich处理器部署到生产中之前对其进行测试和基准测试。</p>
<p>我们不建议使用enrich处理器来附加实时数据。enrich处理器最适合不经常更改的参考数据。</p>
<h3 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h3>
<p>If you use Elasticsearch security features, you must have:</p>
<ul>
<li><code>read</code> index privileges for any indices used</li>
<li>The <code>enrich_user</code> <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/built-in-roles.html">built-in role</a></li>
</ul>
<h3 id="add-enrich-data"><a class="header" href="#add-enrich-data">Add enrich data</a></h3>
<p>首先，将文档添加到一个或多个源索引。这些文档应包含您最终要添加到传入文档中的enrich数据。</p>
<p>You can manage source indices just like regular Elasticsearch indices using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/docs.html">document</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices.html">index</a> APIs.</p>
<p>You also can set up <a href="https://www.elastic.co/guide/en/beats/libbeat/7.17/getting-started.html">Beats</a>, such as a <a href="https://www.elastic.co/guide/en/beats/filebeat/7.17/filebeat-installation-configuration.html">Filebeat</a>, to automatically send and index documents to your source indices. See <a href="https://www.elastic.co/guide/en/beats/libbeat/7.17/getting-started.html">Getting started with Beats</a>.</p>
<h3 id="create-an-enrich-policy"><a class="header" href="#create-an-enrich-policy">Create an enrich policy</a></h3>
<p>After adding enrich data to your source indices, use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/put-enrich-policy-api.html">create enrich policy API</a> to create an enrich policy.</p>
<p>Once created, you can’t update or change an enrich policy. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-setup.html#update-enrich-policies">Update an enrich policy</a>.</p>
<h3 id="execute-the-enrich-policy"><a class="header" href="#execute-the-enrich-policy">Execute the enrich policy</a></h3>
<p>Once the enrich policy is created, you can execute it using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/execute-enrich-policy-api.html">execute enrich policy API</a> to create an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest-enriching-data.html#enrich-index">enrich index</a>.</p>
<p>The <em>enrich index</em> contains documents from the policy’s source indices. Enrich indices always begin with <code>.enrich-*</code>, are read-only, and are <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-forcemerge.html">force merged</a>.</p>
<p>Enrich indices should be used by the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-processor.html">enrich processor</a> only. Avoid using enrich indices for other purposes.</p>
<h3 id="add-an-enrich-processor-to-an-ingest-pipeline"><a class="header" href="#add-an-enrich-processor-to-an-ingest-pipeline">Add an enrich processor to an ingest pipeline</a></h3>
<p>一旦您有了源索引、enrich策略和相关的enrich索引，您就可以为您的策略设置包含一个enrich处理器的ingest pipeline。</p>
<p>Define an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-processor.html">enrich processor</a> and add it to an ingest pipeline using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/put-pipeline-api.html">create or update pipeline API</a>.</p>
<p>When defining the enrich processor, you must include at least the following:</p>
<p>定义enrich处理器时，必须至少包括以下内容:</p>
<ul>
<li>The enrich policy to use.</li>
<li>The field used to match incoming documents to the documents in your enrich index.</li>
<li>用于将传入文档与enrich索引中的文档匹配的字段。</li>
<li>要添加到传入文档的目标字段。此目标字段包含在您的丰富策略中指定的匹配字段和丰富字段。</li>
</ul>
<p>You also can use the <code>max_matches</code> option to set the number of enrich documents an incoming document can match. If set to the default of <code>1</code>, data is added to an incoming document’s target field as a JSON object. Otherwise, the data is added as an array.</p>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-processor.html">Enrich</a> for a full list of configuration options，You also can add other <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/processors.html">processors</a> to your ingest pipeline.</p>
<h3 id="update-an-enrich-index"><a class="header" href="#update-an-enrich-index">Update an enrich index</a></h3>
<p>Once created, you cannot update or index documents to an enrich index. Instead, update your source indices and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/execute-enrich-policy-api.html">execute</a> the enrich policy again. This creates a new enrich index from your updated source indices. The previous enrich index will deleted with a delayed maintenance job. By default this is done every 15 minutes.</p>
<p>创建后，您无法将文档更新或索引为enrich索引。相反，更新您的源索引并再次执行enrich策略。这将从更新的源索引中创建一个新的丰富索引。以前的enrich索引将随着延迟维护作业而删除。默认情况下，每15分钟完成一次。</p>
<p>If wanted, you can <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/docs-reindex.html">reindex</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/docs-update-by-query.html">update</a> any already ingested documents using your ingest pipeline.</p>
<h3 id="update-an-enrich-policy"><a class="header" href="#update-an-enrich-policy">Update an enrich policy</a></h3>
<p>Once created, you can’t update or change an enrich policy. Instead, you can:</p>
<ol>
<li>Create and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/execute-enrich-policy-api.html">execute</a> a new enrich policy.</li>
<li>Replace the previous enrich policy with the new enrich policy in any in-use enrich processors.</li>
<li>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/delete-enrich-policy-api.html">delete enrich policy</a> API to delete the previous enrich policy.</li>
</ol>
<h3 id="enrich-components"><a class="header" href="#enrich-components">Enrich components</a></h3>
<p>The enrich coordinator is a component that manages and performs the searches required to enrich documents on each ingest node. It combines searches from all enrich processors in all pipelines into bulk <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/search-multi-search.html">multi-searches</a>.</p>
<p>The enrich policy executor is a component that manages the executions of all enrich policies. When an enrich policy is executed, this component creates a new enrich index and removes the previous enrich index. The enrich policy executions are managed from the elected master node. The execution of these policies occurs on a different node.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="前言-3"><a class="header" href="#前言-3">前言</a></h2>
<p>enRich Data 本质上是对数据进行处理后吐出新数据、并且依赖第三方数据</p>
<h2 id="example-enrich-your-data-based-on-geolocation"><a class="header" href="#example-enrich-your-data-based-on-geolocation">Example: Enrich your data based on geolocation</a></h2>
<p><code>geo_match</code> <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest-enriching-data.html#enrich-policy">enrich policies</a> match enrich data to incoming documents based on a geographic location, using a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-geo-shape-query.html"><code>geo_shape</code> query</a>.</p>
<p>The following example creates a <code>geo_match</code> enrich policy that adds postal codes to incoming documents based on a set of coordinates. It then adds the <code>geo_match</code> enrich policy to a processor in an ingest pipeline.</p>
<p>以下示例创建<em>geo_match</em>  <em>enrich</em> 策略，该策略基于一组坐标将邮政编码添加到传入文档中。然后，它将<em>geo_match</em> <em>enrich</em>策略添加到摄取管道中的处理器。</p>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-create-index.html">create index API</a> to create a source index containing at least one <code>geo_shape</code> field.</p>
<pre><code class="language-console">PUT /postal_codes
{
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;location&quot;: {
        &quot;type&quot;: &quot;geo_shape&quot;
      },
      &quot;postal_code&quot;: {
        &quot;type&quot;: &quot;keyword&quot;
      }
    }
  }
}
</code></pre>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/docs-index_.html">index API</a> to index enrich data to this source index.</p>
<pre><code class="language-console">PUT /postal_codes/_doc/1?refresh=wait_for
{
  &quot;location&quot;: {
    &quot;type&quot;: &quot;envelope&quot;,
    &quot;coordinates&quot;: [ [ 13.0, 53.0 ], [ 14.0, 52.0 ] ]
  },
  &quot;postal_code&quot;: &quot;96598&quot;
}
</code></pre>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/put-enrich-policy-api.html">create enrich policy API</a> to create an enrich policy with the <code>geo_match</code> policy type. This policy must include:</p>
<ul>
<li>One or more source indices</li>
<li>A <code>match_field</code>, the <code>geo_shape</code> field from the source indices used to match incoming documents</li>
<li>Enrich fields from the source indices you’d like to append to incoming documents</li>
</ul>
<pre><code class="language-console">PUT /_enrich/policy/postal_policy
{
  &quot;geo_match&quot;: {
    &quot;indices&quot;: &quot;postal_codes&quot;,
    &quot;match_field&quot;: &quot;location&quot;,
    &quot;enrich_fields&quot;: [ &quot;location&quot;, &quot;postal_code&quot; ]
  }
}
</code></pre>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/execute-enrich-policy-api.html">execute enrich policy API</a> to create an enrich index for the policy.</p>
<pre><code class="language-console">POST /_enrich/policy/postal_policy/_execute
</code></pre>
<p>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/put-pipeline-api.html">create or update pipeline API</a> to create an ingest pipeline. In the pipeline, add an <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/enrich-processor.html">enrich processor</a> that includes:</p>
<ul>
<li>Your enrich policy.</li>
<li>The <code>field</code> of incoming documents used to match the geoshape of documents from the enrich index.</li>
<li>The <code>target_field</code> used to store appended enrich data for incoming documents. This field contains the <code>match_field</code> and <code>enrich_fields</code> specified in your enrich policy.</li>
<li>The <code>shape_relation</code>, which indicates how the processor matches geoshapes in incoming documents to geoshapes in documents from the enrich index. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-geo-shape-query.html#_spatial_relations">Spatial Relations</a> for valid options and more information.</li>
</ul>
<pre><code class="language-console">PUT /_ingest/pipeline/postal_lookup
{
  &quot;processors&quot;: [
    {
      &quot;enrich&quot;: {
        &quot;description&quot;: &quot;Add 'geo_data' based on 'geo_location'&quot;,
        &quot;policy_name&quot;: &quot;postal_policy&quot;,
        &quot;field&quot;: &quot;geo_location&quot;,
        &quot;target_field&quot;: &quot;geo_data&quot;,
        &quot;shape_relation&quot;: &quot;INTERSECTS&quot;
      }
    }
  ]
}
</code></pre>
<h2 id="example-enrich-your-data-based-on-exact-values"><a class="header" href="#example-enrich-your-data-based-on-exact-values">Example: Enrich your data based on exact values</a></h2>
<p><strong>创建enrich 源索引</strong></p>
<pre><code class="language-console">PUT /users/_doc/1?refresh=wait_for
{
  &quot;email&quot;: &quot;mardy.brown@asciidocsmith.com&quot;,
  &quot;first_name&quot;: &quot;Mardy&quot;,
  &quot;last_name&quot;: &quot;Brown&quot;,
  &quot;city&quot;: &quot;New Orleans&quot;,
  &quot;county&quot;: &quot;Orleans&quot;,
  &quot;state&quot;: &quot;LA&quot;,
  &quot;zip&quot;: 70116,
  &quot;web&quot;: &quot;mardy.asciidocsmith.com&quot;
}
</code></pre>
<p><strong>创建enrich policy</strong></p>
<pre><code class="language-console">PUT /_enrich/policy/users-policy
{
  &quot;match&quot;: {
    &quot;indices&quot;: &quot;users&quot;,
    &quot;match_field&quot;: &quot;email&quot;,
    &quot;enrich_fields&quot;: [&quot;first_name&quot;, &quot;last_name&quot;, &quot;city&quot;, &quot;zip&quot;, &quot;state&quot;]
  }
}
</code></pre>
<p><strong>根据执行策略创建enrich索引</strong></p>
<pre><code class="language-console">POST /_enrich/policy/users-policy/_execute
</code></pre>
<p><strong>创建PIPELINE管道</strong></p>
<pre><code>PUT /_ingest/pipeline/user_lookup
{
  &quot;processors&quot; : [
    {
      &quot;enrich&quot; : {
        &quot;description&quot;: &quot;Add 'user' data based on 'email'&quot;,
        &quot;policy_name&quot;: &quot;users-policy&quot;,
        &quot;field&quot; : &quot;email&quot;,
        &quot;target_field&quot;: &quot;user&quot;,
        &quot;max_matches&quot;: &quot;1&quot;
      }
    }
  ]
}
</code></pre>
<p><strong>使用管道上传数据</strong></p>
<pre><code class="language-console">PUT /my-index-000001/_doc/my_id?pipeline=user_lookup
{
  &quot;email&quot;: &quot;mardy.brown@asciidocsmith.com&quot;
}
</code></pre>
<h2 id="example-enrich-your-data-by-matching-a-value-to-a-range"><a class="header" href="#example-enrich-your-data-by-matching-a-value-to-a-range">Example: Enrich your data by matching a value to a range</a></h2>
<p><strong>创建enrich 源索引</strong></p>
<pre><code class="language-console">PUT /networks
{
  &quot;mappings&quot;: {
    &quot;properties&quot;: {
      &quot;range&quot;: { &quot;type&quot;: &quot;ip_range&quot; },
      &quot;name&quot;: { &quot;type&quot;: &quot;keyword&quot; },
      &quot;department&quot;: { &quot;type&quot;: &quot;keyword&quot; }
    }
  }
}
PUT /networks/_doc/1?refresh=wait_for
{
  &quot;range&quot;: &quot;10.100.0.0/16&quot;,
  &quot;name&quot;: &quot;production&quot;,
  &quot;department&quot;: &quot;OPS&quot;
}
</code></pre>
<p><strong>创建enrich policy</strong></p>
<pre><code class="language-console">PUT /_enrich/policy/networks-policy
{
  &quot;range&quot;: {
    &quot;indices&quot;: &quot;networks&quot;,
    &quot;match_field&quot;: &quot;range&quot;,
    &quot;enrich_fields&quot;: [&quot;name&quot;, &quot;department&quot;]
  }
}
</code></pre>
<p><strong>根据执行策略创建enrich索引</strong></p>
<pre><code class="language-console">POST /_enrich/policy/networks-policy/_execute
</code></pre>
<p><strong>创建PIPELINE管道</strong></p>
<pre><code class="language-console">PUT /_ingest/pipeline/networks_lookup
{
  &quot;processors&quot; : [
    {
      &quot;enrich&quot; : {
        &quot;description&quot;: &quot;Add 'network' data based on 'ip'&quot;,
        &quot;policy_name&quot;: &quot;networks-policy&quot;,
        &quot;field&quot; : &quot;ip&quot;,
        &quot;target_field&quot;: &quot;network&quot;,
        &quot;max_matches&quot;: &quot;10&quot;
      }
    }
  ]
}
</code></pre>
<p><strong>使用管道上传数据</strong></p>
<pre><code class="language-console">PUT /my-index-000001/_doc/my_id?pipeline=networks_lookup
{
  &quot;ip&quot;: &quot;10.100.34.1&quot;
}
</code></pre>
<h2 id=""><a class="header" href="#"></a></h2>
<div style="break-before: page; page-break-before: always;"></div><h2 id="ingest-processor-reference"><a class="header" href="#ingest-processor-reference">Ingest processor reference</a></h2>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.17/processors.html</p>
<p>Elasticsearch includes several configurable processors. To get a list of available processors, use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/cluster-nodes-info.html">nodes info</a> API.</p>
<pre><code class="language-console">GET _nodes/ingest?filter_path=nodes.*.ingest.processors
</code></pre>
<h3 id="processor-plugins"><a class="header" href="#processor-plugins">Processor plugins</a></h3>
<p>You can install additional processors as <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.17/ingest.html">plugins</a>.</p>
<p>您必须在集群中的所有节点上安装任何插件处理器。否则，Elasticsearch将无法创建包含处理器的管道。</p>
<p>通过在elasticsearch.yml中设置plugin，将插件标记为必填项。如果未安装强制插件，则节点将无法启动。</p>
<pre><code class="language-yaml">plugin.mandatory: ingest-attachment
</code></pre>
<h2 id="append-processor"><a class="header" href="#append-processor">Append processor</a></h2>
<p>Appends one or more values to an existing array if the field already exists and it is an array. Converts a scalar to an array and appends one or more values to it if the field exists and it is a scalar. Creates an array containing the provided values if the field doesn’t exist. Accepts a single value or an array of values.</p>
<p>如果字段已经存在并且是数组，则将一个或多个值附加到现有数组。将标量转换为数组，如果字段存在并且是标量，则将一个或多个值附加到数组。如果字段不存在，则创建一个包含提供的值的数组。接受单个值或值数组。</p>
<p><strong>Table 3. Append Options</strong></p>
<table><thead><tr><th>Name</th><th>Required</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>field</code></td><td>yes</td><td>-</td><td>The field to be appended to. Supports <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#template-snippets">template snippets</a>.</td></tr>
<tr><td><code>value</code></td><td>yes</td><td>-</td><td>The value to be appended. Supports <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#template-snippets">template snippets</a>.</td></tr>
<tr><td><code>allow_duplicates</code></td><td>no</td><td>true</td><td>If <code>false</code>, the processor does not append values already present in the field.</td></tr>
<tr><td><code>media_type</code></td><td>no</td><td><code>application/json</code></td><td>The media type for encoding <code>value</code>. Applies only when <code>value</code> is a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#template-snippets">template snippet</a>. Must be one of <code>application/json</code>, <code>text/plain</code>, or <code>application/x-www-form-urlencoded</code>.</td></tr>
<tr><td><code>description</code></td><td>no</td><td>-</td><td>Description of the processor. Useful for describing the purpose of the processor or its configuration.</td></tr>
<tr><td><code>if</code></td><td>no</td><td>-</td><td>Conditionally execute the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#conditionally-run-processor">Conditionally run a processor</a>.</td></tr>
<tr><td><code>ignore_failure</code></td><td>no</td><td><code>false</code></td><td>Ignore failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>on_failure</code></td><td>no</td><td>-</td><td>Handle failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>tag</code></td><td>no</td><td>-</td><td>Identifier for the processor. Useful for debugging and metrics.</td></tr>
</tbody></table>
<pre><code class="language-js">{
  &quot;append&quot;: {
    &quot;field&quot;: &quot;tags&quot;,
    &quot;value&quot;: [&quot;production&quot;, &quot;{{{app}}}&quot;, &quot;{{{owner}}}&quot;]
  }
}
</code></pre>
<h2 id="bytes-processor"><a class="header" href="#bytes-processor">Bytes processor</a></h2>
<p>将人类可读字节值 (例如1kb) 转换为其以字节为单位的值 (例如1024)。如果该字段是字符串数组，则该数组的所有成员都将被转换。</p>
<p>支持的人类可读单位是 “b”，“kb”，“mb”，“gb”，“tb”，“pb” 大小写不敏感。如果该字段不是受支持的格式或结果值超过2 ^ 63，则会发生错误。</p>
<table><thead><tr><th>Name</th><th>Required</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>field</code></td><td>yes</td><td>-</td><td>The field to convert</td></tr>
<tr><td><code>target_field</code></td><td>no</td><td><code>field</code></td><td>The field to assign the converted value to, by default <code>field</code> is updated in-place</td></tr>
<tr><td><code>ignore_missing</code></td><td>no</td><td><code>false</code></td><td>If <code>true</code> and <code>field</code> does not exist or is <code>null</code>, the processor quietly exits without modifying the document</td></tr>
<tr><td><code>description</code></td><td>no</td><td>-</td><td>Description of the processor. Useful for describing the purpose of the processor or its configuration.</td></tr>
<tr><td><code>if</code></td><td>no</td><td>-</td><td>Conditionally execute the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#conditionally-run-processor">Conditionally run a processor</a>.</td></tr>
<tr><td><code>ignore_failure</code></td><td>no</td><td><code>false</code></td><td>Ignore failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>on_failure</code></td><td>no</td><td>-</td><td>Handle failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>tag</code></td><td>no</td><td>-</td><td>Identifier for the processor. Useful for debugging and metrics.</td></tr>
</tbody></table>
<h2 id="circle-processor"><a class="header" href="#circle-processor">Circle processor</a></h2>
<p>Converts circle definitions of shapes to regular polygons which approximate them.</p>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest-circle-processor.html">详见</a></p>
<h2 id="csv-processor"><a class="header" href="#csv-processor">CSV processor</a></h2>
<p>从文档中的单个文本字段中提取CSV行中的字段。CSV中的任何空字段都将被跳过。</p>
<table><thead><tr><th>Name</th><th>Required</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>field</code></td><td>yes</td><td>-</td><td>The field to extract data from</td></tr>
<tr><td><code>target_fields</code></td><td>yes</td><td>-</td><td>The array of fields to assign extracted values to</td></tr>
<tr><td><code>separator</code></td><td>no</td><td>,</td><td>Separator used in CSV, has to be single character string</td></tr>
<tr><td><code>quote</code></td><td>no</td><td>&quot;</td><td>Quote used in CSV, has to be single character string</td></tr>
<tr><td><code>ignore_missing</code></td><td>no</td><td><code>true</code></td><td>If <code>true</code> and <code>field</code> does not exist, the processor quietly exits without modifying the document</td></tr>
<tr><td><code>trim</code></td><td>no</td><td><code>false</code></td><td>Trim whitespaces in unquoted fields</td></tr>
<tr><td><code>empty_value</code></td><td>no</td><td>-</td><td>Value used to fill empty fields, empty fields will be skipped if this is not provided. Empty field is one with no value (2 consecutive separators) or empty quotes (<code>&quot;&quot;</code>)</td></tr>
<tr><td><code>description</code></td><td>no</td><td>-</td><td>Description of the processor. Useful for describing the purpose of the processor or its configuration.</td></tr>
<tr><td><code>if</code></td><td>no</td><td>-</td><td>Conditionally execute the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#conditionally-run-processor">Conditionally run a processor</a>.</td></tr>
<tr><td><code>ignore_failure</code></td><td>no</td><td><code>false</code></td><td>Ignore failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>on_failure</code></td><td>no</td><td>-</td><td>Handle failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>tag</code></td><td>no</td><td>-</td><td>Identifier for the processor. Useful for debugging and metrics.</td></tr>
</tbody></table>
<h2 id="grok-processor"><a class="header" href="#grok-processor">Grok processor</a></h2>
<p>从文档中的单个文本字段中提取结构化字段。您可以选择从中提取匹配字段的字段，以及您期望匹配的grok模式。</p>
<p>grok模式就像一个正则表达式，它支持可以重复使用的别名表达式。</p>
<p>该处理器包装有许多可重复使用的 <a href="https://github.com/elastic/elasticsearch/blob/7.17/libs/grok/src/main/resources/patterns">pattern</a> </p>
<p>If you need help building patterns to match your logs, </p>
<p>如果您需要帮助构建模式来匹配您的日志</p>
<p>you will find the <a href="https://www.elastic.co/guide/en/kibana/7.17/xpack-grokdebugger.html">Grok Debugger</a> tool quite useful! The <a href="https://grokconstructor.appspot.com/">Grok Constructor</a> is also a useful tool.</p>
<h3 id="using-the-grok-processor-in-a-pipeline"><a class="header" href="#using-the-grok-processor-in-a-pipeline">Using the Grok Processor in a Pipeline</a></h3>
<p><strong>Table 21. Grok Options</strong></p>
<table><thead><tr><th>Name</th><th>Required</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>field</code></td><td>yes</td><td>-</td><td>The field to use for grok expression parsing</td></tr>
<tr><td><code>patterns</code></td><td>yes</td><td>-</td><td>An ordered list of grok expression to match and extract named captures with. Returns on the first expression in the list that matches.</td></tr>
<tr><td><code>pattern_definitions</code></td><td>no</td><td>-</td><td>A map of pattern-name and pattern tuples defining custom patterns to be used by the current processor. Patterns matching existing names will override the pre-existing definition.</td></tr>
<tr><td><code>ecs_compatibility</code></td><td>no</td><td><code>disabled</code></td><td>Must be <code>disabled</code> or <code>v1</code>. If <code>v1</code>, the processor uses patterns with <a href="https://www.elastic.co/guide/en/ecs/1.12/ecs-field-reference.html">Elastic Common Schema (ECS)</a> field names.</td></tr>
<tr><td><code>trace_match</code></td><td>no</td><td>false</td><td>when true, <code>_ingest._grok_match_index</code> will be inserted into your matched document’s metadata with the index into the pattern found in <code>patterns</code> that matched.</td></tr>
<tr><td><code>ignore_missing</code></td><td>no</td><td>false</td><td>If <code>true</code> and <code>field</code> does not exist or is <code>null</code>, the processor quietly exits without modifying the document</td></tr>
<tr><td><code>description</code></td><td>no</td><td>-</td><td>Description of the processor. Useful for describing the purpose of the processor or its configuration.</td></tr>
<tr><td><code>if</code></td><td>no</td><td>-</td><td>Conditionally execute the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#conditionally-run-processor">Conditionally run a processor</a>.</td></tr>
<tr><td><code>ignore_failure</code></td><td>no</td><td><code>false</code></td><td>Ignore failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>on_failure</code></td><td>no</td><td>-</td><td>Handle failures for the processor. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ingest.html#handling-pipeline-failures">Handling pipeline failures</a>.</td></tr>
<tr><td><code>tag</code></td><td>no</td><td>-</td><td>Identifier for the processor. Useful for debugging and metrics.</td></tr>
</tbody></table>
<h3 id="custom-patterns"><a class="header" href="#custom-patterns">Custom Patterns</a></h3>
<p>The Grok processor comes pre-packaged with a base set of patterns. These patterns may not always have what you are looking for. Patterns have a very basic format. Each entry has a name and the pattern itself.</p>
<p>Grok处理器预先包装了一组基本模式。这些模式可能并不总是有你想要的。模式有一个非常基本的格式。每个条目都有一个名称和<em>pattern</em>。</p>
<p>您可以在<em>pattern_definitions</em>选项下将自己的模式添加到处理器定义中。下面是指定自定义模式定义的管道示例:</p>
<pre><code class="language-js">{
  &quot;description&quot; : &quot;...&quot;,
  &quot;processors&quot;: [
    {
      &quot;grok&quot;: {
        &quot;field&quot;: &quot;message&quot;,
        &quot;patterns&quot;: [&quot;my %{FAVORITE_DOG:dog} is colored %{RGB:color}&quot;],
        &quot;pattern_definitions&quot; : {
          &quot;FAVORITE_DOG&quot; : &quot;beagle&quot;,
          &quot;RGB&quot; : &quot;RED|GREEN|BLUE&quot;
        }
      }
    }
  ]
}
</code></pre>
<h3 id="providing-multiple-match-patterns"><a class="header" href="#providing-multiple-match-patterns">Providing Multiple Match Patterns</a></h3>
<p>有时，一种模式不足以捕获子段的潜在结构。假设我们要匹配所有包含您最喜欢的猫或狗的宠物品种的消息。实现此目的的一种方法是提供可以匹配的两种不同模式，而不是一种捕获相同或行为的真正复杂的表达。</p>
<p>Here is an example of such a configuration executed against the simulate API:</p>
<pre><code class="language-console">POST _ingest/pipeline/_simulate
{
  &quot;pipeline&quot;: {
  &quot;description&quot; : &quot;parse multiple patterns&quot;,
  &quot;processors&quot;: [
    {
      &quot;grok&quot;: {
        &quot;field&quot;: &quot;message&quot;,
        &quot;patterns&quot;: [&quot;%{FAVORITE_DOG:pet}&quot;, &quot;%{FAVORITE_CAT:pet}&quot;],
        &quot;pattern_definitions&quot; : {
          &quot;FAVORITE_DOG&quot; : &quot;beagle&quot;,
          &quot;FAVORITE_CAT&quot; : &quot;burmese&quot;
        }
      }
    }
  ]
},
&quot;docs&quot;:[
  {
    &quot;_source&quot;: {
      &quot;message&quot;: &quot;I love burmese cats!&quot;
    }
  }
  ]
}
</code></pre>
<p>两种模式都将为字段设置适当的匹配项，但是如果我们想跟踪哪个模式匹配并填充了我们的字段，该怎么办</p>
<p>We can do this with the <code>trace_match</code> parameter. Here is the output of that same pipeline, but with <code>&quot;trace_match&quot;: true</code> configured:</p>
<p>我们可以使用trace_match参数来执行此操作。这里是同一管道的输出，但与 <em>“trace_match”: true</em>配置:</p>
<pre><code class="language-console-result">{
  &quot;docs&quot;: [
    {
      &quot;doc&quot;: {
        &quot;_type&quot;: &quot;_doc&quot;,
        &quot;_index&quot;: &quot;_index&quot;,
        &quot;_id&quot;: &quot;_id&quot;,
        &quot;_source&quot;: {
          &quot;message&quot;: &quot;I love burmese cats!&quot;,
          &quot;pet&quot;: &quot;burmese&quot;
        },
        &quot;_ingest&quot;: {
          &quot;_grok_match_index&quot;: &quot;1&quot;,
          &quot;timestamp&quot;: &quot;2016-11-08T19:43:03.850+0000&quot;
        }
      }
    }
  ]
}
</code></pre>
<p>In the above response, you can see that the index of the pattern that matched was <code>&quot;1&quot;</code>.</p>
<p>This is to say that it was the second (index starts at zero) pattern in <code>patterns</code> to match.</p>
<p>这个跟踪元数据可以调试匹配的模式。此信息存储在ingest metadata  中，不会被索引。</p>
<h3 id="retrieving-patterns-from-rest-endpoint"><a class="header" href="#retrieving-patterns-from-rest-endpoint">Retrieving patterns from REST endpoint</a></h3>
<p>The Grok processor comes packaged with its own REST endpoint for retrieving the patterns included with the processor.</p>
<p><em>Grok processor</em> 随附了自己的REST端点，用于检索 处理器预定义的模式。</p>
<pre><code class="language-console">GET _ingest/processor/grok
</code></pre>
<p>The above request will return a response body containing a key-value representation of the built-in patterns dictionary.</p>
<pre><code class="language-js">{
  &quot;patterns&quot; : {
    &quot;BACULA_CAPACITY&quot; : &quot;%{INT}{1,3}(,%{INT}{3})*&quot;,
    &quot;PATH&quot; : &quot;(?:%{UNIXPATH}|%{WINPATH})&quot;,
    ...
}
</code></pre>
<p>By default, the API returns a list of legacy Grok patterns.</p>
<p>These legacy patterns predate the <a href="https://www.elastic.co/guide/en/ecs/1.12/ecs-field-reference.html">Elastic Common Schema (ECS)</a> and don’t use ECS field names. To return patterns that extract ECS field names, specify <code>v1</code> in the optional <code>ecs_compatibility</code> query parameter.</p>
<pre><code class="language-console">GET _ingest/processor/grok?ecs_compatibility=v1
</code></pre>
<p>By default, the API returns patterns in the order they are read from disk.</p>
<p>默认情况下，API按照从磁盘读取的顺序返回模式。这种排序顺序保留了相关模式的分组。例如，与解析Linux syslog行相关的所有模式都保存在一起。</p>
<p>您可以使用可选的boolean s查询参数来按键名对返回的模式进行排序。</p>
<pre><code class="language-console">GET _ingest/processor/grok?s
</code></pre>
<p>The API returns the following response.</p>
<pre><code class="language-js">{
  &quot;patterns&quot; : {
    &quot;BACULA_CAPACITY&quot; : &quot;%{INT}{1,3}(,%{INT}{3})*&quot;,
    &quot;BACULA_DEVICE&quot; : &quot;%{USER}&quot;,
    &quot;BACULA_DEVICEPATH&quot; : &quot;%{UNIXPATH}&quot;,
    ...
}
</code></pre>
<p>This can be useful to reference as the built-in patterns change across versions.</p>
<h3 id="grok-watchdog"><a class="header" href="#grok-watchdog">Grok watchdog</a></h3>
<p>执行时间太长的Grok表达式被中断，然后grok处理器出现异常失败。</p>
<p>grok处理器有一个看门狗线程，该线程确定grok表达式的求值时间过长，并由以下设置控制:</p>
<table><thead><tr><th>Name</th><th>Default</th><th>Description</th></tr></thead><tbody>
<tr><td><code>ingest.grok.watchdog.interval</code></td><td>1s</td><td>How often to check whether there are grok evaluations that take longer than the maximum allowed execution time.</td></tr>
<tr><td><code>ingest.grok.watchdog.max_execution_time</code></td><td>1s</td><td>The maximum allowed execution of a grok expression evaluation.</td></tr>
</tbody></table>
<h3 id="grok-debugging"><a class="header" href="#grok-debugging">Grok debugging</a></h3>
<p>建议使用 <a href="https://www.elastic.co/guide/en/kibana/7.17/xpack-grokdebugger.html">Grok Debugger</a> 来调试grok模式。从那里，您可以针对示例数据测试UI中的一个或多个模式，它使用与摄取节点处理器相同的引擎。</p>
<p>Additionally, it is recommended to enable debug logging for Grok so that any additional messages may also be seen in the Elasticsearch server log.</p>
<p>此外，建议为Grok启用调试日志记录，以便在Elasticsearch服务器日志中也可以看到任何其他消息。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="query-dsl"><a class="header" href="#query-dsl">Query DSL</a></h1>
<p>Elasticsearch提供了一个基于JSON的完整查询DSL (域特定语言) 来定义查询。将查询DSL视为查询的AST (抽象语法树)，由两种类型的子句组成:</p>
<h3 id="leaf-query-clauses"><a class="header" href="#leaf-query-clauses"><strong>Leaf query clauses</strong></a></h3>
<p>叶查询子句在特定字段中查找特定值，例如 <em>match</em>、<em>term</em> 、<em>range</em>。这些查询可以自己使用。</p>
<h3 id="compound-query-clauses"><a class="header" href="#compound-query-clauses"><strong>Compound query clauses</strong></a></h3>
<p>复合查询子句包装其他叶子或复合查询，用于以逻辑方式组合多个查询 例如：<em>bool</em>、<em>dis_max</em></p>
<p>或者 修改默认行为  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-constant-score-query.html"><code>constant_score</code></a> </p>
<p>Query clauses behave differently depending on whether they are used in <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html">query context or filter context</a>.</p>
<p>查询子句的行为 具体取决于它们是在查询上下文中还是在过滤器上下文中使用（ <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html">query context or filter context</a>.）</p>
<h3 id="allow-expensive-queries"><a class="header" href="#allow-expensive-queries"><strong>Allow expensive queries</strong></a></h3>
<p>某些类型的查询通常会由于其实现方式而执行慢，这会影响群集的稳定性。这些查询可以分类如下:</p>
<ul>
<li>
<p>需要进行线性扫描以识别匹配的查询: </p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-script-query.html"><code>script queries</code></a></li>
</ul>
</li>
<li>
<p>具有较高前期成本的查询:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-fuzzy-query.html"><code>fuzzy queries</code></a> (except on <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/keyword.html#wildcard-field-type"><code>wildcard</code></a> fields)</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-regexp-query.html"><code>regexp queries</code></a> (except on <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/keyword.html#wildcard-field-type"><code>wildcard</code></a> fields)</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-prefix-query.html"><code>prefix queries</code></a> (except on <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/keyword.html#wildcard-field-type"><code>wildcard</code></a> fields or those without <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/index-prefixes.html"><code>index_prefixes</code></a>)</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-wildcard-query.html"><code>wildcard queries</code></a> (except on <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/keyword.html#wildcard-field-type"><code>wildcard</code></a> fields)</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/keyword.html"><code>range queries&gt;&gt; on &lt; and [</code>keyword`</a> fields</li>
</ul>
</li>
<li>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/joining-queries.html"><code>Joining queries</code></a></p>
</li>
<li>
<p>Queries on <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/geo-shape.html#prefix-trees">deprecated geo shapes</a></p>
</li>
<li>
<p>每个文档成本可能很高的查询:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-script-score-query.html"><code>script score queries</code></a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-percolate-query.html"><code>percolate queries</code></a></li>
</ul>
</li>
</ul>
<p>可以通过将<em>search.allow_expensive_queries</em>设置的值设置为false (默认为true) 来防止此类查询的执行。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="query-and-filter-contextquery-and-filter-context"><a class="header" href="#query-and-filter-contextquery-and-filter-context">Query and filter contextQuery and filter context</a></h2>
<h3 id="relevance-scores"><a class="header" href="#relevance-scores">Relevance scores</a></h3>
<p>默认情况下，Elasticsearch按相关性得分对匹配的搜索结果进行排序，该得分衡量每个文档与查询的匹配程度。
相关性分数是一个正的浮点数，在搜索API的 <em>score元数据字段中返回。</em> 分数越高，文档越相关。虽然每种查询类型可以不同地计算相关性分数，但分数计算还取决于查询子句是在  查询还是过滤器  上下文中运行。</p>
<h3 id="query-context"><a class="header" href="#query-context">Query context</a></h3>
<p>在 Query context  中，查询子句回答以下问题: “此文档与该查询子句匹配的程度如何？” </p>
<p>除了确定文档是否匹配外，查询子句还计算 _score元数据字段中的相关性分数。</p>
<p>只要将 <strong>查询子句</strong>  传递给  查询 参数 (例如搜索API中( <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/search-search.html#request-body-search-query">search</a> API)的查询参数)，查询上下文就会生效。</p>
<h3 id="filter-context"><a class="header" href="#filter-context">Filter context</a></h3>
<p>在过滤器上下文中，查询子句回答以下问题: “此文档是否与此查询子句匹配？” </p>
<p>答案是简单的是或否，不会计算分数。过滤器上下文主要用于过滤结构化数据，例如</p>
<ul>
<li><em>Does this <code>timestamp</code> fall into the range 2015 to 2016?</em></li>
<li><em>Is the <code>status</code> field set to <code>&quot;published&quot;</code></em>?</li>
</ul>
<p>常用的过滤器将由Elasticsearch自动缓存，以加快性能。</p>
<p>每当将查询子句传递给 <code>filter</code>参数 (例如bool查询中的  <code>filter</code> or <code>must_not</code> ， <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-constant-score-query.html"><code>constant_score</code></a>  查询中的过滤器参数或 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/search-aggregations-bucket-filter-aggregation.html"><code>filter</code></a> aggregation ) 时，过滤器上下文就会生效。</p>
<h3 id="example-of-query-and-filter-contexts"><a class="header" href="#example-of-query-and-filter-contexts">Example of query and filter contexts</a></h3>
<p>Below is an example of query clauses being used in query and filter context in the <code>search</code> API. This query will match documents where all of the following conditions are met:</p>
<p>下面是在搜索API中的查询和过滤上下文中使用的查询子句的示例。此查询将匹配满足以下所有条件的文档: </p>
<ul>
<li>The <code>title</code> field contains the word <code>search</code>.</li>
<li>The <code>content</code> field contains the word <code>elasticsearch</code>.</li>
<li>The <code>status</code> field contains the exact word <code>published</code>.</li>
<li>The <code>publish_date</code> field contains a date from 1 Jan 2015 onwards.</li>
</ul>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: { //The query parameter indicates query context.
    &quot;bool&quot;: { //The `bool` and two `match` clauses are used in query context, which means that they are used to score how well each document matches.
      &quot;must&quot;: [
        { &quot;match&quot;: { &quot;title&quot;:   &quot;Search&quot;        }},
        { &quot;match&quot;: { &quot;content&quot;: &quot;Elasticsearch&quot; }}
      ], // The `filter` parameter indicates filter context. Its `term` and `range` clauses are used in filter context. They will filter out documents which do not match, but they will not affect the score for matching documents.
      &quot;filter&quot;: [ 
        { &quot;term&quot;:  { &quot;status&quot;: &quot;published&quot; }},
        { &quot;range&quot;: { &quot;publish_date&quot;: { &quot;gte&quot;: &quot;2015-01-01&quot; }}}
      ]
    }
  }
}
</code></pre>
<p>在查询上下文中为查询计算的分数表示为单精度浮点数; 对于有效度，它们只有24位。超过显着性精度的分数计算将被转换为精度损失的浮点数。</p>
<p>在过滤上下文使用 精确匹配以此获取 精确匹配的值</p>
<p>在查询上下文使用的条件应为 影响文档得分的条件</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="compound-queries"><a class="header" href="#compound-queries">Compound queries</a></h2>
<p>复核查询的作用是包装复核查询或者叶子查询</p>
<ul>
<li>以组合其结果和分数</li>
<li>更改其行为 例如更改分数评分规则</li>
<li>或者从查询上下文切换到过滤上下文</li>
</ul>
<p>The queries in this group are:</p>
<ul>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-bool-query.html"><code>bool</code> query</a></strong></p>
<p>The default query for combining multiple leaf or compound query clauses, as <code>must</code>, <code>should</code>, <code>must_not</code>, or <code>filter</code> clauses. The <code>must</code> and <code>should</code> clauses have their scores combined — the more matching clauses, the better — while the <code>must_not</code> and <code>filter</code> clauses are executed in filter context.</p>
<p>组合多个叶子或复合查询子句的默认查询， must、<em>should</em>、<em>must_not</em>、filter子句。<em>must</em>和<em>should</em>子句的分数组合在一起-匹配子句越多评分越好，而<em>must_not</em>和filter子句在filter上下文中执行。</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-boosting-query.html"><code>boosting</code> query</a></strong></p>
<p>Return documents which match a <code>positive</code> query, but reduce the score of documents which also match a <code>negative</code> query.</p>
<p>返回与肯定查询匹配的文档，但减少也与否定查询匹配的文档的分数。</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-constant-score-query.html"><code>constant_score</code> query</a></strong></p>
<p>A query which wraps another query, but executes it in filter context. All matching documents are given the same “constant” <code>_score</code>.</p>
<p>在过滤上下文中执行，所有文档都会被给定 常量分数</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-dis-max-query.html"><code>dis_max</code> query</a></strong></p>
<p>A query which accepts multiple queries, and returns any documents which match any of the query clauses. While the <code>bool</code> query combines the scores from all matching queries, the <code>dis_max</code> query uses the score of the single best- matching query clause.</p>
<p>接受多个查询并返回与任何查询子句匹配的任何文档的查询。虽然bool查询合并了所有匹配查询的分数，但dis_max查询使用了单个最佳匹配查询子句的分数。</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-function-score-query.html"><code>function_score</code> query</a></strong></p>
</li>
</ul>
<p>​		使用函数修改主查询返回的分数，以考虑诸如流行度（popularity），新近度（recency），距离（distance）或使用脚本实现的自定义算法等因素。</p>
<h2 id="boolean-query"><a class="header" href="#boolean-query">Boolean query</a></h2>
<p>A query that matches documents matching boolean combinations of other queries. The bool query maps to Lucene <code>BooleanQuery</code>. It is built using one or more boolean clauses, each clause with a typed occurrence. The occurrence types are:</p>
<p>与其他查询的布尔组合匹配的文档匹配的查询。布尔查询映射到Lucene BooleanQuery。</p>
<p>它是使用一个或多个布尔子句构建的，每个子句都有一个occurrence types  。occurrence types 为:</p>
<table><thead><tr><th>Occur</th><th>Description</th></tr></thead><tbody>
<tr><td><code>must</code></td><td>子句 (查询) 必须出现在匹配的文档中，并将有助于得分。</td></tr>
<tr><td><code>filter</code></td><td>子句 (查询) 必须出现在匹配的文档中 <br />然而，与must不同的是，查询的分数将被忽略 <br />在过滤上下文中执行，意味着忽略计分，并考虑子句进行缓存。</td></tr>
<tr><td><code>should</code></td><td>子句 (查询) 应该出现在匹配文档中。</td></tr>
<tr><td><code>must_not</code></td><td>子句 (查询) 不得出现在匹配的文档中<br />在过滤上下文中执行，意味着忽略计分，并考虑子句进行缓存。<br />由于忽略了评分，因此返回所有文档的评分为0。</td></tr>
</tbody></table>
<p>bool查询采用的是 匹配的条件越多越好  <em>more-matches-is-better</em> 策略，因此每个匹配 <em>must</em> 或 <em>should</em> 子句中的分数将被添加在一起，以提供每个文档的最终评分</p>
<pre><code class="language-console">POST _search
{
  &quot;query&quot;: {
    &quot;bool&quot; : {
      &quot;must&quot; : {
        &quot;term&quot; : { &quot;user.id&quot; : &quot;kimchy&quot; }
      },
      &quot;filter&quot;: {
        &quot;term&quot; : { &quot;tags&quot; : &quot;production&quot; }
      },
      &quot;must_not&quot; : {
        &quot;range&quot; : {
          &quot;age&quot; : { &quot;gte&quot; : 10, &quot;lte&quot; : 20 }
        }
      },
      &quot;should&quot; : [
        { &quot;term&quot; : { &quot;tags&quot; : &quot;env1&quot; } },
        { &quot;term&quot; : { &quot;tags&quot; : &quot;deployed&quot; } }
      ],
      &quot;minimum_should_match&quot; : 1,
      &quot;boost&quot; : 1.0
    }
  }
}
</code></pre>
<h3 id="using-minimum_should_match"><a class="header" href="#using-minimum_should_match">Using <code>minimum_should_match</code></a></h3>
<p>您可以使用 <em>minimum_should_match</em> 参数来指定返回的文档必须匹配的<em>should</em>子句的数量或百分比。</p>
<p>如果 <em>bool</em>查询 只包含一个 <em>should</em>子句 而且没有 <em>must</em> 或者 <em>filter</em> 则 <em>minimum_should_match</em> 默认值为1，否则默认值为0</p>
<p>详见 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-minimum-should-match.html"><code>minimum_should_match</code> parameter</a>.</p>
<h3 id="scoring-with-boolfilter"><a class="header" href="#scoring-with-boolfilter">Scoring with <code>bool.filter</code></a></h3>
<p>在filter元素下指定的查询对评分没有影响分数返回为0。</p>
<p>分数仅受已指定的<em>query</em> 影响。例如，以下所有三个查询都返回状态字段包含 “活动” 一词的所有文档。</p>
<p>第一个查询为所有文档分配0分，因为没有指定评分查询:</p>
<pre><code class="language-console">GET _search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;status&quot;: &quot;active&quot;
        }
      }
    }
  }
}
</code></pre>
<p>此bool查询具有<em>match_all</em>查询，该查询将分数分配给所有文档1.0分。</p>
<pre><code class="language-console">GET _search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;: {
        &quot;match_all&quot;: {}
      },
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;status&quot;: &quot;active&quot;
        }
      }
    }
  }
}
</code></pre>
<p>这个<em>constant_score</em>查询的行为方式与上面的第二个示例完全相同。constant_score查询为过滤器匹配的所有文档分配1.0的分数。</p>
<pre><code class="language-console">GET _search
{
  &quot;query&quot;: {
    &quot;constant_score&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: {
          &quot;status&quot;: &quot;active&quot;
        }
      }
    }
  }
}
</code></pre>
<h3 id="named-queries"><a class="header" href="#named-queries">Named queries</a></h3>
<p>每个查询在其顶层定义中接受一个 _name。您可以使用命名查询来跟踪与返回的文档匹配的查询。</p>
<p>如果使用命名查询，则响应会为每个命中  包含 <em>matched_queries</em> 属性。</p>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        { &quot;match&quot;: { &quot;name.first&quot;: { &quot;query&quot;: &quot;shay&quot;, &quot;_name&quot;: &quot;first&quot; } } },
        { &quot;match&quot;: { &quot;name.last&quot;: { &quot;query&quot;: &quot;banon&quot;, &quot;_name&quot;: &quot;last&quot; } } }
      ],
      &quot;filter&quot;: {
        &quot;terms&quot;: {
          &quot;name.last&quot;: [ &quot;banon&quot;, &quot;kimchy&quot; ],
          &quot;_name&quot;: &quot;test&quot;
        }
      }
    }
  }
}
</code></pre>
<h2 id="boosting-query"><a class="header" href="#boosting-query">Boosting query</a></h2>
<p>Returns documents matching a <code>positive</code> query while reducing the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance score</a> of documents that also match a <code>negative</code> query.</p>
<p>You can use the <code>boosting</code> query to demote certain documents without excluding them from the search results.</p>
<p>返回与 <code>positive</code>  查询匹配的文档，同时降低  与否定查询匹配的文档的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance score</a> 。
您可以使用 <code>boosting</code> query 查询来降低某些文档的评分，但不会将它们从搜索结果中排除。</p>
<pre><code>GET /_search
{
  &quot;query&quot;: {
    &quot;boosting&quot;: {
      &quot;positive&quot;: {
        &quot;term&quot;: {
          &quot;text&quot;: &quot;apple&quot;
        }
      },
      &quot;negative&quot;: {
        &quot;term&quot;: {
          &quot;text&quot;: &quot;pie tart fruit crumble tree&quot;
        }
      },
      &quot;negative_boost&quot;: 0.5
    }
  }
}
</code></pre>
<h3 id="top-level-parameters-for-boosting"><a class="header" href="#top-level-parameters-for-boosting">Top-level parameters for <code>boosting</code></a></h3>
<ul>
<li>
<p><strong><code>positive</code></strong></p>
<p>(Required, query object) Query you wish to run. Any returned documents must match this query.</p>
</li>
<li>
<p><strong><code>negative</code></strong></p>
<p>(Required, query object) Query used to decrease the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance score</a> of matching documents.If a returned document matches the <code>positive</code> query and this query, the <code>boosting</code> query calculates the final <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance score</a> for the document as follows:Take the original relevance score from the <code>positive</code> query.Multiply the score by the <code>negative_boost</code> value.</p>
</li>
<li>
<p><strong><code>negative_boost</code></strong></p>
<p>(Required, float) Floating point number between <code>0</code> and <code>1.0</code> used to decrease the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance scores</a> of documents matching the <code>negative</code> query.</p>
</li>
</ul>
<h2 id="constant-score-query"><a class="header" href="#constant-score-query">Constant score query</a></h2>
<p>Wraps a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-bool-query.html">filter query</a> and returns every matching document with a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance score</a> equal to the <code>boost</code> parameter value.</p>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;constant_score&quot;: {
      &quot;filter&quot;: {
        &quot;term&quot;: { &quot;user.id&quot;: &quot;kimchy&quot; }
      },
      &quot;boost&quot;: 1.2
    }
  }
}
</code></pre>
<h3 id="top-level-parameters-for-constant_score"><a class="header" href="#top-level-parameters-for-constant_score">Top-level parameters for <code>constant_score</code></a></h3>
<ul>
<li>
<p><strong><code>filter</code></strong></p>
<p>(Required, query object) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-bool-query.html">Filter query</a> you wish to run. Any returned documents must match this query.Filter queries do not calculate <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance scores</a>. To speed up performance, Elasticsearch automatically caches frequently used filter queries.</p>
</li>
<li>
<p><strong><code>boost</code></strong></p>
<p>(Optional, float) Floating point number used as the constant <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html#relevance-scores">relevance score</a> for every document matching the <code>filter</code> query. Defaults to <code>1.0</code>.</p>
</li>
</ul>
<p><strong>指定文档评分分数返回</strong></p>
<h2 id="disjunction-max-query"><a class="header" href="#disjunction-max-query">Disjunction max query</a></h2>
<p>Returns documents matching one or more wrapped queries, called query clauses or clauses.</p>
<p>返回与一个或多个包装查询匹配的文档，称为查询子句或子句。</p>
<p>If a returned document matches multiple query clauses, the <code>dis_max</code> query assigns the document the highest relevance score from any matching clause, plus a tie breaking increment for any additional matching subqueries.</p>
<p>如果返回的文档与多个查询子句匹配，则<em>dis_max</em>查询将从匹配的子查询中 选出分数最高的子查询，作为评分。</p>
<p>,plus a tie breaking increment for any additional matching subqueries.</p>
<p>You can use the <code>dis_max</code> to search for a term in fields mapped with different <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/mapping-boost.html">boost</a> factors.</p>
<h3 id="example-request"><a class="header" href="#example-request">Example request</a></h3>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;dis_max&quot;: {
      &quot;queries&quot;: [
        { &quot;term&quot;: { &quot;title&quot;: &quot;Quick pets&quot; } },
        { &quot;term&quot;: { &quot;body&quot;: &quot;Quick pets&quot; } }
      ],
      &quot;tie_breaker&quot;: 0.7
    }
  }
}
</code></pre>
<h3 id="top-level-parameters-for-dis_max"><a class="header" href="#top-level-parameters-for-dis_max">Top-level parameters for <code>dis_max</code></a></h3>
<ul>
<li>
<p><strong><code>queries</code></strong></p>
<p>(Required, array of query objects) Contains one or more query clauses. Returned documents <strong>must match one or more</strong> of these queries. If a document matches multiple queries, Elasticsearch uses the highest <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-filter-context.html">relevance score</a>.</p>
</li>
<li>
<p><strong><code>tie_breaker</code></strong></p>
<p>(可选，浮点数) 0到1.0之间的浮点数，用于增加与多个查询子句匹配的文档的相关性分数。默认为0.0. 可以使用 <em>tie_breaker</em> 值 为 在多个字段中包含相同术语的文档中，选出最好的一个字段</p>
<p>如果文档匹配多个子句，<em>dis_max</em>查询将计算文档的相关性得分如下</p>
<ol>
<li>从具有最高分数的匹配子句中获取相关性分数</li>
<li>将其他匹配子句的分数乘以<em>tie_breaker</em>值，将最高分数加到相乘的分数中</li>
<li>如果tie_breaker值大于0.0，则所有匹配子句都会计数，但得分最高的子句会   counts most.</li>
</ol>
</li>
</ul>
<h2 id="function-score-query"><a class="header" href="#function-score-query">Function score query</a></h2>
<p>The <code>function_score</code> allows you to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents.</p>
<p>function_score允许您修改查询检索文档的分数。例如，如果得分函数在计算上是昂贵的，并且足以在过滤的文档集上计算得分，则这可能是有用的。</p>
<p>要使用<em>function_score</em>，用户必须定义一个查询和一个或多个函数，这些函数为查询返回的每个文档计算新的分数。</p>
<p><em>Function_score</em> 只能与这样的一个函数一起使用:</p>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;function_score&quot;: {
      &quot;query&quot;: { &quot;match_all&quot;: {} },
      &quot;boost&quot;: &quot;5&quot;,
      &quot;random_score&quot;: {}, 
      &quot;boost_mode&quot;: &quot;multiply&quot;
    }
  }
}
</code></pre>
<p>此外，可以组合几种功能。</p>
<p>在这种情况下，只有当文档与给定的过滤查询匹配时，才可以选择应用该函数</p>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;function_score&quot;: {
      &quot;query&quot;: { &quot;match_all&quot;: {} },
      &quot;boost&quot;: &quot;5&quot;, 
      &quot;functions&quot;: [
        {
          &quot;filter&quot;: { &quot;match&quot;: { &quot;test&quot;: &quot;bar&quot; } },
          &quot;random_score&quot;: {}, 
          &quot;weight&quot;: 23
        },
        {
          &quot;filter&quot;: { &quot;match&quot;: { &quot;test&quot;: &quot;cat&quot; } },
          &quot;weight&quot;: 42
        }
      ],
      &quot;max_boost&quot;: 42,
      &quot;score_mode&quot;: &quot;max&quot;,
      &quot;boost_mode&quot;: &quot;multiply&quot;,
      &quot;min_score&quot;: 42
    }
  }
}
</code></pre>
<p>每个函数的过滤查询产生的分数无关紧要。
如果没有使用函数给出过滤器，则等效于指定 “match_all”: {}
首先，每个文档都由定义的函数评分。参数<em>score_mode</em>指定如何组合计算的分数:</p>
<table><thead><tr><th>score_mode</th><th>description</th></tr></thead><tbody>
<tr><td><code>multiply</code></td><td>scores are multiplied (default)</td></tr>
<tr><td><code>sum</code></td><td>scores are summed</td></tr>
<tr><td><code>avg</code></td><td>scores are averaged</td></tr>
<tr><td><code>first</code></td><td>the first function that has a matching filter is applied</td></tr>
<tr><td><code>max</code></td><td>maximum score is used</td></tr>
<tr><td><code>min</code></td><td>minimum score is used</td></tr>
</tbody></table>
<p>Because scores can be on different scales (for example, between 0 and 1 for decay functions but arbitrary for <code>field_value_factor</code>) and also because sometimes a different impact of functions on the score is desirable, the score of each function can be adjusted with a user defined <code>weight</code>. The <code>weight</code> can be defined per function in the <code>functions</code> array (example above) and is multiplied with the score computed by the respective function. If weight is given without any other function declaration, <code>weight</code> acts as a function that simply returns the <code>weight</code>.</p>
<p>因为分数可以在不同的尺度上 (例如，衰减函数在0到1之间，但对于<em>field_value_factor</em>是任意的)，而且因为有时函数对分数的不同影响是可取的，所以每个函数的分数可以用用户定义的权重来调整。可以在函数数组 (上面的示例) 中为每个函数定义权重，并将其与相应函数计算的分数相乘。如果在没有任何其他函数声明的情况下给出了weight，则weight充当简单地返回权重的函数。</p>
<p>如果score_mode设置为avg，则各个分数将通过加权平均值合并。例如，如果两个函数返回分数1和2，并且它们各自的权重为3和4，那么它们的分数将合并为 <code>(1*3+2*4)/(3+4)</code> ，而不是 <code>(1*3+2*4)/2</code>.</p>
<p>可以通过设置<em>max_boost</em>参数将新分数限制为不超过某个限制。max_boost的默认值是FLT_MAX。</p>
<p>新计算的分数与查询的分数相结合。参数<em>boost_mode</em>定义如何:</p>
<table><thead><tr><th>boost_mode</th><th>description</th></tr></thead><tbody>
<tr><td><code>multiply</code></td><td>查询分数和函数分数相乘 (默认)</td></tr>
<tr><td><code>replace</code></td><td>只使用函数分数，查询分数被忽略</td></tr>
<tr><td><code>sum</code></td><td>查询分数和函数分数相加</td></tr>
<tr><td><code>avg</code></td><td>取平均值</td></tr>
<tr><td><code>max</code></td><td>max of query score and function score</td></tr>
<tr><td><code>min</code></td><td>min of query score and function score</td></tr>
</tbody></table>
<p>By default, modifying the score does not change which documents match. To exclude documents that do not meet a certain score threshold the <code>min_score</code> parameter can be set to the desired score threshold.</p>
<p>For <code>min_score</code> to work, <strong>all</strong> documents returned by the query need to be scored and then filtered out one by one.</p>
<p>The <code>function_score</code> query provides several types of score functions.</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-function-score-query.html#function-script-score"><code>script_score</code></a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-function-score-query.html#function-weight"><code>weight</code></a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-function-score-query.html#function-random"><code>random_score</code></a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-function-score-query.html#function-field-value-factor"><code>field_value_factor</code></a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.10/query-dsl-function-score-query.html#function-decay">decay functions</a>: <code>gauss</code>, <code>linear</code>, <code>exp</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="search-after"><a class="header" href="#search-after">Search After</a></h1>
<p>结果的分页可以通过使用 from 和 size 来完成，但是当达到深度分页时成本变得禁止。 <em>index.max_result_window</em> 默认为 10,000 是一种保护，</p>
<p>搜索请求占用堆内存和时间与 from + size 成比例。 </p>
<p>建议使用 Scroll api 进行高效的深层滚动，但滚动上下文是昂贵的，不建议将其用于实时用户请求。 </p>
<p>search_after 参数通过提供活动光标来规避此问题。 </p>
<p>这个想法是使用前一页的结果来帮助检索下一页。</p>
<p>假设检索第一页的查询如下所示：</p>
<pre><code class="language-json">GET twitter/tweet/_search
{
    &quot;size&quot;: 10,
    &quot;query&quot;: {
        &quot;match&quot; : {
            &quot;title&quot; : &quot;elasticsearch&quot;
        }
    },
    &quot;sort&quot;: [
        {&quot;date&quot;: &quot;asc&quot;},
        {&quot;_uid&quot;: &quot;desc&quot;}
    ]
}
</code></pre>
<p><strong>注意</strong></p>
<p>每个文档具有一个唯一值的字段应用作排序规范的仲裁。 </p>
<p>否则，具有相同排序值的文档的排序顺序将是未定义的。 建议的方法是使用字段 _uid，它确保每个文档包含一个唯一值。</p>
<p>上述请求的结果包括每个文档的排序值数组。 这些排序值可以与 search_after 参数结合使用，以便在结果列表中的任何文档之后“返回”结果。</p>
<p>例如，我们可以使用最后一个文档的排序值，并将其传递给search_after 以检索下一页结果：</p>
<pre><code>GET twitter/tweet/_search
{
    &quot;size&quot;: 10,
    &quot;query&quot;: {
        &quot;match&quot; : {
            &quot;title&quot; : &quot;elasticsearch&quot;
        }
    },
    &quot;search_after&quot;: [1463538857, &quot;tweet#654323&quot;],
    &quot;sort&quot;: [
        {&quot;date&quot;: &quot;asc&quot;},
        {&quot;_uid&quot;: &quot;desc&quot;}
    ]
}
</code></pre>
<p><strong>当使用 search_after 时，参数 from 必须设置为 0（或 -1 ）。</strong></p>
<p>search_after 不是一种自由地跳到随机页面的解决方案，而是一种并行地滚动许多查询的解决方案。 它非常类似于滚动 API，</p>
<p>但不同的是，search_after 参数是无状态的，它总是解决对搜索器的最新版本。 因此，排序顺序可能会在步行期间更改，具体取决于您的索引的更新和删除。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="全文索引"><a class="header" href="#全文索引">全文索引</a></h2>
<p>The full text queries enable you to search <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">analyzed text fields</a> such as the body of an email</p>
<p>索引的分析器与 查询的分析器必须相同</p>
<p>The queries in this group are:</p>
<ul>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html"><code>intervals</code> query</a></strong></p>
<p>A full text query that allows fine-grained control of the ordering and proximity of matching terms.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query.html"><code>match</code> query</a></strong></p>
<p>用于执行全文查询的标准查询，包括模糊匹配和短语或邻近查询。</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-bool-prefix-query.html"><code>match_bool_prefix</code> query</a></strong></p>
<p>Creates a <code>bool</code> query that matches each term as a <code>term</code> query, except for the last term, which is matched as a <code>prefix</code> query</p>
<p>创建一个bool查询，该查询将每个term match为术语查询，但最后一个术语除外，该术语被匹配为前缀查询</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query-phrase.html"><code>match_phrase</code> query</a></strong></p>
<p>Like the <code>match</code> query but used for matching exact phrases or word proximity matches.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query-phrase-prefix.html"><code>match_phrase_prefix</code> query</a></strong></p>
<p>Like the <code>match_phrase</code> query, but does a wildcard search on the final word.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-multi-match-query.html"><code>multi_match</code> query</a></strong></p>
<p>The multi-field version of the <code>match</code> query.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-combined-fields-query.html"><code>combined_fields</code> query</a></strong></p>
<p>Matches over multiple fields as if they had been indexed into one combined field.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-query-string-query.html"><code>query_string</code> query</a></strong></p>
<p>Supports the compact Lucene <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-query-string-query.html#query-string-syntax">query string syntax</a>, allowing you to specify AND|OR|NOT conditions and multi-field search within a single query string. For expert users only.</p>
</li>
<li>
<p><strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-simple-query-string-query.html"><code>simple_query_string</code> query</a></strong></p>
<p>A simpler, more robust version of the <code>query_string</code> syntax suitable for exposing directly to users.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="intervals-query"><a class="header" href="#intervals-query">Intervals query</a></h2>
<p>Returns documents based on the order and proximity of matching terms.</p>
<p>The <code>intervals</code> query uses <strong>matching rules</strong>, constructed from a small set of definitions. </p>
<p>These rules are then applied to terms from a specified <code>field</code>.</p>
<p>The definitions produce sequences of minimal intervals that span terms in a body of text. These intervals can be further combined and filtered by parent sources.</p>
<h2 id="example-request-1"><a class="header" href="#example-request-1">Example request</a></h2>
<p>The following <code>intervals</code> search returns documents containing <code>my favorite food</code> without any gap, followed by <code>hot water</code> or <code>cold porridge</code> in the <code>my_text</code> field.</p>
<p>This search would match a <code>my_text</code> value of <code>my favorite food is cold porridge</code> but not <code>when it's cold my favorite food is porridge</code>.</p>
<pre><code class="language-json">POST _search
{
  &quot;query&quot;: { 
    &quot;intervals&quot; : {
      &quot;my_text&quot; : {
        &quot;all_of&quot; : {
          &quot;ordered&quot; : true,
          &quot;intervals&quot; : [
            {
              &quot;match&quot; : {
                &quot;query&quot; : &quot;my favorite food&quot;,
                &quot;max_gaps&quot; : 0,
                &quot;ordered&quot; : true
              }
            },
            {
              &quot;any_of&quot; : {
                &quot;intervals&quot; : [
                  { &quot;match&quot; : { &quot;query&quot; : &quot;hot water&quot; } },
                  { &quot;match&quot; : { &quot;query&quot; : &quot;cold porridge&quot; } }
                ]
              }
            }
          ]
        }
      }
    }
  }
}
</code></pre>
<h2 id="top-level-parameters-for-intervals"><a class="header" href="#top-level-parameters-for-intervals">Top-level parameters for <code>intervals</code></a></h2>
<ul>
<li>
<p><strong><code>&lt;field&gt;</code></strong></p>
<p>(必填，规则对象) 您希望搜索的字段。此参数的值是一个规则对象，用于根据匹配的术语、顺序和接近度来匹配文档。Valid rules include:<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#intervals-match"><code>match</code></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#intervals-prefix"><code>prefix</code></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#intervals-wildcard"><code>wildcard</code></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#intervals-fuzzy"><code>fuzzy</code></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#intervals-all_of"><code>all_of</code></a><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#intervals-any_of"><code>any_of</code></a></p>
</li>
</ul>
<h3 id="match-rule-parameters"><a class="header" href="#match-rule-parameters"><code>match</code> rule parameters</a></h3>
<p>The <code>match</code> rule matches analyzed text.</p>
<ul>
<li>
<p><strong><code>query</code></strong></p>
<p>(Required, string) Text you wish to find in the provided <code>&lt;field&gt;</code>.</p>
</li>
<li>
<p><strong><code>max_gaps</code></strong></p>
<p>(Optional, integer) Maximum number of positions between the matching terms. Terms further apart than this are not considered matches. Defaults to <code>-1</code>.If unspecified or set to <code>-1</code>, there is no width restriction on the match. If set to <code>0</code>, the terms must appear next to each other.</p>
</li>
<li>
<p><strong><code>ordered</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, matching terms must appear in their specified order. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>analyzer</code></strong></p>
<p>(Optional, string) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">analyzer</a> used to analyze terms in the <code>query</code>. Defaults to the top-level <code>&lt;field&gt;</code>'s analyzer.</p>
</li>
<li>
<p><strong><code>filter</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#interval_filter">interval filter</a> rule object) An optional interval filter.</p>
</li>
<li>
<p><strong><code>use_field</code></strong></p>
<p>(Optional, string) If specified, then match intervals from this field rather than the top-level <code>&lt;field&gt;</code>. Terms are analyzed using the search analyzer from this field. This allows you to search across multiple fields as if they were all the same field; for example, you could index the same text into stemmed and unstemmed fields, and search for stemmed tokens near unstemmed ones.</p>
</li>
</ul>
<h3 id="prefix-rule-parameters"><a class="header" href="#prefix-rule-parameters"><code>prefix</code> rule parameters</a></h3>
<p>The <code>prefix</code> rule matches terms that start with a specified set of characters. This prefix can expand to match at most 128 terms. If the prefix matches more than 128 terms, Elasticsearch returns an error. You can use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-prefixes.html"><code>index-prefixes</code></a> option in the field mapping to avoid this limit.</p>
<ul>
<li>
<p><strong><code>prefix</code></strong></p>
<p>(Required, string) Beginning characters of terms you wish to find in the top-level <code>&lt;field&gt;</code>.</p>
</li>
<li>
<p><strong><code>analyzer</code></strong></p>
<p>(Optional, string) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">analyzer</a> used to normalize the <code>prefix</code>. Defaults to the top-level <code>&lt;field&gt;</code>'s analyzer.</p>
</li>
<li>
<p><strong><code>use_field</code></strong></p>
<p>(Optional, string) If specified, then match intervals from this field rather than the top-level <code>&lt;field&gt;</code>.The <code>prefix</code> is normalized using the search analyzer from this field, unless a separate <code>analyzer</code> is specified.</p>
</li>
</ul>
<h3 id="wildcard-rule-parameters"><a class="header" href="#wildcard-rule-parameters"><code>wildcard</code> rule parameters</a></h3>
<p>The <code>wildcard</code> rule matches terms using a wildcard pattern. This pattern can expand to match at most 128 terms. If the pattern matches more than 128 terms, Elasticsearch returns an error.</p>
<ul>
<li>
<p><strong><code>pattern</code></strong></p>
<p>(Required, string) Wildcard pattern used to find matching terms.This parameter supports two wildcard operators:<code>?</code>, which matches any single character<code>*</code>, which can match zero or more characters, including an empty oneAvoid beginning patterns with <code>*</code> or <code>?</code>. This can increase the iterations needed to find matching terms and slow search performance.</p>
</li>
<li>
<p><strong><code>analyzer</code></strong></p>
<p>(Optional, string) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">analyzer</a> used to normalize the <code>pattern</code>. Defaults to the top-level <code>&lt;field&gt;</code>'s analyzer.</p>
</li>
<li>
<p><strong><code>use_field</code></strong></p>
<p>(Optional, string) If specified, match intervals from this field rather than the top-level <code>&lt;field&gt;</code>.The <code>pattern</code> is normalized using the search analyzer from this field, unless <code>analyzer</code> is specified separately.</p>
</li>
</ul>
<h3 id="fuzzy-rule-parameters"><a class="header" href="#fuzzy-rule-parameters"><code>fuzzy</code> rule parameters</a></h3>
<p>The <code>fuzzy</code> rule matches terms that are similar to the provided term, within an edit distance defined by <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/common-options.html#fuzziness">Fuzziness</a>. If the fuzzy expansion matches more than 128 terms, Elasticsearch returns an error.</p>
<ul>
<li>
<p><strong><code>term</code></strong></p>
<p>(Required, string) The term to match</p>
</li>
<li>
<p><strong><code>prefix_length</code></strong></p>
<p>(Optional, integer) Number of beginning characters left unchanged when creating expansions. Defaults to <code>0</code>.</p>
</li>
<li>
<p><strong><code>transpositions</code></strong></p>
<p>(Optional, Boolean) Indicates whether edits include transpositions of two adjacent characters (ab → ba). Defaults to <code>true</code>.</p>
</li>
<li>
<p><strong><code>fuzziness</code></strong></p>
<p>(Optional, string) Maximum edit distance allowed for matching. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/common-options.html#fuzziness">Fuzziness</a> for valid values and more information. Defaults to <code>auto</code>.</p>
</li>
<li>
<p><strong><code>analyzer</code></strong></p>
<p>(Optional, string) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">analyzer</a> used to normalize the <code>term</code>. Defaults to the top-level <code>&lt;field&gt;</code> 's analyzer.</p>
</li>
<li>
<p><strong><code>use_field</code></strong></p>
<p>(Optional, string) If specified, match intervals from this field rather than the top-level <code>&lt;field&gt;</code>.The <code>term</code> is normalized using the search analyzer from this field, unless <code>analyzer</code> is specified separately.</p>
</li>
</ul>
<h3 id="all_of-rule-parameters"><a class="header" href="#all_of-rule-parameters"><code>all_of</code> rule parameters</a></h3>
<p>The <code>all_of</code> rule returns matches that span a combination of other rules.</p>
<ul>
<li>
<p><strong><code>intervals</code></strong></p>
<p>(Required, array of rule objects) An array of rules to combine. All rules must produce a match in a document for the overall source to match.</p>
</li>
<li>
<p><strong><code>max_gaps</code></strong></p>
<p>(Optional, integer) Maximum number of positions between the matching terms. Intervals produced by the rules further apart than this are not considered matches. Defaults to <code>-1</code>.If unspecified or set to <code>-1</code>, there is no width restriction on the match. If set to <code>0</code>, the terms must appear next to each other.</p>
<p>(可选，整数) 匹配项之间的最大位置数。比这更远的规则产生的间隔不被视为匹配。默认值为-1。如果未指定或设置为-1，则匹配项没有宽度限制。如果设置为0，则术语必须彼此相邻出现。</p>
</li>
<li>
<p><strong><code>ordered</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, intervals produced by the rules should appear in the order in which they are specified. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>filter</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#interval_filter">interval filter</a> rule object) Rule used to filter returned intervals.</p>
</li>
</ul>
<h3 id="any_of-rule-parameters"><a class="header" href="#any_of-rule-parameters"><code>any_of</code> rule parameters</a></h3>
<p>The <code>any_of</code> rule returns intervals produced by any of its sub-rules.</p>
<ul>
<li>
<p><strong><code>intervals</code></strong></p>
<p>(Required, array of rule objects) An array of rules to match.</p>
</li>
<li>
<p><strong><code>filter</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#interval_filter">interval filter</a> rule object) Rule used to filter returned intervals.</p>
</li>
</ul>
<h3 id="filter-rule-parameters"><a class="header" href="#filter-rule-parameters"><code>filter</code> rule parameters</a></h3>
<p>The <code>filter</code> rule returns intervals based on a query. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#interval-filter-rule-ex">Filter example</a> for an example.</p>
<ul>
<li>
<p><strong><code>after</code></strong></p>
<p>(Optional, query object) Query used to return intervals that follow an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>before</code></strong></p>
<p>(Optional, query object) Query used to return intervals that occur before an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>contained_by</code></strong></p>
<p>(Optional, query object) Query used to return intervals contained by an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>containing</code></strong></p>
<p>(Optional, query object) Query used to return intervals that contain an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>not_contained_by</code></strong></p>
<p>(Optional, query object) Query used to return intervals that are <strong>not</strong> contained by an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>not_containing</code></strong></p>
<p>(Optional, query object) Query used to return intervals that do <strong>not</strong> contain an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>not_overlapping</code></strong></p>
<p>(Optional, query object) Query used to return intervals that do <strong>not</strong> overlap with an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>overlapping</code></strong></p>
<p>(Optional, query object) Query used to return intervals that overlap with an interval from the <code>filter</code> rule.</p>
</li>
<li>
<p><strong><code>script</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/modules-scripting-using.html">script object</a>) Script used to return matching documents. This script must return a boolean value, <code>true</code> or <code>false</code>. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-intervals-query.html#interval-script-filter">Script filters</a> for an example.</p>
</li>
</ul>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<h3 id="filter-example"><a class="header" href="#filter-example">Filter example</a></h3>
<ol>
<li><code>hot</code> and <code>porridge</code>  之间不超过10个位置</li>
<li>中间不能出现 <em>salty</em> 单词</li>
</ol>
<pre><code class="language-console">POST _search
{
  &quot;query&quot;: {
    &quot;intervals&quot; : {
      &quot;my_text&quot; : {
        &quot;match&quot; : {
          &quot;query&quot; : &quot;hot porridge&quot;,
          &quot;max_gaps&quot; : 10,
          &quot;filter&quot; : {
            &quot;not_containing&quot; : {
              &quot;match&quot; : {
                &quot;query&quot; : &quot;salty&quot;
              }
            }
          }
        }
      }
    }
  }
}
</code></pre>
<h3 id="script-filters"><a class="header" href="#script-filters">Script filters</a></h3>
<p>You can use a script to filter intervals based on their start position, end position, and internal gap count. The following <code>filter</code> script uses the <code>interval</code> variable with the <code>start</code>, <code>end</code>, and <code>gaps</code> methods:</p>
<pre><code class="language-console">POST _search
{
  &quot;query&quot;: {
    &quot;intervals&quot; : {
      &quot;my_text&quot; : {
        &quot;match&quot; : {
          &quot;query&quot; : &quot;hot porridge&quot;,
          &quot;filter&quot; : {
            &quot;script&quot; : {
              &quot;source&quot; : &quot;interval.start &gt; 10 &amp;&amp; interval.end &lt; 20 &amp;&amp; interval.gaps == 0&quot;
            }
          }
        }
      }
    }
  }
}
</code></pre>
<h3 id="minimization"><a class="header" href="#minimization">Minimization</a></h3>
<p>间隙查询优先选择最小化间隔以确保查询在线性时间执行完毕</p>
<p>This can sometimes cause surprising results, particularly when using <code>max_gaps</code> restrictions or filters. For example, take the following query, searching for <code>salty</code> contained within the phrase <code>hot porridge</code>:</p>
<pre><code class="language-console">POST _search
{
  &quot;query&quot;: {
    &quot;intervals&quot; : {
      &quot;my_text&quot; : {
        &quot;match&quot; : {
          &quot;query&quot; : &quot;salty&quot;,
          &quot;filter&quot; : {
            &quot;contained_by&quot; : {
              &quot;match&quot; : {
                &quot;query&quot; : &quot;hot porridge&quot;
              }
            }
          }
        }
      }
    }
  }
}
</code></pre>
<p>This query does <strong>not</strong> match a document containing the phrase <code>hot porridge is salty porridge</code>, because the intervals returned by the match query for <code>hot porridge</code> only cover the initial two terms in this document, and these do not overlap the intervals covering <code>salty</code>.</p>
<h3 id="anyof"><a class="header" href="#anyof">Anyof</a></h3>
<p>Another restriction to be aware of is the case of <code>any_of</code> rules that contain sub-rules which overlap. In particular, if one of the rules is a strict prefix of the other, then the longer rule can never match, which can cause surprises when used in combination with <code>max_gaps</code>. Consider the following query, searching for <code>the</code> immediately followed by <code>big</code> or <code>big bad</code>, immediately followed by <code>wolf</code>:</p>
<pre><code class="language-console">POST _search
{
  &quot;query&quot;: {
    &quot;intervals&quot; : {
      &quot;my_text&quot; : {
        &quot;all_of&quot; : {
          &quot;intervals&quot; : [
            { &quot;match&quot; : { &quot;query&quot; : &quot;the&quot; } },
            { &quot;any_of&quot; : {
                &quot;intervals&quot; : [
                    { &quot;match&quot; : { &quot;query&quot; : &quot;big&quot; } },
                    { &quot;match&quot; : { &quot;query&quot; : &quot;big bad&quot; } }
                ] } },
            { &quot;match&quot; : { &quot;query&quot; : &quot;wolf&quot; } }
          ],
          &quot;max_gaps&quot; : 0,
          &quot;ordered&quot; : true
        }
      }
    }
  }
}
</code></pre>
<p>Counter-intuitively（与直觉相反，）,不会匹配 <code>the big bad wolf</code>  因为<em>any_of</em> 规则 优先匹配 <em>big</em> 间隔查询，而不会匹配  <code>big bad</code> 间隔查询，因为是基于最小化匹配原则</p>
<pre><code class="language-console">POST _search
{
  &quot;query&quot;: {
    &quot;intervals&quot; : {
      &quot;my_text&quot; : {
        &quot;any_of&quot; : {
          &quot;intervals&quot; : [
            { &quot;match&quot; : {
                &quot;query&quot; : &quot;the big bad wolf&quot;,
                &quot;ordered&quot; : true,
                &quot;max_gaps&quot; : 0 } },
            { &quot;match&quot; : {
                &quot;query&quot; : &quot;the big wolf&quot;,
                &quot;ordered&quot; : true,
                &quot;max_gaps&quot; : 0 } }
           ]
        }
      }
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="match-query"><a class="header" href="#match-query">Match query</a></h2>
<p>返回与提供的文本、数字、日期或布尔值匹配的文档。提供的文本在匹配前进行分析。</p>
<p>匹配查询是用于执行全文搜索的标准查询，包括用于模糊匹配的选项。</p>
<h3 id="example-request-2"><a class="header" href="#example-request-2">Example request</a></h3>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;message&quot;: {
        &quot;query&quot;: &quot;this is a test&quot;
      }
    }
  }
}
</code></pre>
<h3 id="top-level-parameters-for-match"><a class="header" href="#top-level-parameters-for-match">Top-level parameters for <code>match</code></a></h3>
<ul>
<li>
<p><strong><code>&lt;field&gt;</code></strong></p>
<p>(Required, object) Field you wish to search.</p>
</li>
</ul>
<h3 id="parameters-for-field"><a class="header" href="#parameters-for-field">Parameters for <code>&lt;field&gt;</code></a></h3>
<ul>
<li>
<p><strong><code>query</code></strong></p>
<p>(Required) Text, number, boolean value or date you wish to find in the provided <code>&lt;field&gt;</code>.The <code>match</code> query <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">analyzes</a> any provided text before performing a search. This means the <code>match</code> query can search <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/text.html"><code>text</code></a> fields for analyzed tokens rather than an exact term.</p>
</li>
<li>
<p><strong><code>analyzer</code></strong></p>
<p>(Optional, string) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/analysis.html">Analyzer</a> used to convert the text in the <code>query</code> value into tokens. Defaults to the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/specify-analyzer.html#specify-index-time-analyzer">index-time analyzer</a> mapped for the <code>&lt;field&gt;</code>. If no analyzer is mapped, the index’s default analyzer is used.</p>
</li>
<li>
<p><strong><code>auto_generate_synonyms_phrase_query</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query-phrase.html">match phrase</a> queries are automatically created for multi-term synonyms. Defaults to <code>true</code>.See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query.html#query-dsl-match-query-synonyms">Use synonyms with match query</a> for an example.</p>
</li>
<li>
<p><strong><code>fuzziness</code></strong></p>
<p>(Optional, string) Maximum edit distance allowed for matching. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/common-options.html#fuzziness">Fuzziness</a> for valid values and more information. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query.html#query-dsl-match-query-fuzziness">Fuzziness in the match query</a> for an example.</p>
</li>
<li>
<p><strong><code>max_expansions</code></strong></p>
<p>(Optional, integer) Maximum number of terms to which the query will expand. Defaults to <code>50</code>.</p>
</li>
<li>
<p><strong><code>prefix_length</code></strong></p>
<p>(Optional, integer) Number of beginning characters left unchanged for fuzzy matching. Defaults to <code>0</code>.</p>
</li>
<li>
<p><strong><code>fuzzy_transpositions</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, edits for fuzzy matching include transpositions of two adjacent characters (ab → ba). Defaults to <code>true</code>.</p>
</li>
<li>
<p><strong><code>fuzzy_rewrite</code></strong></p>
<p>(Optional, string) Method used to rewrite the query. See the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-multi-term-rewrite.html"><code>rewrite</code> parameter</a> for valid values and more information.If the <code>fuzziness</code> parameter is not <code>0</code>, the <code>match</code> query uses a <code>fuzzy_rewrite</code> method of <code>top_terms_blended_freqs_${max_expansions}</code> by default.</p>
</li>
<li>
<p><strong><code>lenient</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, format-based errors, such as providing a text <code>query</code> value for a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/number.html">numeric</a> field, are ignored. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>operator</code></strong></p>
<p>(Optional, string) Boolean logic used to interpret text in the <code>query</code> value. Valid values are:**<code>OR</code> (Default)<strong>For example, a <code>query</code> value of <code>capital of Hungary</code> is interpreted as <code>capital OR of OR Hungary</code>.</strong><code>AND</code>**For example, a <code>query</code> value of <code>capital of Hungary</code> is interpreted as <code>capital AND of AND Hungary</code>.</p>
</li>
<li>
<p><strong><code>minimum_should_match</code></strong></p>
<p>(Optional, string) Minimum number of clauses that must match for a document to be returned. See the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-minimum-should-match.html"><code>minimum_should_match</code> parameter</a> for valid values and more information.</p>
</li>
<li>
<p><strong><code>zero_terms_query</code></strong></p>
<p>(Optional, string) Indicates whether no documents are returned if the <code>analyzer</code> removes all tokens, such as when using a <code>stop</code> filter. Valid values are:**<code>none</code> (Default)<strong>No documents are returned if the <code>analyzer</code> removes all tokens.</strong><code>all</code>**Returns all documents, similar to a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-all-query.html"><code>match_all</code></a> query.See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-match-query.html#query-dsl-match-query-zero">Zero terms query</a> for an example.</p>
</li>
</ul>
<h3 id="notes"><a class="header" href="#notes">Notes</a></h3>
<h4 id="short-request-example"><a class="header" href="#short-request-example">Short request example</a></h4>
<p>You can simplify the match query syntax by combining the <code>&lt;field&gt;</code> and <code>query</code> parameters. For example:</p>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;message&quot;: &quot;this is a test&quot;
    }
  }
}
</code></pre>
<h4 id="how-the-match-query-works"><a class="header" href="#how-the-match-query-works">How the match query works</a></h4>
<p>The <code>match</code> query is of type <code>boolean</code>. </p>
<ol>
<li>为<em>boolean</em> query</li>
<li>这意味着对提供的文本进行分析，分析过程从提供的文本中构造一个布尔查询。</li>
<li><code>operator</code> 参数 可以为 <em>or</em> <em>and</em>  控制<em>boolean</em> 子句，默认是 <em>or</em></li>
<li>可以使用<em>minimum_should_match</em>参数设置要匹配的可选应该子句的最小数量。</li>
</ol>
<p>Here is an example with the <code>operator</code> parameter:</p>
<pre><code class="language-console">GET /_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;message&quot;: {
        &quot;query&quot;: &quot;this is a test&quot;,
        &quot;operator&quot;: &quot;and&quot;
      }
    }
  }
}
</code></pre>
<p>可以设置分析器来控制哪个分析器将对文本执行分析过程。</p>
<p>它默认为字段显式映射定义或默认搜索分析器。</p>
<p>可以将<em>lenient</em>参数设置为true，以忽略由数据类型不匹配引起的异常，例如尝试使用文本查询字符串查询数字字段。默认为false。</p>
<h4 id="fuzziness-in-the-match-query"><a class="header" href="#fuzziness-in-the-match-query">Fuzziness in the match query</a></h4>
<p><code>fuzziness</code> allows <em>fuzzy matching</em> based on the type of field being queried. See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/common-options.html#fuzziness">Fuzziness</a> for allowed settings.</p>
<p>The <code>prefix_length</code> and <code>max_expansions</code> can be set in this case to control the fuzzy process.</p>
<p>If the fuzzy option is set the query will use <code>top_terms_blended_freqs_${max_expansions}</code> as its <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.17/query-dsl-multi-term-rewrite.html">rewrite method</a> the <code>fuzzy_rewrite</code> parameter allows to control how the query will get rewritten.</p>
<p>Fuzzy transpositions (<code>ab</code> → <code>ba</code>) are allowed by default but can be disabled by setting <code>fuzzy_transpositions</code> to <code>false</code>.</p>
<p>Fuzzy matching is not applied to terms with synonyms or in cases where the analysis process produces multiple tokens at the same position. Under the hood these terms are expanded to a special synonym query that blends term frequencies, which does not support fuzzy expansion.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="snapshot-and-restore"><a class="header" href="#snapshot-and-restore">Snapshot and restore</a></h2>
<ol>
<li>
<p>快照是从正在运行的Elasticsearch集群获取的备份。</p>
</li>
<li>
<p>您可以拍摄整个集群的快照，包括其所有数据流和索引。</p>
</li>
<li>
<p>您也可以仅对集群中的特定数据流或索引进行快照。</p>
</li>
</ol>
<h3 id="注册快照仓库"><a class="header" href="#注册快照仓库">注册快照仓库</a></h3>
<ol>
<li>
<p>You must <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html">register a snapshot repository</a> before you can <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-take-snapshot.html">create snapshots</a>.</p>
</li>
<li>
<p>快照可以存储在本地或远程存储库中</p>
</li>
<li>
<p>Remote repositories can reside on Amazon S3, HDFS, Microsoft Azure, Google Cloud Storage, and other platforms supported by a <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository.html">repository plugin</a></p>
</li>
</ol>
<p><strong>Elasticsearch会增量获取快照:</strong> </p>
<ol>
<li>可以安全地以最小的开销非常频繁地拍摄快照</li>
<li>这种增量仅适用于单个存储库。因为存储库之间没有共享数据</li>
<li>快照在逻辑上也彼此独立，即使在单个存储库中也是如此: 删除快照不会影响任何其他快照的完整性。</li>
<li>但是，您可以选择仅从快照中恢复群集状态或特定数据流或索引。</li>
<li>You can use <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/getting-started-snapshot-lifecycle-management.html">snapshot lifecycle management</a> to automatically take and manage snapshots.</li>
</ol>
<p>备份群集的唯一可靠且受支持的方法是拍摄快照。您不能通过复制Elasticsearch集群节点的数据目录来备份它。没有支持的方法可以从文件系统级备份中还原任何数据。如果您尝试从这样的备份中恢复群集，它可能会因损坏或丢失文件或其他数据不一致的报告而失败，或者它似乎已经成功地无声地丢失了一些数据。</p>
<p>群集节点的数据目录的副本不能用作备份，因为它不是它们在单个时间点的内容的一致表示。您不能通过在复制时关闭节点来解决这个问题，也不能通过获取原子文件系统级快照来解决这个问题，因为Elasticsearch具有跨越整个集群的一致性要求。集群备份必须使用内置的快照功能。</p>
<h3 id="version-compatibility"><a class="header" href="#version-compatibility">Version compatibility</a></h3>
<p>版本兼容性是指基础Lucene索引兼容性。在版本之间迁移时，请遵循<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-restore.html#:%7E:text=Upgrade%20documentation">升级文档。</a></p>
<p>快照包含包含索引或数据流的备份索引的磁盘上数据结构的副本。这意味着快照只能恢复到可以读取索引的Elasticsearch版本。</p>
<p>快照包含包含索引或数据流的后备索引的磁盘上数据结构的副本。这意味着快照只能恢复到可以读取索引的Elasticsearch版本。</p>
<p>下表显示了版本之间的快照兼容性。第一列表示可以从中恢复快照的基本版本。</p>
<table><thead><tr><th><strong>Cluster version</strong></th><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td><strong>Snapshot version</strong></td><td>2.x</td><td>5.x</td><td>6.x</td><td>7.x</td></tr>
<tr><td><strong>1.x</strong> →</td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td></tr>
<tr><td><strong>2.x</strong> →</td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td></tr>
<tr><td><strong>5.x</strong> →</td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td></tr>
<tr><td><strong>6.x</strong> →</td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td></tr>
<tr><td><strong>7.x</strong> →</td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-no.png" alt="No" /></td><td><img src="https://doc-icons.s3.us-east-2.amazonaws.com/icon-yes.png" alt="Yes" /></td></tr>
</tbody></table>
<p>总结：</p>
<p>快照只能向前兼容一个majar版本，没法向后兼容</p>
<p>以下条件适用于跨版本恢复快照和索引: •</p>
<ul>
<li><strong>Snapshots</strong>: 您不能将Elasticsearch版本的快照恢复到运行早期Elasticsearch版本的集群中。例如，您无法将7.6.0中拍摄的快照还原到运行7.5.0的群集。</li>
<li><strong>Indices</strong>: 您不能将索引还原到运行Elasticsearch版本的集群中，该版本比用于快照索引的Elasticsearch版本更新了多个 marjar verson。例如，无法将索引从5.0中获取的快照还原到运行7.0的群集。</li>
</ul>
<p>需要注意的是，Elasticsearch 2.0拍摄的快照可以在运行Elasticsearch 5.0的集群中恢复。</p>
<p>每个快照都可以包含在各种版本的Elasticsearch中创建的索引。这包括为数据流创建的支持索引。还原快照时，必须可以将所有这些索引还原到目标群集中。如果快照中的任何索引都是在不兼容的版本中创建的，则将无法还原快照。</p>
<p><strong>注意</strong></p>
<ol>
<li>在升级前备份数据时，记住 如果升级后的版本不兼容，则不能还原快照</li>
<li>如果实在需要 还原到 不兼容的版本，可以先还原到 最近兼容版本，使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docs-reindex.html#reindex-from-remote">reindex-from-remote</a> 去重建数据流或者索引到 当前的版本</li>
<li>获取数据 然后 重新索引 会比 快照恢复 更加慢，建议操作之前 先计算 消耗时长</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="clean-up-snapshot-repository-api"><a class="header" href="#clean-up-snapshot-repository-api">Clean up snapshot repository API</a></h2>
<p>触发对快照存储库内容的审查，并删除现有快照未引用的所有陈旧数据。</p>
<pre><code>POST /_snapshot/my_repository/_cleanup
</code></pre>
<h3 id="request-3"><a class="header" href="#request-3">Request</a></h3>
<pre><code>POST /_snapshot/&lt;repository&gt;/_cleanup
</code></pre>
<h3 id="description-2"><a class="header" href="#description-2">Description</a></h3>
<p>随着时间的推移，快照存储库可能会累积不再被现有快照引用的陈旧数据。</p>
<p>尽管此未引用的数据不会对快照存储库的性能或安全性产生负面影响，但它可能导致比必要时更多的存储使用。</p>
<p>您可以使用 “清理快照存储库” API来检测和删除此未引用的数据。</p>
<p>小提示</p>
<ol>
<li>当从存储库中删除快照时，此API执行的大多数清理操作都是自动执行的。</li>
<li>如果您定期删除快照，调用此API可能只会稍微减少您的存储空间，或者根本不会减少。</li>
</ol>
<h3 id="path-parameters-3"><a class="header" href="#path-parameters-3">Path parameters</a></h3>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) Name of the snapshot repository to review and clean up.</p>
<h3 id="query-parameters-3"><a class="header" href="#query-parameters-3">Query parameters</a></h3>
<ul>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
</ul>
<h3 id="response-body-1"><a class="header" href="#response-body-1">Response body</a></h3>
<h4 id="results"><a class="header" href="#results"><strong>results</strong></a></h4>
<h5 id="deleted_bytes"><a class="header" href="#deleted_bytes"><strong>deleted_bytes</strong></a></h5>
<p>(整数) 通过清理操作释放的字节数。</p>
<p><strong>deleted_blobs</strong></p>
<p>(整数) 在清理操作期间从快照存储库中删除的二进制大对象 (blob) 的数量。任何非零值都意味着发现了未引用的blobs并随后进行了清理。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="clone-snapshot-api"><a class="header" href="#clone-snapshot-api">Clone snapshot API</a></h2>
<p>Clones part or all of a snapshot into a new snapshot.</p>
<pre><code class="language-console">PUT /_snapshot/my_repository/source_snapshot/_clone/target_snapshot
{
  &quot;indices&quot;: &quot;index_a,index_b&quot;
}
</code></pre>
<h3 id="request-4"><a class="header" href="#request-4">Request</a></h3>
<pre><code>PUT /_snapshot/&lt;repository&gt;/&lt;source_snapshot&gt;/_clone/&lt;target_snapshot&gt;
</code></pre>
<h3 id="description-3"><a class="header" href="#description-3">Description</a></h3>
<p>clone snapshot API允许在同一存储库中创建全部或部分现有快照的副本。</p>
<h3 id="path-parameters-4"><a class="header" href="#path-parameters-4">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) Name of the snapshot repository that both source and target snapshot belong to.</p>
</li>
</ul>
<h3 id="query-parameters-4"><a class="header" href="#query-parameters-4">Query parameters</a></h3>
<ul>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Specifies the period of time to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Specifies the period of time to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>indices</code></strong></p>
<p>(Required, string) A comma-separated list of indices to include in the snapshot. <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/multi-index.html">Multi-index syntax</a> is supported.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="create-or-update-snapshot-repository-api"><a class="header" href="#create-or-update-snapshot-repository-api">Create or update snapshot repository API</a></h2>
<p>Registers or updates a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html">snapshot repository</a>.</p>
<pre><code class="language-console">PUT /_snapshot/my_repository
{
  &quot;type&quot;: &quot;fs&quot;,
  &quot;settings&quot;: {
    &quot;location&quot;: &quot;my_backup_location&quot;
  }
}
</code></pre>
<h3 id="request-5"><a class="header" href="#request-5">Request</a></h3>
<pre><code>PUT /_snapshot/&lt;repository&gt;
POST /_snapshot/&lt;repository&gt;
</code></pre>
<h3 id="description-4"><a class="header" href="#description-4">Description</a></h3>
<p>必须先注册 repository 才能执行 快照跟恢复操作 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-restore.html">snapshot and restore</a> </p>
<p>快照格式每个大版本都会变，详见 See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-restore.html#snapshot-restore-version-compatibility">Version compatibility</a>.</p>
<h3 id="path-parameters-5"><a class="header" href="#path-parameters-5">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) Name of the snapshot repository to register or update.</p>
</li>
</ul>
<h3 id="query-parameters-5"><a class="header" href="#query-parameters-5">Query parameters</a></h3>
<p>注意：可以使用 query parameter 或 request body parameter 指定此API的多个选项。如果两个参数都指定，则仅使用查询参数。</p>
<ul>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Specifies the period of time to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Specifies the period of time to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>verify</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, the request verifies the repository is functional on all master and data nodes in the cluster. If <code>false</code>, this verification is skipped. Defaults to <code>true</code>.You can manually perform this verification using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/verify-snapshot-repo-api.html">verify snapshot repository API</a>.</p>
</li>
</ul>
<h3 id="request-body-2"><a class="header" href="#request-body-2">Request body</a></h3>
<h4 id="type-1"><a class="header" href="#type-1"><strong>type</strong></a></h4>
<p>快照仓库类型</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-s3.html">repository-s3</a> for S3 repository support</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-hdfs.html">repository-hdfs</a> for HDFS repository support in Hadoop environments</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-azure.html">repository-azure</a> for Azure storage repositories</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-gcs.html">repository-gcs</a> for Google Cloud Storage repositories</li>
</ul>
<h4 id="settings-1"><a class="header" href="#settings-1">settings</a></h4>
<p>仓库容器配置</p>
<h5 id="chunk_size"><a class="header" href="#chunk_size"><strong>chunk_size</strong></a></h5>
<ol>
<li>文件块大小：字节数</li>
<li>如果快照比这个数值大。则快照会被拆分成几个小文件</li>
<li>默认为空，没有限制</li>
</ol>
<h5 id="compress"><a class="header" href="#compress"><strong>compress</strong></a></h5>
<ol>
<li>如果为TRUE 则 metadata files 例如 索引 mappings settings 会被压缩存储。索引数据不会压缩。</li>
<li>默认TRUE</li>
</ol>
<h5 id="max_number_of_snapshots"><a class="header" href="#max_number_of_snapshots"><strong>max_number_of_snapshots</strong></a></h5>
<ol>
<li>仓库最大 快照数</li>
<li>默认500</li>
<li>不建议增大这个值。因为 过大的快照仓库 会 影响 主节点的性能。带来稳定性问题。</li>
<li>相反，删除旧的快照或者使用 多仓库</li>
</ol>
<h5 id="max_restore_bytes_per_sec"><a class="header" href="#max_restore_bytes_per_sec"><strong>max_restore_bytes_per_sec</strong></a></h5>
<ol>
<li>最大的每秒 恢复速度。字节值</li>
<li>默认无限制。</li>
<li>restore 也会受到 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/put-snapshot-repo-api.html#:%7E:text=also%20throttled%20through-,recovery%20settings,-.">recovery settings</a>影响</li>
</ol>
<h5 id="max_snapshot_bytes_per_sec"><a class="header" href="#max_snapshot_bytes_per_sec"><strong>max_snapshot_bytes_per_sec</strong></a></h5>
<ol>
<li>最大每秒 快照取样时间。单位字节</li>
<li>默认40M/s</li>
</ol>
<h5 id="readonly"><a class="header" href="#readonly"><strong>readonly</strong></a></h5>
<ol>
<li>为TRUE 则 仓库是只读的</li>
<li>只能读快照 不能写快照</li>
<li>默认为 FALSE</li>
<li>如果多集群中注册了同一个快照存储，则只有一个集群具有对该存储库的写访问 权限，让多个集群同时写入存储库有损坏存储库内容的风险。</li>
<li>只有具有写访问权限的集群才能在存储库中创建快照。连接到存储库的所有其他群集都应将readonly参数设置为true。这意味着这些群集可以从存储库中检索或还原快照，但不能在其中创建快照。</li>
</ol>
<p>其他可接受的设置属性取决于使用type参数设置的存储库类型。</p>
<h4 id="fs-repo-settings"><a class="header" href="#fs-repo-settings">FS repo settings</a></h4>
<h5 id="location"><a class="header" href="#location"><strong>location</strong></a></h5>
<ol>
<li>必选。本地文件系统的位置</li>
<li>必须注册在 主节点或者数据节点的 path.repo 设置的路径</li>
</ol>
<h4 id="source-repo-settings"><a class="header" href="#source-repo-settings">source repo settings</a></h4>
<h5 id="delegate_type"><a class="header" href="#delegate_type"><strong>delegate_type</strong></a></h5>
<ol>
<li>代码存储库类型</li>
<li>source repositories 可以使用 代理存储库的 配置</li>
<li>详见：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html#snapshots-source-only-repository">source ONLY repository</a></li>
</ol>
<h4 id="url-repo-settings"><a class="header" href="#url-repo-settings">URL repo settings</a></h4>
<ol>
<li>基于URL的 共享文件系统 repository 根路径</li>
<li>支持以下格式
<ol>
<li>file</li>
<li>ftp</li>
<li>http</li>
<li>https</li>
<li>jar</li>
</ol>
</li>
<li>使用文件协议的url必须指向群集中所有主节点和数据节点都可以访问的共享文件系统的位置。此位置必须在path.repo设置中注册。</li>
<li>必须通过<em>repositoriesurl.allowed_urls</em>设置明确允许使用http、https或ftp协议的url。此设置支持在URL中代替主机、路径、查询或片段的通配符。</li>
</ol>
<h4 id="verify"><a class="header" href="#verify"><strong>verify</strong></a></h4>
<ol>
<li>检验储存库是否在所有 主节点或者数据节点 起作用</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h3 id="create-snapshot-api"><a class="header" href="#create-snapshot-api">Create snapshot API</a></h3>
<pre><code>PUT /_snapshot/my_repository/my_snapshot

</code></pre>
<h3 id="request-6"><a class="header" href="#request-6">Request</a></h3>
<pre><code>PUT /_snapshot/&lt;repository&gt;/&lt;snapshot&gt;
POST /_snapshot/&lt;repository&gt;/&lt;snapshot&gt;
</code></pre>
<h3 id="description-5"><a class="header" href="#description-5">Description</a></h3>
<p>您可以使用create snapshot API创建快照，该快照是从正在运行的Elasticsearch集群中获取的备份。</p>
<p>默认情况下，快照包括集群中的所有数据流和开放索引，以及集群状态。您可以通过在快照请求的request body 中 指定要备份的数据流和索引列表来更改此行为。</p>
<p>注意必须先注册 快照仓库</p>
<p>快照是增量的</p>
<p>快照过程以非阻塞方式执行，因此所有索引和搜索操作都可以在 正在快照的数据流或索引同时运行。</p>
<p>快照表示创建快照的时刻的时间点视图。快照过程开始后，没有添加到数据流或索引的记录将出现在快照中。</p>
<ol>
<li>
<p>对于尚未启动且当前未重新定位的主分片，快照过程将立即启动。</p>
</li>
<li>
<p>如果分片正在启动或重新定位，Elasticsearch会在拍摄快照之前等待这些进程完成。</p>
</li>
<li>
<p>重要：</p>
<ol>
<li>拍摄快照期间。分片移动到其他节点</li>
<li>分配重路由、重定位 会被快照 干扰知道快照过程结束</li>
<li>除了copy数据之外。还copy集群元配置、包括索引模板、组件模板、集群持久化配置 、</li>
<li>瞬时配置 和 注册的 快照仓库 配置 不会快照</li>
</ol>
</li>
</ol>
<h3 id="query-parameters-6"><a class="header" href="#query-parameters-6">Query parameters</a></h3>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<p><strong><code>wait_for_completion</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, the request returns a response when the snapshot is complete. If <code>false</code>, the request returns a response when the snapshot initializes. Defaults to <code>false</code>.</p>
<h3 id="request-body-3"><a class="header" href="#request-body-3">Request body</a></h3>
<h4 id="ignore_unavailable"><a class="header" href="#ignore_unavailable"><strong>ignore_unavailable</strong></a></h4>
<ol>
<li>是否忽略 不可用索引</li>
<li>如果 数据流或者 索引 被关闭了。或者丢失了。则该快照过程失败</li>
<li>默认 FALSE，不失败</li>
</ol>
<h3 id="indices"><a class="header" href="#indices"><strong>indices</strong></a></h3>
<ol>
<li>逗号分割的 索引列表</li>
<li>详见：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/multi-index.html">Multi-index syntax</a></li>
<li>快照默认包含集群中的所有数据流和索引。如果提供此参数，则快照仅包含指定的数据流和群集。</li>
</ol>
<h3 id="include_global_state"><a class="header" href="#include_global_state"><strong>include_global_state</strong></a></h3>
<ol>
<li>包含当前集群的全局状态</li>
<li>默认TRUE</li>
<li>全局状态包括
<ol>
<li>Persistent cluster settings</li>
<li>Index templates、Legacy index templates</li>
<li>Ingest pipelines</li>
<li>ILM lifecycle policies</li>
<li>Data stored in system indices, such as Watches and task records (configurable via feature_states)</li>
</ol>
</li>
</ol>
<h3 id="feature_states"><a class="header" href="#feature_states"><strong>feature_states</strong></a></h3>
<ol>
<li>可选的 数组string</li>
<li>快照一些列的特点。</li>
<li>可以使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/get-features-api.html">get features API</a> 得到 这些描述</li>
<li>每个特征状态包括一个或多个系统索引
<ol>
<li>每个特征状态包括一个或多个系统索引，</li>
<li>其中包含该特征的功能所需的数据。</li>
<li>提供空数组将在快照中不包含任何特征状态，</li>
<li>而不管include_global_state的值如何。</li>
<li>默认的。所有可用的 特征 状态 将会被 快照</li>
<li>如果 include_global_state 设置为TRUE。则会包含所有的。设置为FALSE 则 不包含任何</li>
</ol>
</li>
</ol>
<h3 id="metadata"><a class="header" href="#metadata"><strong>metadata</strong></a></h3>
<ol>
<li>附加自定义元数据。例如 谁建立的快照、为什么建立快照 。必须小于 1024个字节</li>
</ol>
<h3 id="partial"><a class="header" href="#partial"><strong>partial</strong></a></h3>
<ol>
<li>如果设置成FALSE 。一旦 当中某些索引 存在 主分片不可用的情况 则 快照失败</li>
<li>设置为TRUE表示 可以 接受部分 快照</li>
</ol>
<h3 id="example-1"><a class="header" href="#example-1">example</a></h3>
<pre><code class="language-console">PUT /_snapshot/my_repository/snapshot_2?wait_for_completion=true
{
  &quot;indices&quot;: &quot;index_1,index_2&quot;,
  &quot;ignore_unavailable&quot;: true,
  &quot;include_global_state&quot;: false,
  &quot;metadata&quot;: {
    &quot;taken_by&quot;: &quot;user123&quot;,
    &quot;taken_because&quot;: &quot;backup before upgrading&quot;
  }
}
</code></pre>
<pre><code>{
  &quot;snapshot&quot;: {
    &quot;snapshot&quot;: &quot;snapshot_2&quot;,
    &quot;uuid&quot;: &quot;vdRctLCxSketdKb54xw67g&quot;,
    &quot;version_id&quot;: &lt;version_id&gt;,
    &quot;version&quot;: &lt;version&gt;,
    &quot;indices&quot;: [],
    &quot;data_streams&quot;: [],
    &quot;feature_states&quot;: [],
    &quot;include_global_state&quot;: false,
    &quot;metadata&quot;: {
      &quot;taken_by&quot;: &quot;user123&quot;,
      &quot;taken_because&quot;: &quot;backup before upgrading&quot;
    },
    &quot;state&quot;: &quot;SUCCESS&quot;,
    &quot;start_time&quot;: &quot;2020-06-25T14:00:28.850Z&quot;,
    &quot;start_time_in_millis&quot;: 1593093628850,
    &quot;end_time&quot;: &quot;2020-06-25T14:00:28.850Z&quot;,
    &quot;end_time_in_millis&quot;: 1593094752018,
    &quot;duration_in_millis&quot;: 0,
    &quot;failures&quot;: [],
    &quot;shards&quot;: {
      &quot;total&quot;: 0,
      &quot;failed&quot;: 0,
      &quot;successful&quot;: 0
    }
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="delete-snapshot-api"><a class="header" href="#delete-snapshot-api">Delete snapshot API</a></h2>
<pre><code>DELETE /_snapshot/my_repository/my_snapshot

</code></pre>
<h3 id="request-7"><a class="header" href="#request-7">Request</a></h3>
<pre><code>DELETE /_snapshot/&lt;repository&gt;/&lt;snapshot&gt;
</code></pre>
<h3 id="description-6"><a class="header" href="#description-6">Description</a></h3>
<ol>
<li>使用删除快照API删除快照，快照是从运行中的Elasticsearch集群获取的备份。</li>
<li>从存储库中删除快照时，Elasticsearch会删除与该快照关联且未被任何其他快照使用的所有文件。与至少一个其他现有快照共享的所有文件都保持不变。</li>
<li>如果在创建快照时尝试删除快照，快照过程将中止，并且所有关联的快照都将被删除。</li>
<li>要在单个请求中删除多个快照，请使用逗号分隔快照名称或使用通配符 (*)。提示</li>
<li>使用删除快照API取消错误启动的长时间运行的快照操作。</li>
</ol>
<h3 id="path-parameters-6"><a class="header" href="#path-parameters-6">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) Name of the repository to delete a snapshot from.</p>
</li>
<li>
<p><strong><code>&lt;snapshot&gt;</code></strong></p>
<p>(Required, string) Comma-separated list of snapshot names to delete. Also accepts wildcards (<code>*</code>).</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="delete-snapshot-repository-api"><a class="header" href="#delete-snapshot-repository-api">Delete snapshot repository API</a></h2>
<p>Unregisters one or more <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html">snapshot repositories</a>.</p>
<p>当取消注册存储库时，Elasticsearch仅删除对存储库存储快照的位置的引用。快照本身保持不变。</p>
<pre><code>DELETE /_snapshot/my_repository
</code></pre>
<h3 id="request-8"><a class="header" href="#request-8">Request</a></h3>
<pre><code>DELETE /_snapshot/&lt;repository&gt;
</code></pre>
<h3 id="path-parameters-7"><a class="header" href="#path-parameters-7">Path parameters</a></h3>
<p>(Required, string) Name of the snapshot repository to unregister. Wildcard (*) patterns are supported.</p>
<h3 id="query-parameters-7"><a class="header" href="#query-parameters-7">Query parameters</a></h3>
<ul>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Specifies the period of time to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Specifies the period of time to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h3 id="get-snapshot-api"><a class="header" href="#get-snapshot-api">Get snapshot API</a></h3>
<pre><code>GET /_snapshot/my_repository/my_snapshot
</code></pre>
<h3 id="request-9"><a class="header" href="#request-9">Request</a></h3>
<pre><code>GET /_snapshot/&lt;repository&gt;/&lt;snapshot&gt;
</code></pre>
<h3 id="description-7"><a class="header" href="#description-7">Description</a></h3>
<p>使用get snapshot API返回有关一个或多个快照的信息，包括: •</p>
<ul>
<li>Start and end time values</li>
<li>Version of Elasticsearch that created the snapshot</li>
<li>List of included indices</li>
<li>Current state of the snapshot</li>
<li>List of failures that occurred during the snapshot</li>
</ul>
<h3 id="path-parameters-8"><a class="header" href="#path-parameters-8">Path parameters</a></h3>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) Snapshot repository name used to limit the request</p>
<p><strong><code>&lt;snapshot&gt;</code></strong></p>
<p>(Required, string) Comma-separated list of snapshot names to retrieve. Also accepts wildcards (<code>*</code>).</p>
<ul>
<li>To get information about all snapshots in a registered repository, use a wildcard (<code>*</code>) or <code>_all</code>.</li>
<li>To get information about any snapshots that are currently running, use <code>_current</code>.</li>
</ul>
<p>Using _all in a request fails if any snapshots are unavailable. Set ignore_unavailable to true to return only available snapshots.</p>
<h3 id="query-parameters-8"><a class="header" href="#query-parameters-8">Query parameters</a></h3>
<h4 id="master_timeout"><a class="header" href="#master_timeout"><strong>master_timeout</strong></a></h4>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a></li>
<li>等待主节点连接的超时时长</li>
<li>如果指定超时时间没有返回。则失败。</li>
<li>默认30是</li>
</ol>
<h4 id="ignore_unavailable-1"><a class="header" href="#ignore_unavailable-1"><strong>ignore_unavailable</strong></a></h4>
<ol>
<li>部分快照不可用。任然可以返回</li>
<li>默认FALSE</li>
</ol>
<h4 id="verbose"><a class="header" href="#verbose"><strong>verbose</strong></a></h4>
<ol>
<li>返回额外信息：拍摄快照时的集群版本</li>
<li>快照开始结束时间</li>
<li>快照的分片数量</li>
<li>默认TRUE。如果是FALSE 则忽略额外信息</li>
</ol>
<h4 id="index_details"><a class="header" href="#index_details"><strong>index_details</strong></a></h4>
<ol>
<li>返回索引的详细信息</li>
<li>包括索引中的分片数，索引的总大小 (以字节为单位) 以及索引中每个分片的最大段数。默认为false，意味着此信息被省略。</li>
</ol>
<h3 id="response-body-2"><a class="header" href="#response-body-2">Response body</a></h3>
<h4 id="snapshot"><a class="header" href="#snapshot"><strong>snapshot</strong></a></h4>
<p>(string) Name of the snapshot.</p>
<h4 id="uuid"><a class="header" href="#uuid"><strong><code>uuid</code></strong></a></h4>
<p>(string) Universally unique identifier (UUID) of the snapshot.</p>
<h4 id="version_id"><a class="header" href="#version_id"><strong>version_id</strong></a></h4>
<p>ES 版本ID</p>
<h4 id="version"><a class="header" href="#version">VERSION</a></h4>
<p>ES版本</p>
<h4 id="indices-1"><a class="header" href="#indices-1"><strong>indices</strong></a></h4>
<p>快照索引</p>
<h4 id="index_details-1"><a class="header" href="#index_details-1"><strong>index_details</strong></a></h4>
<p>快照索引明细</p>
<ol>
<li>shard_count:索引分片数量</li>
<li><strong>size</strong>:分片总大小。当human 参数设置时才有</li>
<li><strong>size_in_bytes</strong>:分片总大小。当human 参数设置时才有</li>
<li><strong>max_segments_per_shard</strong>:
<ol>
<li>当前索引 快照的 最大 段数</li>
</ol>
</li>
</ol>
<h3 id="data_streams"><a class="header" href="#data_streams"><strong>data_streams</strong></a></h3>
<p>包含的dataStream</p>
<h4 id="include_global_state-1"><a class="header" href="#include_global_state-1"><strong>include_global_state</strong></a></h4>
<p>是否包含全局状态</p>
<p><strong>feature_states</strong></p>
<p>指定的全局状态</p>
<h4 id="start_time"><a class="header" href="#start_time"><strong>start_time</strong></a></h4>
<h4 id="start_time_in_millis"><a class="header" href="#start_time_in_millis"><strong>start_time_in_millis</strong></a></h4>
<h4 id="end_time"><a class="header" href="#end_time"><strong>end_time</strong></a></h4>
<h4 id="end_time_in_millis"><a class="header" href="#end_time_in_millis"><strong>end_time_in_millis</strong></a></h4>
<h4 id="duration_in_millis"><a class="header" href="#duration_in_millis"><strong>duration_in_millis</strong></a></h4>
<h4 id="failures"><a class="header" href="#failures"><strong>failures</strong></a></h4>
<h4 id="shards"><a class="header" href="#shards"><strong>shards</strong></a></h4>
<p>分片信息</p>
<h5 id="total"><a class="header" href="#total">`<strong>total</strong></a></h5>
<p>快照`中最多 分片数</p>
<p><strong><code>successful</code></strong></p>
<p>(integer) Number of shards that were successfully included in the snapshot.</p>
<p><strong><code>failed</code></strong></p>
<p>(integer) Number of shards that failed to be included in the snapshot. </p>
<h3 id="state"><a class="header" href="#state"><strong>state</strong></a></h3>
<h4 id="in_progress"><a class="header" href="#in_progress"><strong>IN_PROGRESS</strong></a></h4>
<p>运行中</p>
<h4 id="success"><a class="header" href="#success"><strong>SUCCESS</strong></a></h4>
<p>运行成功</p>
<h4 id="failed"><a class="header" href="#failed"><strong>FAILED</strong></a></h4>
<p>失败</p>
<h3 id="partial-1"><a class="header" href="#partial-1"><strong><code>PARTIAL</code></strong></a></h3>
<p>部分失败</p>
<h3 id="examples-2"><a class="header" href="#examples-2">Examples</a></h3>
<pre><code class="language-console">GET /_snapshot/my_repository/snapshot_2
</code></pre>
<pre><code class="language-console-result">{
  &quot;snapshots&quot;: [
    {
      &quot;snapshot&quot;: &quot;snapshot_2&quot;,
      &quot;uuid&quot;: &quot;vdRctLCxSketdKb54xw67g&quot;,
      &quot;version_id&quot;: &lt;version_id&gt;,
      &quot;version&quot;: &lt;version&gt;,
      &quot;indices&quot;: [],
      &quot;data_streams&quot;: [],
      &quot;feature_states&quot;: [],
      &quot;include_global_state&quot;: true,
      &quot;state&quot;: &quot;SUCCESS&quot;,
      &quot;start_time&quot;: &quot;2020-07-06T21:55:18.129Z&quot;,
      &quot;start_time_in_millis&quot;: 1593093628850,
      &quot;end_time&quot;: &quot;2020-07-06T21:55:18.876Z&quot;,
      &quot;end_time_in_millis&quot;: 1593094752018,
      &quot;duration_in_millis&quot;: 0,
      &quot;failures&quot;: [],
      &quot;shards&quot;: {
        &quot;total&quot;: 0,
        &quot;failed&quot;: 0,
        &quot;successful&quot;: 0
      }
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="register-a-snapshot-repository"><a class="header" href="#register-a-snapshot-repository">Register a snapshot repository</a></h2>
<ol>
<li>必须先 注册 快照仓库</li>
<li>Use the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/put-snapshot-repo-api.html">create or update snapshot repository API</a> to register or update a snapshot repository. </li>
<li>We recommend creating a new snapshot repository for each major version. The valid repository settings depend on the repository type.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="restore-snapshot-api"><a class="header" href="#restore-snapshot-api">Restore snapshot API</a></h2>
<pre><code class="language-console">POST /_snapshot/my_repository/my_snapshot/_restore
</code></pre>
<h3 id="request-10"><a class="header" href="#request-10">Request</a></h3>
<pre><code>POST /_snapshot/&lt;repository&gt;/&lt;snapshot&gt;/_restore
</code></pre>
<h3 id="description-8"><a class="header" href="#description-8">Description</a></h3>
<ol>
<li>使用restore snapshot API恢复集群的快照，包括快照中的所有数据流和索引。</li>
<li>如果您不想恢复整个快照，可以选择要恢复的特定数据流或索引。</li>
</ol>
<p><strong>恢复的条件</strong></p>
<ol>
<li>
<p>您可以在 </p>
<ol>
<li>包含所选主节点</li>
<li>并 具有足够容量以容纳要还原的快照的数据节点的 </li>
</ol>
<p>群集上运行还原操作。</p>
</li>
<li>
<p>仅当现有索引已关闭并且与快照中的索引具有相同数量的分片时，才能恢复它们。</p>
</li>
<li>
<p>如果它们已关闭：还原操作会自动打开已恢复的索引</p>
</li>
<li>
<p>如果它们在群集中不存在：则会创建新的索引 </p>
</li>
</ol>
<p><strong>数据流的恢复</strong></p>
<ol>
<li>
<p>如果恢复了数据流，则其后备索引也将恢复。</p>
</li>
<li>
<p>或者，您可以在不恢复整个数据流的情况下恢复单个备份索引。</p>
</li>
<li>
<p>如果您恢复单个备份索引，它们不会自动添加到任何现有数据流中。</p>
</li>
<li>
<p>例如，如果只有。ds-logs-2099.03.08-00003备份索引从快照中恢复，则不会自动将其添加到现有日志数据流中。</p>
</li>
</ol>
<p>重要点</p>
<ol>
<li><code>index_settings</code> and <code>ignore_index_settings</code> 这两个参数只影响 数据流的后备索引</li>
<li>新的后备索引使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/set-up-a-data-stream.html#create-index-template">index template</a>. 指定的设置</li>
<li>如果在 restore过程中改变了 索引设置 推荐 最好对  index template 也做同样的配置，这确保 新的后备索引 保持同样的 设置</li>
</ol>
<p>​	</p>
<h3 id="path-parameters-9"><a class="header" href="#path-parameters-9">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) Name of the repository to restore a snapshot from.</p>
</li>
<li>
<p><strong><code>&lt;snapshot&gt;</code></strong></p>
<p>(Required, string) Name of the snapshot to restore.</p>
</li>
</ul>
<h3 id="query-parameters-9"><a class="header" href="#query-parameters-9">Query parameters</a></h3>
<ul>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>wait_for_completion</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, the request returns a response when the restore operation completes. The operation is complete when it finishes all attempts to <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-monitor-snapshot-restore.html#_monitoring_restore_operations">recover primary shards</a> for restored indices. This applies even if one or more of the recovery attempts fail.If <code>false</code>, the request returns a response when the restore operation initializes. Defaults to <code>false</code>.</p>
</li>
</ul>
<h3 id="request-body-4"><a class="header" href="#request-body-4">Request body</a></h3>
<h4 id="ignore_unavailable-2"><a class="header" href="#ignore_unavailable-2"><strong>ignore_unavailable</strong></a></h4>
<ol>
<li>设置为FALSE。则如果索引或者数据流缺失或者关闭的情况下该请求会报错</li>
</ol>
<h4 id="ignore_unavailable-3"><a class="header" href="#ignore_unavailable-3"><strong>ignore_unavailable</strong></a></h4>
<ol>
<li>(可选，字符串) 以逗号分隔的索引设置列表，不应从快照中还原。</li>
</ol>
<h4 id="ignore_index_settings"><a class="header" href="#ignore_index_settings"><strong>ignore_index_settings</strong></a></h4>
<ol>
<li>不应该还原的配置</li>
<li>逗号分割的key</li>
</ol>
<h4 id="include_aliases"><a class="header" href="#include_aliases"><strong><code>include_aliases</code></strong></a></h4>
<ol>
<li>是否恢复 索引别名</li>
<li>默认TRUE</li>
</ol>
<h4 id="include_global_state-2"><a class="header" href="#include_global_state-2"><strong>include_global_state</strong></a></h4>
<ol>
<li>是否还原 全局配置。默认FALSE不还原</li>
<li>如果为TRUE 则以下几个状态会被还原
<ol>
<li>Persistent cluster settings</li>
<li>Index templates</li>
<li>Legacy index templates</li>
<li>Ingest pipelines</li>
<li>ILM lifecycle policies</li>
<li>For snapshots taken after 7.12.0, data stored in system indices, such as Watches and task records, replacing any existing configuration (configurable via <code>feature_states</code>)</li>
</ol>
</li>
</ol>
<p><strong>还原细节</strong></p>
<p>如果include_global_state为true，则还原操作将群集中的Legacy index templates与快照中包含的模板合并，并替换名称与快照中的模板匹配的任何现有模板。它完全删除了集群中存在的所有持久性设置，非传统索引模板，摄取管道和ILM生命周期策略，并将其替换为快照中的相应项目。</p>
<h4 id="feature_states-1"><a class="header" href="#feature_states-1"><strong><code>feature_states</code></strong></a></h4>
<ol>
<li>可选的。逗号分隔的字符串</li>
<li>指定 还原的 状态</li>
<li>每一个  feature state  包含 系统索引状态</li>
<li>通过  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/get-snapshot-api.html#get-snapshot-api-feature-states">Get Snapshot API</a>.  可以获取 feature states</li>
<li>feature states 会直接覆盖</li>
<li>空数组 则导致 不会恢复  feature states, 从而就忽略 <code>include_global_state</code></li>
<li>默认下，当include_global_state 设置成 TRUE表示 恢复所有的。设置为FALSE 表示都不恢复</li>
</ol>
<h4 id="index_settings"><a class="header" href="#index_settings"><strong>index_settings</strong></a></h4>
<ol>
<li>逗号分割的 设置列表</li>
<li>可以用来 新增或者修改 所有索引的配置</li>
<li>对于 索引级别的配置 详见 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html">index modules</a>.</li>
</ol>
<h4 id="indices-2"><a class="header" href="#indices-2"><strong>indices</strong></a></h4>
<ol>
<li>逗号分割的字符串。指定恢复的索引</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/multi-index.html">Multi-index syntax</a> is supported.</li>
<li>默认情况下，还原操作包括快照中的所有数据流和索引。如果提供此参数，则还原操作仅包括指定的数据流和索引。</li>
</ol>
<h4 id="partial-2"><a class="header" href="#partial-2"><strong>partial</strong></a></h4>
<ol>
<li>设置为FALSE，如果快照中包含的一个或多个索引中 如果没有做到 所有主分片都是可用的，则整个还原操作将失败。默认为false。</li>
<li>设置为TRUE：允许部分索引 的不可用分片出现</li>
<li>只有成功包含在快照中的分片才会被还原。所有丢失的分片将被重新创建为空的。</li>
</ol>
<h4 id="rename_pattern"><a class="header" href="#rename_pattern"><strong>rename_pattern</strong></a></h4>
<ol>
<li>
<p>定义一个 索引或数据流 重命名的模式</p>
</li>
<li>
<p>如果匹配这个模式 则 会被重命名</p>
</li>
<li>
<p>可以使用正则替换。支持引用原始文本。根据  <a href="https://docs.oracle.com/javase/8/docs/api/java/util/regex/Matcher.html#appendReplacement-java.lang.StringBuffer-java.lang.String-"><code>appendReplacement</code></a> 逻辑</p>
</li>
<li>
<p>如果两个不同索引被 重命名成一个名字 则 请求失败</p>
</li>
<li>
<p>数据流的后备索引同样会被重命名 </p>
<pre><code>例如 logs -&gt; restored-logs
.ds-logs-2099.03.09-000005 
is renamed to 
.ds-restored-logs-2099.03.09-000005
</code></pre>
</li>
<li>
<p>注意：要确保 数据流的 索引模板 能匹配新的 索引名字。否则就不能 rollover</p>
</li>
</ol>
<h4 id="rename_replacement"><a class="header" href="#rename_replacement"><strong>rename_replacement</strong></a></h4>
<ol>
<li>定义替换后的索引名</li>
</ol>
<h3 id="examples-3"><a class="header" href="#examples-3">Examples</a></h3>
<pre><code class="language-console">POST /_snapshot/my_repository/snapshot_2/_restore?wait_for_completion=true
{
  &quot;indices&quot;: &quot;index_1,index_2&quot;,
  &quot;ignore_unavailable&quot;: true,
  &quot;include_global_state&quot;: false,
  &quot;rename_pattern&quot;: &quot;index_(.+)&quot;,
  &quot;rename_replacement&quot;: &quot;restored_index_$1&quot;,
  &quot;include_aliases&quot;: false
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="slm-manage-the-snapshot-lifecycle"><a class="header" href="#slm-manage-the-snapshot-lifecycle">SLM: Manage the snapshot lifecycle</a></h2>
<ol>
<li>
<p>您可以设置快照生命周期策略来自动控制快照的定时、频率和保留。快照策略可以应用于多个数据流和索引。</p>
</li>
<li>
<p>The snapshot lifecycle management (SLM) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-lifecycle-management-api.html">CRUD APIs</a> 为Kibana管理的一部分提供快照策略功能的构建块。</p>
</li>
<li>
<p><a href="https://www.elastic.co/guide/en/kibana/7.13/snapshot-repositories.html">Snapshot and Restore</a> 可以轻松设置策略，注册快照存储库，查看和管理快照以及还原数据流或索引。</p>
</li>
<li>
<p>您可以停止并重新启动SLM，以在执行升级或其他维护时暂时暂停自动备份。</p>
</li>
</ol>
<h2 id="教程"><a class="header" href="#教程">教程</a></h2>
<ol>
<li>
<p>本教程演示如何使用SLM策略自动备份Elasticsearch数据流和索引</p>
</li>
<li>
<p>该策略对集群中的所有数据流和索引进行快照，并将它们存储在本地存储库中。它还定义了保留策略，并在不再需要快照时自动删除快照。</p>
</li>
</ol>
<p>要使用SLM管理快照，您可以</p>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/getting-started-snapshot-lifecycle-management.html#slm-gs-register-repository">Register a repository</a>.</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/getting-started-snapshot-lifecycle-management.html#slm-gs-create-policy">Create an SLM policy</a>.</li>
</ol>
<p>要测试策略，您可以手动触发它以获取初始快照。</p>
<h3 id="register-a-repository"><a class="header" href="#register-a-repository">Register a repository</a></h3>
<ol>
<li>
<p>要使用SLM，您必须配置快照存储库。</p>
</li>
<li>
<p>存储库可以是本地 (共享文件系统) 或远程 (云存储)。远程存储库可以驻留在S3、HDFS、Azure、谷歌云存储或存储库插件  <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository.html">repository plugin</a>  支持的任何其他平台上。</p>
</li>
<li>
<p>远程存储库一般用于生产部署。</p>
</li>
</ol>
<pre><code class="language-console">PUT /_snapshot/my_repository
{
  &quot;type&quot;: &quot;fs&quot;,
  &quot;settings&quot;: {
    &quot;location&quot;: &quot;my_backup_location&quot;
  }
}
</code></pre>
<h3 id="set-up-a-snapshot-policy"><a class="header" href="#set-up-a-snapshot-policy">Set up a snapshot policy</a></h3>
<ol>
<li>一旦有了存储库，就可以定义SLM策略以自动拍摄快照</li>
<li>该策略定义了何时拍摄快照，应包括哪些数据流或索引  以及如何命名快照。</li>
<li>策略还可以指定保留策略 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-retention.html">retention policy</a> ，并在不再需要快照时自动删除快照。</li>
<li>不要害怕配置频繁快照的策略。快照是增量的，可以有效地利用存储。</li>
</ol>
<p><strong>注意</strong></p>
<p>您可以通过Kibana Management或使用创建或更新策略API定义和管理策略。</p>
<h4 id="example-2"><a class="header" href="#example-2"><strong>Example</strong></a></h4>
<p>例如，您可以定义一个夜间快照策略，以每天在UTC上午1:30备份所有数据流和索引。</p>
<pre><code class="language-console">PUT /_slm/policy/nightly-snapshots
{
  &quot;schedule&quot;: &quot;0 30 1 * * ?&quot;,  // 1. Cron syntax
  &quot;name&quot;: &quot;&lt;nightly-snap-{now/d}&gt;&quot;, //2. 快照命名
  &quot;repository&quot;: &quot;my_repository&quot;,  //存储库
  &quot;config&quot;: { 
    &quot;indices&quot;: [&quot;*&quot;]  //包含的索引
  },
  &quot;retention&quot;: {  //快照维持天数
    &quot;expire_after&quot;: &quot;30d&quot;,  //保持快照30天，
    &quot;min_count&quot;: 5, //无论年龄大小，至少保留5张且不超过50张快照
    &quot;max_count&quot;: 50  
  }
}
</code></pre>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/trigger-schedule.html#schedule-cron">Cron syntax</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/date-math-index-names.html">date math</a> </li>
</ol>
<h4 id="其他配置"><a class="header" href="#其他配置"><strong>其他配置</strong></a></h4>
<p>您可以指定其他快照配置选项来自定义快照的拍摄方式</p>
<p>例如，如果缺少指定的数据流或索引之一，则可以将策略配置为使快照失败，详见： <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-take-snapshot.html">snapshot requests</a></p>
<h3 id="test-the-snapshot-policy"><a class="header" href="#test-the-snapshot-policy">Test the snapshot policy</a></h3>
<ol>
<li>
<p>SLM拍摄的快照与其他快照一样</p>
</li>
<li>
<p>您可以在Kibana Management中查看有关快照的信息，也可以使用快照api获取信息。 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-monitor-snapshot-restore.html">snapshot APIs</a></p>
</li>
<li>
<p>此外，SLM会跟踪策略的成功和失败，因此您可以深入了解策略的工作方式</p>
</li>
<li>
<p>如果策略至少执行了一次，则 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-api-get-policy.html">get policy</a>  将返回其他元数据，这些元数据将显示快照是否成功。</p>
</li>
</ol>
<h4 id="手动执行策略"><a class="header" href="#手动执行策略">手动执行策略</a></h4>
<ol>
<li>可以手动立即执行快照策略</li>
<li>这对于在进行配置更改、升级或测试新策略之前拍摄快照非常有用</li>
<li>手动执行策略不会影响其配置的计划。</li>
</ol>
<pre><code>POST /_slm/policy/nightly-snapshots/_execute
</code></pre>
<p>强制运行nightly-snapshots策略后，您可以检索策略以获取成功或失败信息。</p>
<pre><code class="language-console">GET /_slm/policy/nightly-snapshots?human
</code></pre>
<ol>
<li>仅返回最近的成功和失败，但所有策略执行都记录在 <code>.slm-history*</code> indices. </li>
<li>响应还显示策略何时计划下一步执行。</li>
<li>响应显示策略是否成功启动快照，但是，这并不能保证快照成功完成，例如，如果在复制文件时丢失了与远程存储库的连接，则启动的快照可能会失败。</li>
</ol>
<pre><code class="language-console-result">{
  &quot;nightly-snapshots&quot; : {
    &quot;version&quot;: 1,
    &quot;modified_date&quot;: &quot;2019-04-23T01:30:00.000Z&quot;,
    &quot;modified_date_millis&quot;: 1556048137314,
    &quot;policy&quot; : {
      &quot;schedule&quot;: &quot;0 30 1 * * ?&quot;,
      &quot;name&quot;: &quot;&lt;nightly-snap-{now/d}&gt;&quot;,
      &quot;repository&quot;: &quot;my_repository&quot;,
      &quot;config&quot;: {
        &quot;indices&quot;: [&quot;*&quot;],
      },
      &quot;retention&quot;: {
        &quot;expire_after&quot;: &quot;30d&quot;,
        &quot;min_count&quot;: 5,
        &quot;max_count&quot;: 50
      }
    },
    &quot;last_success&quot;: { //关于策略最后一次成功创建快照的信息                           
      &quot;snapshot_name&quot;: &quot;nightly-snap-2019.04.24-tmtnyjtrsxkhbrrdcgg18a&quot;, //成功启动的快照名称
      &quot;time_string&quot;: &quot;2019-04-24T16:43:49.316Z&quot;,
      &quot;time&quot;: 1556124229316
    } ,
    &quot;last_failure&quot;: { //有关策略上次启动快照失败的信息
      &quot;snapshot_name&quot;: &quot;nightly-snap-2019.04.02-lohisb5ith2n8hxacaq3mw&quot;,
      &quot;time_string&quot;: &quot;2019-04-02T01:30:00.000Z&quot;,
      &quot;time&quot;: 1556042030000,
      &quot;details&quot;: &quot;{\&quot;type\&quot;:\&quot;index_not_found_exception\&quot;,\&quot;reason\&quot;:\&quot;no such index [important]\&quot;,\&quot;resource.type\&quot;:\&quot;index_or_alias\&quot;,\&quot;resource.id\&quot;:\&quot;important\&quot;,\&quot;index_uuid\&quot;:\&quot;_na_\&quot;,\&quot;index\&quot;:\&quot;important\&quot;,\&quot;stack_trace\&quot;:\&quot;[important] IndexNotFoundException[no such index [important]]\\n\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.indexNotFoundException(IndexNameExpressionResolver.java:762)\\n\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.innerResolve(IndexNameExpressionResolver.java:714)\\n\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver$WildcardExpressionResolver.resolve(IndexNameExpressionResolver.java:670)\\n\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndices(IndexNameExpressionResolver.java:163)\\n\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndexNames(IndexNameExpressionResolver.java:142)\\n\\tat org.elasticsearch.cluster.metadata.IndexNameExpressionResolver.concreteIndexNames(IndexNameExpressionResolver.java:102)\\n\\tat org.elasticsearch.snapshots.SnapshotsService$1.execute(SnapshotsService.java:280)\\n\\tat org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47)\\n\\tat org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687)\\n\\tat org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310)\\n\\tat org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210)\\n\\tat org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150)\\n\\tat org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188)\\n\\tat org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:688)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252)\\n\\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\\n\\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\\n\\tat java.base/java.lang.Thread.run(Thread.java:834)\\n\&quot;}&quot;
    } ,
    &quot;next_execution&quot;: &quot;2019-04-24T01:30:00.000Z&quot;,                        
    &quot;next_execution_millis&quot;: 1556048160000 //下次策略执行时
  }
}
</code></pre>
<h2 id="security-and-slm"><a class="header" href="#security-and-slm">Security and SLM</a></h2>
<p>启用Elasticsearch安全功能时，以下群集权限控制对SLM操作的访问:</p>
<ul>
<li>
<p><strong><code>manage_slm</code></strong></p>
<p>允许用户执行所有SLM操作，包括创建和更新策略以及启动和停止SLM。</p>
</li>
<li>
<p><strong><code>read_slm</code></strong></p>
<p>允许用户执行所有只读SLM操作，例如获取策略和检查SLM状态。</p>
</li>
<li>
<p><strong><code>cluster:admin/snapshot/\*</code></strong></p>
</li>
</ul>
<p>​		允许用户获取和删除任何索引的快照，无论他们是否有权访问该索引。</p>
<ol>
<li>
<p>您可以通过Kibana Management创建和管理角色来分配这些权限。</p>
</li>
<li>
<p>要授予创建和管理SLM策略和快照所需的权限，您可以使用manage_slm和 <code>cluster:admin/snapshot/*</code>   集群权限和对SLM历史索引的完全访问权限。</p>
</li>
</ol>
<p>例如，以下请求创建了slm-admin角色:</p>
<pre><code class="language-console">POST /_security/role/slm-admin
{
  &quot;cluster&quot;: [&quot;manage_slm&quot;, &quot;cluster:admin/snapshot/*&quot;],
  &quot;indices&quot;: [
    {
      &quot;names&quot;: [&quot;.slm-history-*&quot;],
      &quot;privileges&quot;: [&quot;all&quot;]
    }
  ]
}
</code></pre>
<ol>
<li>要授予对SLM策略和快照历史记录的只读访问权限，可以设置具有<em>read_slm</em> 集群权限的角色，并读取对快照生命周期管理历史记录索引的访问权限。</li>
<li>例如，以下请求创建了一个  <code>slm-read-only</code> </li>
</ol>
<pre><code class="language-console">POST /_security/role/slm-read-only
{
  &quot;cluster&quot;: [&quot;read_slm&quot;],
  &quot;indices&quot;: [
    {
      &quot;names&quot;: [&quot;.slm-history-*&quot;],
      &quot;privileges&quot;: [&quot;read&quot;]
    }
  ]
}
</code></pre>
<h2 id="snapshot-retention"><a class="header" href="#snapshot-retention">Snapshot retention</a></h2>
<ol>
<li>
<p>您可以在SLM策略中包含保留策略，以自动删除旧快照</p>
</li>
<li>
<p>Retention 作为集群级任务运行，并且不与特定策略的计划相关联</p>
</li>
<li>
<p>retention criteria 作为 Retention task 的一部分进行评估，而不是在策略执行时</p>
</li>
<li>
<p>为了使保留任务自动删除快照，您需要在SLM策略中包含一个 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-api-put-policy.html#slm-api-put-retention"><code>retention</code></a>  Object</p>
</li>
<li>
<p>要控制保留任务何时运行 配置  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-settings.html#slm-retention-schedule"><code>slm.retention_schedule</code></a> 集群配置</p>
</li>
<li>
<p>可以定义周期性或者 绝对时间  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/trigger-schedule.html#schedule-cron">cron schedule</a>. </p>
</li>
<li>
<p>The <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-settings.html#slm-retention-duration"><code>slm.retention_duration</code></a> 设置 限制SLM删除旧快照应该花费多长时间。</p>
</li>
<li>
<p>使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-update-settings.html">update settings</a>  动态变更 schedule and duration  设置</p>
</li>
<li>
<p>可以使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-api-execute-retention.html">execute retention </a>API 手动执行</p>
</li>
</ol>
<p>The retention task 仅考虑通过 SLM策略 拍摄的快照，无论是根据策略计划还是通过 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-api-execute-lifecycle.html">execute lifecycle</a></p>
<p>手动快照将被忽略，并且不会计入 retention limits。</p>
<p>要检索有关快照保留任务历史记录的信息， <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/slm-api-get-stats.html">get stats</a> API:</p>
<pre><code class="language-console">GET /_slm/stats
</code></pre>
<p><strong>response</strong></p>
<pre><code class="language-json">{
  &quot;retention_runs&quot;: 13, //运行的次数
  &quot;retention_failed&quot;: 0, //失败的次数
  &quot;retention_timed_out&quot;: 0,  //超时的次数：retention次数达到slm.retention_duration时间限制，必须在删除所有符合条件的快照之前停止
  &quot;retention_deletion_time&quot;: &quot;1.4s&quot;,  //定期删除快照总花费时间
  &quot;retention_deletion_time_millis&quot;: 1404, 
  &quot;policy_stats&quot;: [ //被 daily-snapshots 策略 拍摄的快照信息
    {
      &quot;policy&quot;: &quot;daily-snapshots&quot;,
      &quot;snapshots_taken&quot;: 1,
      &quot;snapshots_failed&quot;: 1,
      &quot;snapshots_deleted&quot;: 0, 
      &quot;snapshot_deletion_failures&quot;: 0 
    }
  ],
  &quot;total_snapshots_taken&quot;: 1,
  &quot;total_snapshots_failed&quot;: 1,
  &quot;total_snapshots_deleted&quot;: 0, 
  &quot;total_snapshot_deletion_failures&quot;: 0 
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="searchable-snapshots"><a class="header" href="#searchable-snapshots">Searchable snapshots</a></h2>
<ol>
<li>可搜索快照使您可以使用快照以非常经济高效的方式搜索不经常访问的数据和只读数据。</li>
<li>冷数据层和冻结数据层使用可搜索的快照来降低存储和运营成本。</li>
<li>可搜索的快照 消除了对副本分片的需求，从而可能将搜索数据所需的本地存储减半。可搜索的快照依赖于您已经用于备份的相同快照机制，并且对快照存储库存储成本的影响最小。</li>
</ol>
<h3 id="using-searchable-snapshots"><a class="header" href="#using-searchable-snapshots">Using searchable snapshots</a></h3>
<ol>
<li>搜索可搜索的快照索引与搜索任何其他索引相同。</li>
<li>默认情况下，可搜索的快照索引没有副本</li>
<li>底层快照提供了弹性，并且预计查询量足够低，以至于单个分片副本就足够了。</li>
<li>但是，如果您需要支持更高的查询量，则可以通过调整<em>index.number_of_replicas</em>索引设置来添加副本。</li>
<li>如果某个节点发生故障，并且需要在其他地方恢复可搜索快照的分片，则在Elasticsearch将分片分配的其他节点时，集群运行状况不为绿色，会有一个短暂的时间窗口。击中这些分片的搜索可能会失败或返回部分结果，直到将分片重新分配给健康节点为止。</li>
<li>您通常通过ILM管理可搜索的快照。可搜索快照操作在到达冷或冻结阶段时会自动将常规索引转换为可搜索快照索引。</li>
<li>您还可以通过使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots-api-mount-snapshot.html">mount snapshot</a> API. 手动挂载索引来使现有快照中的索引可搜索。</li>
<li>要从包含多个索引的快照中挂载索引，我们建议创建快照的克隆 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/clone-snapshot-api.html">clone</a> ，该克隆仅包含要搜索的索引，并挂载该克隆的快照</li>
<li>如果快照有任何已挂在的索引，则不应删除该快照，因此，创建克隆使您能够独立于任何可搜索的快照来管理备份快照的生命周期。如果您使用ILM来管理您的可搜索快照，那么它将在根据需要克隆快照后自动查看。</li>
<li>您可以使用与常规索引相同的机制来控制可搜索快照索引的分片的分配。</li>
<li>例如可以使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">Index-level shard allocation filtering</a>  设置 将可搜索的快照分片数限制为节点数的子集。</li>
<li>可搜索快照索引的恢复速度受到  repository setting： max_restore_bytes_per_sec 和节点设置 indices.recovery.max_bytes_per_sec的限制，就像正常的还原操作一样</li>
<li>默认情况下，<em>max_restore_bytes_per_sec</em>是无限的，但<em>indices.recovery.max_bytes_per_sec</em>的默认值取决于节点的配置。See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/recovery.html#recovery-settings">Recovery settings</a>.</li>
<li>我们建议您在  获取快照之前，将索引强制合并，使得每个分片 单个片段，该快照将作为可搜索的快照索引挂载</li>
<li>从快照存储库中的每次读取都会花费时间并花费金钱，并且段越少，恢复快照或响应搜索所需的读取就越少。</li>
<li>可搜索的快照是管理大量历史数据档案的理想选择。历史信息的搜索频率通常低于最近的数据，因此可能不需要副本来获得其性能优势。</li>
<li>对于更复杂或耗时的搜索，您可以将异步搜索 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/async-search.html">Async search</a>  与可搜索快照一起使用。</li>
</ol>
<p>将以下任何存储库类型与可搜索的快照一起使用:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-s3.html">AWS S3</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-gcs.html">Google Cloud Storage</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-azure.html">Azure Blob Storage</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-hdfs.html">Hadoop Distributed File Store (HDFS)</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html#snapshots-filesystem-repository">Shared filesystems</a> such as NFS</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html#snapshots-read-only-repository">Read-only HTTP and HTTPS repositories</a></li>
</ul>
<p>您还可以使用这些存储库类型的替代实现，例如 <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-s3-client.html#repository-s3-compatible-services">Minio</a>，只要它们完全兼容即可。使用存储库分析API分析您的存储库是否适合与可搜索快照一起使用。</p>
<h3 id="how-searchable-snapshots-work"><a class="header" href="#how-searchable-snapshots-work">How searchable snapshots work</a></h3>
<blockquote>
<p>可搜索快照如何工作</p>
</blockquote>
<p>When an index is mounted from a snapshot, Elasticsearch allocates its shards to data nodes within the cluster.</p>
<ol>
<li>从快照挂载索引时，Elasticsearch将其分片分配给集群内的 data nodes</li>
<li>然后，数据节点自动将相关的分片数据从存储库中检索到本地存储,基于 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#searchable-snapshot-mount-storage-options">mount options</a> .如果可能，搜索使用来自本地存储的数据.  如果数据在本地不可用，Elasticsearch将从快照存储库中下载所需的数据。</li>
<li>如果持有其中一个分片的节点发生故障，Elasticsearch会自动将受影响的分片分配到另一个节点上，并且该节点会从存储库中恢复相关的分片数据。</li>
<li>不需要副本，也不需要复杂的监视或编排来恢复丢失的分片</li>
<li>尽管默认情况下可搜索的快照索引没有副本，您可以通过调整index.number_of_replicas将副本添加到这些索引。</li>
<li>通过从快照存储库中复制数据来恢复可搜索快照分片的副本。就像可搜索快照 分片的主分片一样。</li>
<li>相反，常规索引的副本 通过从主分片 复制数据来恢复。</li>
</ol>
<h4 id="mount-options"><a class="header" href="#mount-options">Mount options</a></h4>
<p>要搜索快照，您必须首先将其作为索引在本地挂载。通常ILM会自动执行此操作，但是您也可以自己调用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots-api-mount-snapshot.html">mount snapshot</a> API </p>
<p>从快照挂载索引有两个选项，每个选项都具有不同的性能特征和本地存储足迹:</p>
<ul>
<li>
<p><strong>Fully mounted index</strong></p>
<ol>
<li>将快照索引的分片的完整副本加载到群集内的节点本地存储中，ILM在 热和冷阶段 使用此选项</li>
<li>完全挂载索引的搜索性能通常与常规索引相当，因为访问快照存储库的需求最小。</li>
<li>在恢复过程中，搜索性能可能比常规索引慢，因为搜索可能需要一些尚未检索到本地副本中的数据，如果发生这种情况，Elasticsearch将急切地检索与正在进行的恢复  并行完成搜索所需的数据。</li>
</ol>
</li>
<li>
<p><strong>Partially mounted index</strong></p>
<ol>
<li>使用仅包含最近搜索的快照索引数据部分的本地缓存。</li>
<li>此缓存具有固定大小，并且在冻结层中的节点之间共享。ILM在冻结阶段使用此选项，</li>
<li>如果搜索需要不在缓存中的数据，则Elasticsearch会从快照存储库中获取丢失的数据，需要这些提取的搜索速度较慢，但是提取的数据存储在缓存中，以便将来可以更快地提供类似的搜索。</li>
<li>Elasticsearch将从缓存中逐出不经常使用的数据以释放空间</li>
<li>虽然比完全挂载索引或常规索引慢，部分挂载的索引仍然会快速返回搜索结果，即使对于大型数据集，因为存储库中数据的布局针对搜索进行了大量优化</li>
<li>在返回结果之前，许多搜索将只需要检索总分片数据的一小部分。</li>
<li>要部分挂载索引，必须有一个或多个具有共享缓存的节点。</li>
<li>默认情况下，专用冻结数据层节点（具有data_frozen角色且没有其他数据角色的节点）具有 共享缓存，可以使用 总磁盘空间的90%，以及 总磁盘空间减100GB的净空</li>
<li>强烈建议在生产中使用专用的冷冻层</li>
<li>如果没有专用的冻结层，则必须配置<em>xpack.searchable.snapshot.shared_cache.size</em>设置为一个或多个节点上的缓存预留空间。部分挂载的索引仅分配给具有共享缓存的节点。</li>
</ol>
</li>
<li>
<p><strong><code>xpack.searchable.snapshot.shared_cache.size</code></strong></p>
<p>(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/settings.html#static-cluster-setting">Static</a>) Disk space reserved for the shared cache of partially mounted indices. Accepts a percentage of total disk space or an absolute <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#byte-units">byte value</a>. Defaults to <code>90%</code> of total disk space for dedicated frozen data tier nodes. Otherwise defaults to <code>0b</code>.</p>
</li>
<li>
<p><strong><code>xpack.searchable.snapshot.shared_cache.size.max_headroom</code></strong></p>
<p>(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/settings.html#static-cluster-setting">Static</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#byte-units">byte value</a>) For dedicated frozen tier nodes, the max headroom to maintain. If <code>xpack.searchable.snapshot.shared_cache.size</code> is not explicitly set, this setting defaults to <code>100GB</code>. Otherwise it defaults to <code>-1</code> (not set). You can only configure this setting if <code>xpack.searchable.snapshot.shared_cache.size</code> is set as a percentage.</p>
</li>
</ul>
<p>为了说明这些设置如何协同工作，让我们看两个示例，当在专用冻结节点上使用设置的默认值时:</p>
<ul>
<li>A 4000 GB disk will result in a shared cache sized at 3900 GB. 90% of 4000 GB is 3600 GB, leaving 400 GB headroom. The default <code>max_headroom</code> of 100 GB takes effect, and the result is therefore 3900 GB.</li>
<li>A 400 GB disk will result in a shared cache sized at 360 GB.</li>
</ul>
<p>You can configure the settings in <code>elasticsearch.yml</code>:</p>
<pre><code class="language-yaml">xpack.searchable.snapshot.shared_cache.size: 4TB
</code></pre>
<ol>
<li>目前，您可以在任何节点上配置 xpack.searchable.snapshot.shared_cache.size。</li>
<li>如果在 没有  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-frozen-node"><code>data_frozen</code></a>  角色 的节点上设置 该 设置 它将被视为设置为0b。</li>
<li>此外，具有共享缓存的节点只能具有单个数据路径 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#path-settings">data path</a>.</li>
</ol>
<h3 id="back-up-and-restore-searchable-snapshots"><a class="header" href="#back-up-and-restore-searchable-snapshots">Back up and restore searchable snapshots</a></h3>
<ol>
<li>您可以使用常规快照 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-lifecycle-management.html">regular snapshots</a>  来备份包含可搜索快照索引的集群。</li>
<li>恢复包含可搜索快照索引的快照时，这些索引将再次恢复为可搜索快照索引。</li>
<li>在还原包含可搜索快照索引的快照之前，必须先注册  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshots-register-repository.html">register the repository</a>  包含原始索引快照的存储库。</li>
<li>恢复后，可搜索的快照索引将从其原始存储库中挂载原始索引快照。</li>
<li>如果需要，您可以将单独的存储库用于常规快照和可搜索快照。</li>
<li>可搜索快照索引的快照仅包含少量元数据，这些元数据标识其原始索引快照，它不包含来自原始索引的任何数据</li>
<li>备份的恢复将  无法恢复任何   原始索引快照不可用的  可搜索快照索引</li>
</ol>
<h3 id="reliability-of-searchable-snapshots"><a class="header" href="#reliability-of-searchable-snapshots">Reliability of searchable snapshots</a></h3>
<blockquote>
<p>可搜索快照的可靠性</p>
</blockquote>
<ol>
<li>
<p>可搜索快照索引中数据的唯一副本是存储在存储库中的基础快照。</p>
</li>
<li>
<p>如果存储库失败或损坏快照的内容，则数据将丢失</p>
</li>
<li>
<p>尽管Elasticsearch可能已将数据复制到本地存储中，但这些副本可能不完整，并且在存储库失败后无法用于恢复任何数据。</p>
</li>
<li>
<p>您必须确保您的存储库是可靠的，并在存储库中的数据处于静止状态时防止数据损坏。</p>
</li>
<li>
<p>所有主要的公共云提供商 提供的blob存储通常提供非常好的保护，防止数据丢失或损坏。如果您管理自己的存储库存储，则应对其可靠性负责。</p>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="mount-snapshot-api"><a class="header" href="#mount-snapshot-api">Mount snapshot API</a></h2>
<p>Mount a snapshot as a searchable snapshot index.</p>
<h3 id="request-11"><a class="header" href="#request-11">Request</a></h3>
<pre><code>POST /_snapshot/&lt;repository&gt;/&lt;snapshot&gt;/_mount
</code></pre>
<h3 id="prerequisites-3"><a class="header" href="#prerequisites-3">Prerequisites</a></h3>
<p>If the Elasticsearch security features are enabled, you must have the <code>manage</code> cluster privilege and the <code>manage</code> index privilege for any included indices to use this API. For more information, see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-privileges.html">Security privileges</a>.</p>
<h3 id="description-9"><a class="header" href="#description-9">Description</a></h3>
<h3 id="path-parameters-10"><a class="header" href="#path-parameters-10">Path parameters</a></h3>
<ul>
<li>
<p><strong><code>&lt;repository&gt;</code></strong></p>
<p>(Required, string) The name of the repository containing the snapshot of the index to mount.</p>
</li>
<li>
<p><strong><code>&lt;snapshot&gt;</code></strong></p>
<p>(Required, string) The name of the snapshot of the index to mount.</p>
</li>
</ul>
<h3 id="query-parameters-10"><a class="header" href="#query-parameters-10">Query parameters</a></h3>
<ul>
<li>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
</li>
<li>
<p><strong><code>wait_for_completion</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, the request blocks until the operation is complete. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>storage</code></strong></p>
<p>(Optional, string) <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#searchable-snapshot-mount-storage-options">Mount option</a> for the searchable snapshot index. Possible values are:<strong><code>full_copy</code> (Default)</strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#fully-mounted">Fully mounted index</a>.<strong><code>shared_cache</code></strong><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#partially-mounted">Partially mounted index</a>.</p>
</li>
</ul>
<h3 id="request-body-5"><a class="header" href="#request-body-5">Request body</a></h3>
<ul>
<li>
<p><strong><code>index</code></strong></p>
<p>(Required, string) Name of the index contained in the snapshot whose data is to be mounted.</p>
</li>
</ul>
<p>If no <code>renamed_index</code> is specified this name will also be used to create the new index.</p>
<ul>
<li>
<p><strong><code>renamed_index</code></strong></p>
<p>(Optional, string) Name of the index that will be created.</p>
</li>
<li>
<p><strong><code>index_settings</code></strong></p>
<p>(Optional, object) Settings that should be added to the index when it is mounted.</p>
</li>
<li>
<p><strong><code>ignore_index_settings</code></strong></p>
<p>(Optional, array of strings) Names of settings that should be removed from the index when it is mounted.</p>
</li>
</ul>
<h3 id="examples-4"><a class="header" href="#examples-4">Examples</a></h3>
<p>Mounts the index <code>my_docs</code> from an existing snapshot named <code>my_snapshot</code> stored in the <code>my_repository</code> as a new index <code>docs</code>:</p>
<pre><code class="language-console">POST /_snapshot/my_repository/my_snapshot/_mount?wait_for_completion=true
{
  &quot;index&quot;: &quot;my_docs&quot;, //快照中的索引
  &quot;renamed_index&quot;: &quot;docs&quot;, //重命名后的索引
  &quot;index_settings&quot;: { //索引设置
    &quot;index.number_of_replicas&quot;: 0
  },
  &quot;ignore_index_settings&quot;: [ &quot;index.refresh_interval&quot; ]  //忽略的索引设置
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="s3-repository-plugin"><a class="header" href="#s3-repository-plugin">S3 Repository Plugin</a></h2>
<p>该插件添加了 对 AWS S3 的 快照备份恢复的 支持</p>
<h3 id="installation"><a class="header" href="#installation">Installation</a></h3>
<pre><code>sudo bin/elasticsearch-plugin install repository-s3
</code></pre>
<h3 id="removal"><a class="header" href="#removal">Removal</a></h3>
<pre><code>sudo bin/elasticsearch-plugin remove repository-s3
</code></pre>
<p>插件必须安装在集群中的每个节点上，并且每个节点都必须在安装后重新启动。</p>
<pre><code class="language-shell">multipass exec primary sudo /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-s3

multipass exec primary sudo systemctl restart elasticsearch

multipass exec node-2 sudo /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-s3
multipass exec node-2 sudo systemctl restart elasticsearch

multipass exec node-3 sudo /usr/share/elasticsearch/bin/elasticsearch-plugin install repository-s3
multipass exec node-3 sudo systemctl restart elasticsearch
</code></pre>
<h2 id="配置"><a class="header" href="#配置">配置</a></h2>
<p>The plugin provides a repository type named <code>s3</code> which may be used when creating a repository. </p>
<p>The repository defaults to using <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html">ECS IAM Role</a> or <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html">EC2 IAM Role</a> credentials for authentication. </p>
<p>The only mandatory setting is the bucket name:</p>
<pre><code class="language-console">PUT _snapshot/my_s3_repository
{
  &quot;type&quot;: &quot;s3&quot;,
  &quot;settings&quot;: {
    &quot;bucket&quot;: &quot;my-bucket&quot;
  }
}
</code></pre>
<h3 id="客户端配置"><a class="header" href="#客户端配置">客户端配置</a></h3>
<ol>
<li>
<p>用于连接到S3的客户端具有许多可用设置。</p>
</li>
<li>
<p>The settings have the form <code>s3.client.CLIENT_NAME.SETTING_NAME</code>. </p>
</li>
<li>
<p>By default, <code>s3</code> repositories use a client named <code>default</code>, but this can be modified using the <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-s3-repository.html">repository setting</a> <code>client</code>. For example:</p>
</li>
</ol>
<pre><code>PUT _snapshot/my_s3_repository
{
  &quot;type&quot;: &quot;s3&quot;,
  &quot;settings&quot;: {
    &quot;bucket&quot;: &quot;my-bucket&quot;,
    &quot;client&quot;: &quot;my-alternate-client&quot;
  }
}
</code></pre>
<ol>
<li>
<p>大多数配置可以添加到 <code>elasticsearch.yml</code>，除了安全配置（他们被添加到Elasticsearch keystore ）</p>
</li>
<li>
<p>For more information about creating and updating the Elasticsearch keystore, see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">Secure settings</a>.</p>
</li>
</ol>
<p>例如，如果要使用特定凭据访问S3，则运行以下命令将这些凭据添加到密钥库:</p>
<pre><code>bin/elasticsearch-keystore add s3.client.default.access_key
bin/elasticsearch-keystore add s3.client.default.secret_key
# a session token is optional so the following command may not be needed
bin/elasticsearch-keystore add s3.client.default.session_token
</code></pre>
<p>相反，如果要使用实例角色或容器角色来访问S3，则应清空这些设置</p>
<p>可以通过 移除以下设置，来从特定凭据   切换回  instance role or container role 的默认凭据</p>
<pre><code class="language-sh">bin/elasticsearch-keystore remove s3.client.default.access_key
bin/elasticsearch-keystore remove s3.client.default.secret_key
# a session token is optional so the following command may not be needed
bin/elasticsearch-keystore remove s3.client.default.session_token
</code></pre>
<ol>
<li><strong>All</strong> client secure settings of this plugin are <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html#reloadable-secure-settings">reloadable</a>. </li>
<li>当你手动 重载配置后。内部S3客户端也会重载</li>
<li>正在进行的快照/还原任务不会被  客户端安全设置的重新加载所抢占。</li>
<li>该任务将使用客户端完成，因为它是在操作开始时构建的。</li>
</ol>
<p>以下列表包含可用的客户端设置，必须存储在密钥库中的配置，且被标记为 “安全” 可以重新加载; 其他设置属于elasticsearch.yml文件。</p>
<ul>
<li>
<p><strong><code>access_key</code> (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">Secure</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html#reloadable-secure-settings">reloadable</a>)</strong></p>
<p>An S3 access key. If set, the <code>secret_key</code> setting must also be specified. If unset, the client will use the instance or container role instead.</p>
</li>
<li>
<p><strong><code>secret_key</code> (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">Secure</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html#reloadable-secure-settings">reloadable</a>)</strong></p>
<p>An S3 secret key. If set, the <code>access_key</code> setting must also be specified.</p>
</li>
<li>
<p><strong><code>session_token</code> (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">Secure</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html#reloadable-secure-settings">reloadable</a>)</strong></p>
<p>An S3 session token. If set, the <code>access_key</code> and <code>secret_key</code> settings must also be specified.</p>
</li>
<li>
<p><strong><code>endpoint</code></strong></p>
<p>The S3 service endpoint to connect to. This defaults to <code>s3.amazonaws.com</code> but the <a href="https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region">AWS documentation</a> lists alternative S3 endpoints. If you are using an <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-s3-client.html#repository-s3-compatible-services">S3-compatible service</a> then you should set this to the service’s endpoint.</p>
</li>
<li>
<p><strong><code>protocol</code></strong></p>
<p>The protocol to use to connect to S3. Valid values are either <code>http</code> or <code>https</code>. Defaults to <code>https</code>.</p>
</li>
<li>
<p><strong><code>proxy.host</code></strong></p>
<p>The host name of a proxy to connect to S3 through.</p>
</li>
<li>
<p><strong><code>proxy.port</code></strong></p>
<p>The port of a proxy to connect to S3 through.</p>
</li>
<li>
<p><strong><code>proxy.username</code> (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">Secure</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html#reloadable-secure-settings">reloadable</a>)</strong></p>
<p>The username to connect to the <code>proxy.host</code> with.</p>
</li>
<li>
<p><strong><code>proxy.password</code> (<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">Secure</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html#reloadable-secure-settings">reloadable</a>)</strong></p>
<p>The password to connect to the <code>proxy.host</code> with.</p>
</li>
<li>
<p><strong><code>read_timeout</code></strong></p>
<p>The socket timeout for connecting to S3. The value should specify the unit. For example, a value of <code>5s</code> specifies a 5 second timeout. The default value is 50 seconds.</p>
</li>
<li>
<p><strong><code>max_retries</code></strong></p>
<p>The number of retries to use when an S3 request fails. The default value is <code>3</code>.</p>
</li>
<li>
<p><strong><code>use_throttle_retries</code></strong></p>
<p>Whether retries should be throttled (i.e. should back off). Must be <code>true</code> or <code>false</code>. Defaults to <code>true</code>.</p>
</li>
<li>
<p><strong><code>path_style_access</code></strong></p>
<p>Whether to force the use of the path style access pattern. If <code>true</code>, the path style access pattern will be used. If <code>false</code>, the access pattern will be automatically determined by the AWS Java SDK (See <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#setPathStyleAccessEnabled-java.lang.Boolean-">AWS documentation</a> for details). Defaults to <code>false</code>.</p>
</li>
</ul>
<p>In versions <code>7.0</code>, <code>7.1</code>, <code>7.2</code> and <code>7.3</code> all bucket operations used the <a href="https://aws.amazon.com/blogs/aws/amazon-s3-path-deprecation-plan-the-rest-of-the-story/">now-deprecated</a> path style access pattern. If your deployment requires the path style access pattern then you should set this setting to <code>true</code> when upgrading.</p>
<ul>
<li>
<p><strong><code>disable_chunked_encoding</code></strong></p>
<p>Whether chunked encoding should be disabled or not. If <code>false</code>, chunked encoding is enabled and will be used where appropriate. If <code>true</code>, chunked encoding is disabled and will not be used, which may mean that snapshot operations consume more resources and take longer to complete. It should only be set to <code>true</code> if you are using a storage service that does not support chunked encoding. See the <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/services/s3/AmazonS3Builder.html#disableChunkedEncoding--">AWS Java SDK documentation</a> for details. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>region</code></strong></p>
<p>Allows specifying the signing region to use. Specificing this setting manually should not be necessary for most use cases. Generally, the SDK will correctly guess the signing region to use. It should be considered an expert level setting to support S3-compatible APIs that require <a href="https://docs.aws.amazon.com/general/latest/gr/signature-version-4.html">v4 signatures</a> and use a region other than the default <code>us-east-1</code>. Defaults to empty string which means that the SDK will try to automatically determine the correct signing region.</p>
</li>
<li>
<p><strong><code>signer_override</code></strong></p>
<p>Allows specifying the name of the signature algorithm to use for signing requests by the S3 client. Specifying this setting should not be necessary for most use cases. It should be considered an expert level setting to support S3-compatible APIs that do not support the signing algorithm that the SDK automatically determines for them. See the <a href="https://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/ClientConfiguration.html#setSignerOverride-java.lang.String-">AWS Java SDK documentation</a> for details. Defaults to empty string which means that no signing algorithm override will be used.</p>
</li>
</ul>
<h3 id="s3-compatible-services"><a class="header" href="#s3-compatible-services">S3-compatible services</a></h3>
<ol>
<li>有许多存储系统提供了S3-compatible的API</li>
<li><code>repository-s3</code> plugin 可以使 这些系统 与 AWS S3 开箱即用</li>
<li>需要提供 <code>s3.client.CLIENT_NAME.endpoint</code></li>
<li>也可以提供  <code>s3.client.CLIENT_NAME.protocol</code> </li>
</ol>
<p><strong>Minio兼容</strong></p>
<p><a href="https://minio.io/">Minio</a> is an example of a storage system that provides an S3-compatible API. The <code>repository-s3</code> plugin allows Elasticsearch to work with Minio-backed repositories as well as repositories stored on AWS S3. Other S3-compatible storage systems may also work with Elasticsearch, but these are not covered by the Elasticsearch test suite.</p>
<p><strong>完全的S3API兼容</strong></p>
<p>Note that some storage systems claim to be S3-compatible without correctly supporting the full S3 API. The <code>repository-s3</code> plugin requires full compatibility with S3. In particular it must support the same set of API endpoints, return the same errors in case of failures, and offer a consistency model no weaker than S3’s when accessed concurrently by multiple nodes. Incompatible error codes and consistency models may be particularly hard to track down since errors and consistency failures are usually rare and hard to reproduce.</p>
<p><strong>使用仓库分析来检查兼容性</strong></p>
<p>You can perform some basic checks of the suitability of your storage system using the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/repo-analysis-api.html">repository analysis API</a>. If this API does not complete successfully, or indicates poor performance, then your storage system is not fully compatible with AWS S3 and therefore unsuitable for use as a snapshot repository. You will need to work with the supplier of your storage system to address any incompatibilities you encounter.</p>
<h3 id="repository-settings"><a class="header" href="#repository-settings">Repository Settings</a></h3>
<pre><code class="language-console">PUT _snapshot/my_s3_repository
{
  &quot;type&quot;: &quot;s3&quot;,
  &quot;settings&quot;: {
    &quot;bucket&quot;: &quot;my-bucket&quot;,
    &quot;another_setting&quot;: &quot;setting-value&quot;
  }
}
</code></pre>
<p><strong><code>bucket</code></strong></p>
<p>(Required) Name of the S3 bucket to use for snapshots.</p>
<p>The bucket name must adhere to Amazon’s <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules">S3 bucket naming rules</a>.</p>
<p><strong><code>client</code></strong></p>
<p>The name of the <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository-s3-client.html">S3 client</a> to use to connect to S3. Defaults to <code>default</code>.</p>
<p><strong><code>base_path</code></strong></p>
<p>Specifies the path to the repository data within its bucket. Defaults to an empty string, meaning that the repository is at the root of the bucket. The value of this setting should not start or end with a <code>/</code>.</p>
<p><strong><code>chunk_size</code></strong></p>
<p>Big files can be broken down into chunks during snapshotting if needed. Specify the chunk size as a value and unit, for example: <code>1TB</code>, <code>1GB</code>, <code>10MB</code>. Defaults to the maximum size of a blob in the S3 which is <code>5TB</code>.</p>
<p><strong><code>compress</code></strong></p>
<p>When set to <code>true</code> metadata files are stored in compressed format. This setting doesn’t affect index files that are already compressed by default. Defaults to <code>false</code>.</p>
<p><strong><code>max_restore_bytes_per_sec</code></strong></p>
<p>Throttles per node restore rate. Defaults to unlimited. Note that restores are also throttled through <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/recovery.html">recovery settings</a>.</p>
<p><strong><code>max_snapshot_bytes_per_sec</code></strong></p>
<p>Throttles per node snapshot rate. Defaults to <code>40mb</code> per second.</p>
<p><strong><code>readonly</code></strong></p>
<p>Makes repository read-only. Defaults to <code>false</code>.</p>
<p><strong><code>server_side_encryption</code></strong></p>
<p>When set to <code>true</code> files are encrypted on server side using AES256 algorithm. Defaults to <code>false</code>.</p>
<p><strong><code>buffer_size</code></strong></p>
<p>Minimum threshold below which the chunk is uploaded using a single request. Beyond this threshold, the S3 repository will use the <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html">AWS Multipart Upload API</a> to split the chunk into several parts, each of <code>buffer_size</code> length, and to upload each part in its own request. Note that setting a buffer size lower than <code>5mb</code> is not allowed since it will prevent the use of the Multipart API and may result in upload errors. It is also not possible to set a buffer size greater than <code>5gb</code> as it is the maximum upload size allowed by S3. Defaults to <code>100mb</code> or <code>5%</code> of JVM heap, whichever is smaller.</p>
<p><strong><code>canned_acl</code></strong></p>
<p>The S3 repository supports all <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl">S3 canned ACLs</a> : <code>private</code>, <code>public-read</code>, <code>public-read-write</code>, <code>authenticated-read</code>, <code>log-delivery-write</code>, <code>bucket-owner-read</code>, <code>bucket-owner-full-control</code>. Defaults to <code>private</code>. You could specify a canned ACL using the <code>canned_acl</code> setting. When the S3 repository creates buckets and objects, it adds the canned ACL into the buckets and objects.</p>
<p><strong><code>storage_class</code></strong></p>
<p>Sets the S3 storage class for objects stored in the snapshot repository. Values may be <code>standard</code>, <code>reduced_redundancy</code>, <code>standard_ia</code>, <code>onezone_ia</code> and <code>intelligent_tiering</code>. Defaults to <code>standard</code>. Changing this setting on an existing repository only affects the storage class for newly created objects, resulting in a mixed usage of storage classes. Additionally, S3 Lifecycle Policies can be used to manage the storage class of existing objects. Due to the extra complexity with the Glacier class lifecycle, it is not currently supported by the plugin. For more information about the different classes, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html">AWS Storage Classes Guide</a></p>
<ol>
<li>除了上述设置之外，您还可以在存储库设置中指定所有非安全客户端设置。</li>
<li>repository settings 优先级 大于 客户端设置</li>
</ol>
<pre><code>PUT _snapshot/my_s3_repository
{
  &quot;type&quot;: &quot;s3&quot;,
  &quot;settings&quot;: {
    &quot;client&quot;: &quot;my-client&quot;,
    &quot;bucket&quot;: &quot;my-bucket&quot;,
    &quot;endpoint&quot;: &quot;my.s3.endpoint&quot;
  }
}
</code></pre>
<h4 id="recommended-s3-permissions"><a class="header" href="#recommended-s3-permissions">Recommended S3 Permissions</a></h4>
<ol>
<li>
<p>为了将Elasticsearch快照进程限制为所需的最低资源</p>
</li>
<li>
<p>我们建议将Amazon IAM与预先存在的S3存储桶结合使用。</p>
</li>
<li>
<p>Here is an example policy which will allow the snapshot access to an S3 bucket named &quot;snaps.example.com&quot;. This may be configured through the AWS IAM console, by creating a Custom Policy, and using a Policy Document similar to this (changing snaps.example.com to your bucket name).</p>
</li>
</ol>
<pre><code class="language-js">{
  &quot;Statement&quot;: [
    {
      &quot;Action&quot;: [
        &quot;s3:ListBucket&quot;,
        &quot;s3:GetBucketLocation&quot;,
        &quot;s3:ListBucketMultipartUploads&quot;,
        &quot;s3:ListBucketVersions&quot;
      ],
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Resource&quot;: [
        &quot;arn:aws:s3:::snaps.example.com&quot;
      ]
    },
    {
      &quot;Action&quot;: [
        &quot;s3:GetObject&quot;,
        &quot;s3:PutObject&quot;,
        &quot;s3:DeleteObject&quot;,
        &quot;s3:AbortMultipartUpload&quot;,
        &quot;s3:ListMultipartUploadParts&quot;
      ],
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Resource&quot;: [
        &quot;arn:aws:s3:::snaps.example.com/*&quot;
      ]
    }
  ],
  &quot;Version&quot;: &quot;2012-10-17&quot;
}
</code></pre>
<pre><code class="language-js">{
  &quot;Statement&quot;: [
    {
      &quot;Action&quot;: [
        &quot;s3:ListBucket&quot;,
        &quot;s3:GetBucketLocation&quot;,
        &quot;s3:ListBucketMultipartUploads&quot;,
        &quot;s3:ListBucketVersions&quot;
      ],
      &quot;Condition&quot;: {
        &quot;StringLike&quot;: {
          &quot;s3:prefix&quot;: [
            &quot;foo/*&quot;
          ]
        }
      },
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Resource&quot;: [
        &quot;arn:aws:s3:::snaps.example.com&quot;
      ]
    },
    {
      &quot;Action&quot;: [
        &quot;s3:GetObject&quot;,
        &quot;s3:PutObject&quot;,
        &quot;s3:DeleteObject&quot;,
        &quot;s3:AbortMultipartUpload&quot;,
        &quot;s3:ListMultipartUploadParts&quot;
      ],
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Resource&quot;: [
        &quot;arn:aws:s3:::snaps.example.com/foo/*&quot;
      ]
    }
  ],
  &quot;Version&quot;: &quot;2012-10-17&quot;
}
</code></pre>
<h3 id="aws-vpc-bandwidth-settings"><a class="header" href="#aws-vpc-bandwidth-settings">AWS VPC Bandwidth Settings</a></h3>
<p>AWS instances resolve S3 endpoints to a public IP. If the Elasticsearch instances reside in a private subnet in an AWS VPC then all traffic to S3 will go through the VPC’s NAT instance. If your VPC’s NAT instance is a smaller instance size (e.g. a t2.micro) or is handling a high volume of network traffic your bandwidth to S3 may be limited by that NAT instance’s networking bandwidth limitations. Instead we recommend creating a <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-endpoints.html">VPC endpoint</a> that enables connecting to S3 in instances that reside in a private subnet in an AWS VPC. This will eliminate any limitations imposed by the network bandwidth of your VPC’s NAT instance.</p>
<p>Instances residing in a public subnet in an AWS VPC will connect to S3 via the VPC’s internet gateway and not be bandwidth limited by the VPC’s NAT instance.</p>
<div style="break-before: page; page-break-before: always;"></div><h3 id="重启"><a class="header" href="#重启">重启</a></h3>
<pre><code>multipass exec primary sudo systemctl restart elasticsearch

multipass exec node-2 sudo systemctl restart elasticsearch
 
multipass exec node-3 sudo systemctl restart elasticsearch
</code></pre>
<h3 id="配置aksk"><a class="header" href="#配置aksk">配置AKSK</a></h3>
<pre><code>export accessKey=xxxx
export secretkey=xxxxxxxxxxx
export nodeName=node-3
echo $accessKey | multipass exec $nodeName  sudo /usr/share/elasticsearch/bin/elasticsearch-keystore \
add -- -f  s3.client.default.access_key 

echo $secretkey | multipass exec $nodeName  sudo /usr/share/elasticsearch/bin/elasticsearch-keystore \
add   -- -f s3.client.default.secret_key 
</code></pre>
<h3 id="重载安全秘钥"><a class="header" href="#重载安全秘钥">重载安全秘钥</a></h3>
<pre><code>POST _nodes/reload_secure_settings
</code></pre>
<h3 id="配置client"><a class="header" href="#配置client">配置Client</a></h3>
<pre><code>PUT _snapshot/mys3repository
{
  &quot;type&quot;: &quot;s3&quot;,
  &quot;settings&quot;: {
    &quot;bucket&quot;: &quot;bucket-name&quot;,
    &quot;base_path&quot;: &quot;xiaojiaquan&quot;, 
    &quot;endpoint&quot;:&quot;xxx-beijing.aliyuncs.com&quot;,
    &quot;protocol&quot;:&quot;http&quot;,
    &quot;compress&quot;: false,
    &quot;disable_chunked_encoding&quot;:true
  }
}
</code></pre>
<h3 id="拍摄快照"><a class="header" href="#拍摄快照">拍摄快照</a></h3>
<pre><code>PUT /_snapshot/mys3repository/snapshot_3
{
  &quot;indices&quot;: &quot;person&quot;,
  &quot;ignore_unavailable&quot;: true,
  &quot;metadata&quot;: {
    &quot;taken_by&quot;: &quot;weisanju&quot;,
    &quot;taken_because&quot;: &quot;firstS3Backup&quot;
  }
}
</code></pre>
<h3 id="恢复快照"><a class="header" href="#恢复快照">恢复快照</a></h3>
<pre><code>
POST /_snapshot/mys3repository/snapshot_2/_restore
{
  &quot;include_global_state&quot;: false,
  &quot;ignore_unavailable&quot;: true,
  &quot;indices&quot;: &quot;person&quot;,
  &quot;rename_pattern&quot;: &quot;(person)&quot;,
  &quot;rename_replacement&quot;: &quot;$1_copy2_restore&quot;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="安装-elasticsearch"><a class="header" href="#安装-elasticsearch">安装 Elasticsearch</a></h2>
<p>本节包括有关如何设置Elasticsearch并使其运行的信息，包括:</p>
<ul>
<li>Downloading</li>
<li>Installing</li>
<li>Starting</li>
<li>Configuring</li>
</ul>
<h2 id="supported-platforms"><a class="header" href="#supported-platforms">Supported platforms</a></h2>
<p>所有版本支持的： <a href="https://www.elastic.co/support/matrix">Support Matrix</a></p>
<h2 id="java-jvm-version"><a class="header" href="#java-jvm-version">Java (JVM) Version</a></h2>
<p>Elasticsearch is built using Java, and includes a bundled version of <a href="https://openjdk.java.net/">OpenJDK</a> from the JDK maintainers (GPLv2+CE) within each distribution. The bundled JVM is the recommended JVM and is located within the <code>jdk</code> directory of the Elasticsearch home directory.</p>
<p>To use your own version of Java, set the <code>ES_JAVA_HOME</code> environment variable. If you must use a version of Java that is different from the bundled JVM, we recommend using a <a href="https://www.elastic.co/support/matrix">supported</a> <a href="https://www.oracle.com/technetwork/java/eol-135779.html">LTS version of Java</a>. Elasticsearch will refuse to start if a known-bad version of Java is used. The bundled JVM directory may be removed when using your own JVM.</p>
<ol>
<li>内置JDK</li>
<li>可以  使用 <em>ES_JAVA_HOME</em>  自定义JDK </li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="手动安装"><a class="header" href="#手动安装">手动安装</a></h2>
<table><thead><tr><th>平台</th><th>资源包</th></tr></thead><tbody>
<tr><td>Linux and MacOS <code>tar.gz</code> archives</td><td>The <code>tar.gz</code> archives are available for installation on any Linux distribution and MacOS.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/targz.html">Install Elasticsearch from archive on Linux or MacOS</a></td></tr>
<tr><td>Windows <code>.zip</code> archive</td><td>The <code>zip</code> archive is suitable for installation on Windows.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/zip-windows.html">Install Elasticsearch with <code>.zip</code> on Windows</a></td></tr>
<tr><td><code>deb</code></td><td>The <code>deb</code> package is suitable for Debian, Ubuntu, and other Debian-based systems. Debian packages may be downloaded from the Elasticsearch website or from our Debian repository.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/deb.html">Install Elasticsearch with Debian Package</a></td></tr>
<tr><td><code>rpm</code></td><td>The <code>rpm</code> package is suitable for installation on Red Hat, Centos, SLES, OpenSuSE and other RPM-based systems. RPMs may be downloaded from the Elasticsearch website or from our RPM repository.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/rpm.html">Install Elasticsearch with RPM</a></td></tr>
<tr><td><code>msi</code></td><td>[beta] This functionality is in beta and is subject to change. The design and code is less mature than official GA features and is being provided as-is with no warranties. Beta features are not subject to the support SLA of official GA features.The <code>msi</code> package is suitable for installation on Windows 64-bit systems with at least .NET 4.5 framework installed, and is the easiest choice for getting started with Elasticsearch on Windows. MSIs may be downloaded from the Elasticsearch website.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/windows.html">Install Elasticsearch with Windows MSI Installer</a></td></tr>
<tr><td><code>docker</code></td><td>Images are available for running Elasticsearch as Docker containers. They may be downloaded from the Elastic Docker Registry.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docker.html">Install Elasticsearch with Docker</a></td></tr>
<tr><td><code>brew</code></td><td>Formulae are available from the Elastic Homebrew tap for installing Elasticsearch on macOS with the Homebrew package manager.<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/brew.html">Install Elasticsearch on macOS with Homebrew</a></td></tr>
</tbody></table>
<h2 id="使用rpm-安装"><a class="header" href="#使用rpm-安装">使用RPM 安装</a></h2>
<p>The RPM for Elasticsearch can be <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/rpm.html#install-rpm">downloaded from our website</a> or from our <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/rpm.html#rpm-repo">RPM repository</a>. It can be used to install Elasticsearch on any RPM-based system such as OpenSuSE, SLES, Centos, Red Hat, and Oracle Enterprise.</p>
<p>RPM install is not supported on distributions with old versions of RPM, such as SLES 11 and CentOS 5. Please see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/targz.html">Install Elasticsearch from archive on Linux or MacOS</a> instead.</p>
<ol>
<li>可以通过上述链接下载 RPM包。支持任务 基于RPM的包管理系统</li>
<li>不支持旧版本的 RPM</li>
</ol>
<h3 id="import-the-elasticsearch-gpg-key"><a class="header" href="#import-the-elasticsearch-gpg-key">Import the Elasticsearch GPG Key</a></h3>
<p>We sign all of our packages with the Elasticsearch Signing Key (PGP key <a href="https://pgp.mit.edu/pks/lookup?op=vindex&amp;search=0xD27D666CD88E42B4">D88E42B4</a>, available from <a href="https://pgp.mit.edu/">https://pgp.mit.edu</a>) with fingerprint:</p>
<pre><code>4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
</code></pre>
<pre><code class="language-sh">rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
</code></pre>
<h3 id="installing-from-the-rpm-repository"><a class="header" href="#installing-from-the-rpm-repository">Installing from the RPM repository</a></h3>
<p>Create a file called <code>elasticsearch.repo</code> in the <code>/etc/yum.repos.d/</code> directory for RedHat based distributions, or in the <code>/etc/zypp/repos.d/</code> directory for OpenSuSE based distributions, containing:</p>
<pre><code class="language-ini">[elasticsearch]
name=Elasticsearch repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=0
autorefresh=1
type=rpm-md
</code></pre>
<pre><code class="language-sh">// centos7
sudo yum install --enablerepo=elasticsearch elasticsearch 
//centos8
sudo dnf install --enablerepo=elasticsearch elasticsearch 
//opensuse
sudo zypper modifyrepo --enable elasticsearch &amp;&amp; \
  sudo zypper install elasticsearch; \
  sudo zypper modifyrepo --disable elasticsearch 
</code></pre>
<p>默认情况下，已配置的存储库处于禁用状态。</p>
<p>这消除了升级系统其余部分时意外升级elasticsearch的可能性。</p>
<p>每个安装或升级命令都必须明确启用存储库，如上面的示例命令所示。</p>
<h3 id="download-and-install-the-rpm-manually"><a class="header" href="#download-and-install-the-rpm-manually">Download and install the RPM manually</a></h3>
<pre><code class="language-sh">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-x86_64.rpm
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-x86_64.rpm.sha512
shasum -a 512 -c elasticsearch-7.13.4-x86_64.rpm.sha512 
sudo rpm --install elasticsearch-7.13.4-x86_64.rpm
</code></pre>
<p>比较下载的RPM和发布的校验和的SHA，应该输出elasticsearch-{version}-x86_64.rpm: OK。</p>
<p>On systemd-based distributions, the installation scripts will attempt to set kernel parameters (e.g., <code>vm.max_map_count</code>); you can skip this by masking the systemd-sysctl.service unit.</p>
<h3 id="enable-automatic-creation-of-system-indices"><a class="header" href="#enable-automatic-creation-of-system-indices">Enable automatic creation of system indices</a></h3>
<p>一些商业功能会自动在Elasticsearch内创建索引。</p>
<p>默认情况下，Elasticsearch配置为允许自动创建索引，并且不需要其他步骤。</p>
<p>但是，如果禁用了Elasticsearch中的自动索引创建，则必须在elasticsearch.yml中配置<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/rpm.html#:%7E:text=you%20must%20configure-,action.auto_create_index,-in%20elasticsearch.yml">action.auto_create_index</a>，以允许商业功能创建以下索引:</p>
<pre><code class="language-yaml">action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*
</code></pre>
<ol>
<li>
<p>如果您使用Logstash或Beats，则很可能在<em>action.auto_create_index</em>设置中需要其他索引名称，确切的值将取决于您的本地配置。</p>
</li>
<li>
<p>如果不确定环境的正确值，则可以考虑将该值设置为 *，这将允许自动创建所有索引。</p>
</li>
</ol>
<h3 id="sysv-init-vs-systemd"><a class="header" href="#sysv-init-vs-systemd">SysV <code>init</code> vs <code>systemd</code></a></h3>
<p>安装后，Elasticsearch不会自动启动。如何启动和停止Elasticsearch取决于您的系统是使用SysV init还是systemd (由较新的发行版使用)。您可以通过运行此命令来判断正在使用哪个命令:</p>
<pre><code class="language-sh">ps -p 1
</code></pre>
<h4 id="running-elasticsearch-with-sysv-init"><a class="header" href="#running-elasticsearch-with-sysv-init">Running Elasticsearch with SysV <code>init</code></a></h4>
<pre><code class="language-sh">sudo chkconfig --add elasticsearch
sudo -i service elasticsearch start
sudo -i service elasticsearch stop
//日志文件
/var/log/elasticsearch/
</code></pre>
<h4 id="running-elasticsearch-with-systemd"><a class="header" href="#running-elasticsearch-with-systemd">Running Elasticsearch with <code>systemd</code></a></h4>
<pre><code class="language-sh">sudo /bin/systemctl daemon-reload
sudo /bin/systemctl enable elasticsearch.service
sudo systemctl start elasticsearch.service
sudo systemctl stop elasticsearch.service
//日志文件
/var/log/elasticsearch/


</code></pre>
<h5 id="密码文件"><a class="header" href="#密码文件"><strong>密码文件</strong></a></h5>
<p>如果您的Elasticsearch密钥库受到密码保护，则需要使用本地文件和systemd环境变量为systemd提供密钥库密码。</p>
<p>此本地文件应在存在时受到保护，并且一旦Elasticsearch启动并运行，就可以安全地删除该本地文件。</p>
<pre><code class="language-sh">echo &quot;keystore_password&quot; &gt; /path/to/my_pwd_file.tmp
chmod 600 /path/to/my_pwd_file.tmp
sudo systemctl set-environment ES_KEYSTORE_PASSPHRASE_FILE=/path/to/my_pwd_file.tmp
sudo systemctl start elasticsearch.service
</code></pre>
<h5 id="启用journalctl日志"><a class="header" href="#启用journalctl日志"><strong>启用journalctl日志</strong></a></h5>
<p>默认情况下，Elasticsearch服务不会将信息记录在systemd日志中。要启用journalctl日志记录，必须从elasticsearch.service文件中的ExecStart命令行中删除 -- quiet选项。</p>
<pre><code class="language-sh">sudo journalctl -f
sudo journalctl --unit elasticsearch
sudo journalctl --unit elasticsearch --since  &quot;2016-10-30 18:17:16&quot;
</code></pre>
<p>Check <code>man journalctl</code> or https://www.freedesktop.org/software/systemd/man/journalctl.html for more command line options..</p>
<h3 id="checking-that-elasticsearch-is-running"><a class="header" href="#checking-that-elasticsearch-is-running">Checking that Elasticsearch is running</a></h3>
<pre><code class="language-console">GET /
</code></pre>
<pre><code class="language-js">{
  &quot;name&quot; : &quot;Cp8oag6&quot;,
  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,
  &quot;cluster_uuid&quot; : &quot;AT69_T_DTp-1qgIJlatQqA&quot;,
  &quot;version&quot; : {
    &quot;number&quot; : &quot;7.13.4&quot;,
    &quot;build_flavor&quot; : &quot;default&quot;,
    &quot;build_type&quot; : &quot;tar&quot;,
    &quot;build_hash&quot; : &quot;f27399d&quot;,
    &quot;build_date&quot; : &quot;2016-03-30T09:51:41.449Z&quot;,
    &quot;build_snapshot&quot; : false,
    &quot;lucene_version&quot; : &quot;8.8.2&quot;,
    &quot;minimum_wire_compatibility_version&quot; : &quot;1.2.3&quot;,
    &quot;minimum_index_compatibility_version&quot; : &quot;1.2.3&quot;
  },
  &quot;tagline&quot; : &quot;You Know, for Search&quot;
}
</code></pre>
<h3 id="configuring-elasticsearch"><a class="header" href="#configuring-elasticsearch">Configuring Elasticsearch</a></h3>
<ol>
<li>
<p>/etc/elasticsearch目录包含Elasticsearch的默认运行时配置。此目录和所有包含文件的所有权设置为root:elasticsearch。</p>
</li>
<li>
<p>setgid标志在/etc/elasticsearch目录上应用组权限，以确保Elasticsearch可以读取任何包含的文件和子目录。所有文件和子目录都继承 root:elasticsearch 所有权。</p>
</li>
<li>
<p>Running commands from this directory or any subdirectories, such as the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">elasticsearch-keystore tool</a>, requires <code>root:elasticsearch</code> permissions.</p>
</li>
</ol>
<p>默认情况下，Elasticsearch从/etc/elasticsearch.yml文件加载其配置。</p>
<p>此配置文件的格式在 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/settings.html"><em>Configuring Elasticsearch</em></a>.中进行了说明。</p>
<h4 id="系统配置"><a class="header" href="#系统配置">系统配置</a></h4>
<p>RPM还有一个系统配置文件 (/etc/sysconfig/elasticsearch)，它允许您设置以下参数:</p>
<table><thead><tr><th>配置项</th><th>描述</th></tr></thead><tbody>
<tr><td><code>ES_JAVA_HOME</code></td><td>Set a custom Java path to be used.</td></tr>
<tr><td><code>MAX_OPEN_FILES</code></td><td>Maximum number of open files, defaults to <code>65535</code>.</td></tr>
<tr><td><code>MAX_LOCKED_MEMORY</code></td><td>Maximum locked memory size. Set to <code>unlimited</code> if you use the <code>bootstrap.memory_lock</code> option in elasticsearch.yml.</td></tr>
<tr><td><code>MAX_MAP_COUNT</code></td><td>Maximum number of memory map areas a process may have. If you use <code>mmapfs</code> as index store type, make sure this is set to a high value. For more information, check the <a href="https://github.com/torvalds/linux/blob/master/Documentation/sysctl/vm.txt">linux kernel documentation</a> about <code>max_map_count</code>. This is set via <code>sysctl</code> before starting Elasticsearch. Defaults to <code>262144</code>.</td></tr>
<tr><td><code>ES_PATH_CONF</code></td><td>Configuration file directory (which needs to include <code>elasticsearch.yml</code>, <code>jvm.options</code>, and <code>log4j2.properties</code> files); defaults to <code>/etc/elasticsearch</code>.</td></tr>
<tr><td><code>ES_JAVA_OPTS</code></td><td>Any additional JVM system properties you may want to apply.</td></tr>
<tr><td><code>RESTART_ON_UPGRADE</code></td><td>Configure restart on package upgrade, defaults to <code>false</code>. This means you will have to restart your Elasticsearch instance after installing a package manually. The reason for this is to ensure, that upgrades in a cluster do not result in a continuous shard reallocation resulting in high network traffic and reducing the response times of your cluster.</td></tr>
</tbody></table>
<p>使用systemd 配置时 ，可以通过 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/setting-system-settings.html#systemd">Systemd configuration</a> 修改 配置 而不是 /etc/sysconfig/elasticsearch</p>
<h2 id="directory-layout-of-rpm"><a class="header" href="#directory-layout-of-rpm">Directory layout of RPM</a></h2>
<table><thead><tr><th>Type</th><th>Description</th><th>Default Location</th><th>Setting</th></tr></thead><tbody>
<tr><td><strong>home</strong></td><td>Elasticsearch home directory or <code>$ES_HOME</code></td><td><code>/usr/share/elasticsearch</code></td><td></td></tr>
<tr><td><strong>bin</strong></td><td>Binary scripts including <code>elasticsearch</code> to start a node and <code>elasticsearch-plugin</code> to install plugins</td><td><code>/usr/share/elasticsearch/bin</code></td><td></td></tr>
<tr><td><strong>conf</strong></td><td>Configuration files including <code>elasticsearch.yml</code></td><td><code>/etc/elasticsearch</code></td><td><code>ES_PATH_CONF</code></td></tr>
<tr><td><strong>conf</strong></td><td>Environment variables including heap size, file descriptors.</td><td><code>/etc/sysconfig/elasticsearch</code></td><td></td></tr>
<tr><td><strong>data</strong></td><td>The location of the data files of each index / shard allocated on the node.</td><td><code>/var/lib/elasticsearch</code></td><td><code>path.data</code></td></tr>
<tr><td><strong>jdk</strong></td><td>The bundled Java Development Kit used to run Elasticsearch. Can be overridden by setting the <code>ES_JAVA_HOME</code> environment variable in <code>/etc/sysconfig/elasticsearch</code>.</td><td><code>/usr/share/elasticsearch/jdk</code></td><td></td></tr>
<tr><td><strong>logs</strong></td><td>Log files location.</td><td><code>/var/log/elasticsearch</code></td><td><code>path.logs</code></td></tr>
<tr><td><strong>plugins</strong></td><td>Plugin files location. Each plugin will be contained in a subdirectory.</td><td><code>/usr/share/elasticsearch/plugins</code></td><td></td></tr>
<tr><td><strong>repo</strong></td><td>Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here.</td><td>Not configured</td><td><code>path.repo</code></td></tr>
</tbody></table>
<h2 id="管理工具-2"><a class="header" href="#管理工具-2">管理工具</a></h2>
<table><thead><tr><th>工具，名</th><th>链接</th></tr></thead><tbody>
<tr><td>Puppet</td><td><a href="https://github.com/elastic/puppet-elasticsearch">puppet-elasticsearch</a></td></tr>
<tr><td>Chef</td><td><a href="https://github.com/elastic/cookbook-elasticsearch">cookbook-elasticsearch</a></td></tr>
<tr><td>Ansible</td><td><a href="https://github.com/elastic/ansible-elasticsearch">ansible-elasticsearch</a></td></tr>
</tbody></table>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/install-elasticsearch.html#_installing_elasticsearch_yourself">参考链接</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="import-the-elasticsearch-pgp-key"><a class="header" href="#import-the-elasticsearch-pgp-key">Import the Elasticsearch PGP Key</a></h2>
<pre><code class="language-sh">wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

curl -L  https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
</code></pre>
<h2 id="installing-from-the-apt-repository"><a class="header" href="#installing-from-the-apt-repository">Installing from the APT repository</a></h2>
<pre><code class="language-sh">sudo apt-get install apt-transport-https
</code></pre>
<pre><code class="language-sh">echo &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | sudo tee /etc/apt/sources.list.d/elastic-7.x.list
</code></pre>
<p>These instructions do not use <code>add-apt-repository</code> for several reasons:</p>
<ol>
<li>
<p><code>add-apt-repository</code> adds entries to the system <code>/etc/apt/sources.list</code> file rather than a clean per-repository file in <code>/etc/apt/sources.list.d</code></p>
</li>
<li>
<p><code>add-apt-repository</code> is not part of the default install on many distributions and requires a number of non-default dependencies.</p>
</li>
<li>
<p>Older versions of <code>add-apt-repository</code> always add a <code>deb-src</code> entry which will cause errors because we do not provide a source package. If you have added the <code>deb-src</code> entry, you will see an error like the following until you delete the <code>deb-src</code> line:</p>
<pre><code>Unable to find expected entry 'main/source/Sources' in Release file
(Wrong sources.list entry or malformed file)
</code></pre>
</li>
</ol>
<pre><code class="language-sh">sudo apt-get update &amp;&amp; sudo apt-get install elasticsearch
</code></pre>
<p>If two entries exist for the same Elasticsearch repository, you will see an error like this during <code>apt-get update</code>:</p>
<pre><code>Duplicate sources.list entry https://artifacts.elastic.co/packages/7.x/apt/ ...`
</code></pre>
<p>Examine <code>/etc/apt/sources.list.d/elasticsearch-7.x.list</code> for the duplicate entry or locate the duplicate entry amongst the files in <code>/etc/apt/sources.list.d/</code> and the <code>/etc/apt/sources.list</code> file.</p>
<h2 id="手动下载"><a class="header" href="#手动下载">手动下载</a></h2>
<pre><code class="language-shell">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-amd64.deb
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.13.4-amd64.deb.sha512
shasum -a 512 -c elasticsearch-7.13.4-amd64.deb.sha512 
sudo dpkg -i elasticsearch-7.13.4-amd64.deb
</code></pre>
<h3 id="running-elasticsearch-with-sysv-init-1"><a class="header" href="#running-elasticsearch-with-sysv-init-1">Running Elasticsearch with SysV <code>init</code></a></h3>
<p>自启</p>
<pre><code class="language-sh">sudo update-rc.d elasticsearch defaults 95 10
sudo -i service elasticsearch start
sudo -i service elasticsearch stop
</code></pre>
<h3 id="running-elasticsearch-with-systemd-1"><a class="header" href="#running-elasticsearch-with-systemd-1">Running Elasticsearch with <code>systemd</code></a></h3>
<pre><code class="language-sh">sudo /bin/systemctl daemon-reload
sudo /bin/systemctl enable elasticsearch.service
sudo systemctl start elasticsearch.service
sudo systemctl stop elasticsearch.service
</code></pre>
<h3 id="密码文件-1"><a class="header" href="#密码文件-1">密码文件</a></h3>
<pre><code class="language-sh">echo &quot;keystore_password&quot; &gt; /path/to/my_pwd_file.tmp
chmod 600 /path/to/my_pwd_file.tmp
sudo systemctl set-environment ES_KEYSTORE_PASSPHRASE_FILE=/path/to/my_pwd_file.tmp
sudo systemctl start elasticsearch.service
</code></pre>
<h2 id="journactl集成"><a class="header" href="#journactl集成">journactl集成</a></h2>
<p>By default the Elasticsearch service doesn’t log information in the <code>systemd</code> journal. To enable <code>journalctl</code> logging, the <code>--quiet</code> option must be removed from the <code>ExecStart</code> command line in the <code>elasticsearch.service</code> file.</p>
<pre><code class="language-sh">sudo journalctl -f
sudo journalctl --unit elasticsearch
sudo journalctl --unit elasticsearch --since  &quot;2016-10-30 18:17:16&quot;

</code></pre>
<h2 id="directory-layout-of-debian-package"><a class="header" href="#directory-layout-of-debian-package">Directory layout of Debian package</a></h2>
<p>The Debian package places config files, logs, and the data directory in the appropriate locations for a Debian-based system:</p>
<table><thead><tr><th>Type</th><th>Description</th><th>Default Location</th><th>Setting</th></tr></thead><tbody>
<tr><td><strong>home</strong></td><td>Elasticsearch home directory or <code>$ES_HOME</code></td><td><code>/usr/share/elasticsearch</code></td><td></td></tr>
<tr><td><strong>bin</strong></td><td>Binary scripts including <code>elasticsearch</code> to start a node and <code>elasticsearch-plugin</code> to install plugins</td><td><code>/usr/share/elasticsearch/bin</code></td><td></td></tr>
<tr><td><strong>conf</strong></td><td>Configuration files including <code>elasticsearch.yml</code></td><td><code>/etc/elasticsearch</code></td><td><code>ES_PATH_CONF</code></td></tr>
<tr><td><strong>conf</strong></td><td>Environment variables including heap size, file descriptors.</td><td><code>/etc/default/elasticsearch</code></td><td></td></tr>
<tr><td><strong>data</strong></td><td>The location of the data files of each index / shard allocated on the node.</td><td><code>/var/lib/elasticsearch</code></td><td><code>path.data</code></td></tr>
<tr><td><strong>jdk</strong></td><td>The bundled Java Development Kit used to run Elasticsearch. Can be overridden by setting the <code>ES_JAVA_HOME</code> environment variable in <code>/etc/default/elasticsearch</code>.</td><td><code>/usr/share/elasticsearch/jdk</code></td><td></td></tr>
<tr><td><strong>logs</strong></td><td>Log files location.</td><td><code>/var/log/elasticsearch</code></td><td><code>path.logs</code></td></tr>
<tr><td><strong>plugins</strong></td><td>Plugin files location. Each plugin will be contained in a subdirectory.</td><td><code>/usr/share/elasticsearch/plugins</code></td><td></td></tr>
<tr><td><strong>repo</strong></td><td>Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here.</td><td>Not configured</td><td><code>path.repo</code>e</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="全文搜索"><a class="header" href="#全文搜索">全文搜索</a></h1>
<p>全文搜索两个最重要的方面是：</p>
<ul>
<li><strong>相关性（Relevance）</strong></li>
<li>**分析（Analysis）**它是将文本块转换为有区别的、规范化的 token 的一个过程，目的是为了创建倒排索引</li>
</ul>
<h2 id="基于词项与基于全文"><a class="header" href="#基于词项与基于全文">基于词项与基于全文</a></h2>
<p>所有查询会或多或少的执行相关度计算，但不是所有查询都有分析阶段，和一些特殊的完全不会对文本进行操作的查询（如 <code>bool</code> 或 <code>function_score</code> ）不同，文本查询可以划分成两大家族：</p>
<p><strong>基于词项的查询</strong></p>
<p>如 <code>term</code> 或 <code>fuzzy</code> 这样的底层查询不需要分析阶段，它们对单个词项进行操作</p>
<ol>
<li>用 <code>term</code> 查询词项 <code>Foo</code> 只要在倒排索引中查找 <em>准确词项</em> ，</li>
<li>并且用 TF/IDF 算法为每个包含该词项的文档计算相关度评分 <code>_score</code> 。</li>
</ol>
<p><code>term</code> 查询只对倒排索引的词项精确匹配，这点很重要，它不会对词的多样性进行处理（如， <code>foo</code> 或 <code>FOO</code> ）</p>
<p><strong>基于全文的查询</strong></p>
<p>像 <code>match</code> 或 <code>query_string</code> 这样的查询是高层查询，它们了解字段映射的信息：</p>
<ul>
<li>如果查询 <code>日期（date）</code> 或 <code>整数（integer）</code> 字段，它们会将查询字符串分别作为日期或整数对待。</li>
<li>如果查询一个（ <code>not_analyzed</code> ）未分析的精确值字符串字段，它们会将整个查询字符串作为单个词项对待。</li>
<li>但如果要查询一个（ <code>analyzed</code> ）已分析的全文字段，它们会先将查询字符串传递到一个合适的分析器，然后生成一个供查询的词项列表。</li>
</ul>
<p>一旦组成了词项列表，这个查询会对每个词项逐一执行底层的查询，再将结果合并，然后为每个文档生成一个最终的相关度评分。</p>
<p>我们很少直接使用基于词项的搜索，通常情况下都是对全文进行查询，这只需要简单的执行一个高层全文查询（进而在高层查询内部会以基于词项的底层查询完成搜索）。</p>
<h2 id="匹配查询"><a class="header" href="#匹配查询">匹配查询</a></h2>
<p>匹配查询 <code>match</code> 是个 <em>核心</em> 查询。无论需要查询什么字段， <code>match</code> 查询都应该会是首选的查询方式。它是一个高级 <em>全文查询</em> ，这表示它既能处理全文字段，又能处理精确字段。</p>
<p>这就是说， <code>match</code> 查询主要的应用场景就是进行全文搜索，我们以下面一个简单例子来说明全文搜索是如何工作的：</p>
<pre><code class="language-sense">GET /my_index/my_type/_search
{
    &quot;query&quot;: {
        &quot;match&quot;: {
            &quot;title&quot;: &quot;QUICK!&quot;
        }
    }
}
</code></pre>
<p>Elasticsearch 执行上面这个 <code>match</code> 查询的步骤是：</p>
<ol>
<li>
<p><em>检查字段类型</em> 。</p>
<p>标题 <code>title</code> 字段是一个 <code>string</code> 类型（ <code>analyzed</code> ）已分析的全文字段，这意味着查询字符串本身也应该被分析。</p>
</li>
<li>
<p><em>分析查询字符串</em> </p>
<ol>
<li>将查询的字符串 <code>QUICK!</code> 传入标准分析器中，输出的结果是单个项 <code>quick</code> 。因为只有一个单词项，所以 <code>match</code> 查询执行的是单个底层 <code>term</code> 查询。</li>
</ol>
</li>
<li>
<p><em>查找匹配文档</em> 。</p>
<ol>
<li>用 <code>term</code> 查询在倒排索引中查找 <code>quick</code> 然后获取一组包含该项的文档，本例的结果是文档：1、2 和 3 。</li>
</ol>
</li>
<li>
<p><em>为每个文档评分</em> 。</p>
<ol>
<li>用 <code>term</code> 查询计算每个文档相关度评分 <code>_score</code> ，这是种将词频（term frequency，即词 <code>quick</code> 在相关文档的 <code>title</code> 字段中出现的频率）和反向文档频率（inverse document frequency，即词 <code>quick</code> 在所有文档的 <code>title</code> 字段中出现的频率），以及字段的长度（即字段越短相关度越高）相结合的计算方式</li>
</ol>
</li>
</ol>
<h2 id="多词查询"><a class="header" href="#多词查询">多词查询</a></h2>
<p>幸运的是 <code>match</code> 查询让多词查询变得简单：</p>
<pre><code class="language-sense">GET /my_index/my_type/_search
{
    &quot;query&quot;: {
        &quot;match&quot;: {
            &quot;title&quot;: &quot;BROWN DOG!&quot;
        }
    }
}
</code></pre>
<p>因为 <code>match</code> 查询必须查找两个词（ <code>[&quot;brown&quot;,&quot;dog&quot;]</code> ），它在内部实际上先执行两次 <code>term</code> 查询，然后将两次查询的结果合并作为最终结果输出。为了做到这点，它将两个 <code>term</code> 查询包入一个 <code>bool</code> 查询中，</p>
<p>上面这个查询返回所有四个文档：</p>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;:      &quot;4&quot;,
        &quot;_score&quot;:   0.73185337, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Brown fox brown dog&quot;
        }
     },
     {
        &quot;_id&quot;:      &quot;2&quot;,
        &quot;_score&quot;:   0.47486103, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;The quick brown fox jumps over the lazy dog&quot;
        }
     },
     {
        &quot;_id&quot;:      &quot;3&quot;,
        &quot;_score&quot;:   0.47486103, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;The quick brown fox jumps over the quick dog&quot;
        }
     },
     {
        &quot;_id&quot;:      &quot;1&quot;,
        &quot;_score&quot;:   0.11914785, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;The quick brown fox&quot;
        }
     }
  ]
}
</code></pre>
<ol>
<li>文档 4 最相关，因为它包含词 &quot;brown&quot; 两次以及 &quot;dog&quot; 一次。</li>
<li>文档 2、3 同时包含 brown 和 dog 各一次，而且它们 title 字段的长度相同，所以具有相同的评分。</li>
<li>文档 1 也能匹配，尽管它只有 <code>brown</code> 没有 <code>dog</code> 。</li>
</ol>
<p>因为 <code>match</code> 查询必须查找两个词（ <code>[&quot;brown&quot;,&quot;dog&quot;]</code> ），它在内部实际上先执行两次 <code>term</code> 查询，然后将两次查询的结果合并作为最终结果输出。为了做到这点，它将两个 <code>term</code> 查询包入一个 <code>bool</code> 查询中</p>
<h3 id="提高精度"><a class="header" href="#提高精度">提高精度</a></h3>
<p>用 <em>任意</em> 查询词项匹配文档可能会导致结果中出现不相关的长尾。<strong>这是种散弹式搜索</strong>。可能我们只想搜索包含 <em>所有</em> 词项的文档，也就是说，不去匹配 <code>brown OR dog</code> ，而通过匹配 <code>brown AND dog</code> 找到所有文档。</p>
<p><code>match</code> 查询还可以接受 <code>operator</code> 操作符作为输入参数，默认情况下该操作符是 <code>or</code> 。我们可以将它修改成 <code>and</code> 让所有指定词项都必须匹配：</p>
<pre><code class="language-sense">GET /my_index/my_type/_search
{
    &quot;query&quot;: {
        &quot;match&quot;: {
            &quot;title&quot;: {      
                &quot;query&quot;:    &quot;BROWN DOG!&quot;,
                &quot;operator&quot;: &quot;and&quot;
            }
        }
    }
}
</code></pre>
<h3 id="控制精度"><a class="header" href="#控制精度">控制精度</a></h3>
<p>在 <em>所有</em> 与 <em>任意</em> 间二选一有点过于非黑即白。</p>
<p>如果用户给定 5 个查询词项，想查找只包含其中 4 个的文档，该如何处理？</p>
<p><code>match</code> 查询支持 <code>minimum_should_match</code> 最小匹配参数，这让我们可以指定必须匹配的词项数用来表示一个文档是否相关。我们可以将其设置为某个具体数字，更常用的做法是将其设置为一个百分数，因为我们无法控制用户搜索时输入的单词数量：</p>
<pre><code class="language-sense">GET /my_index/my_type/_search
{
  &quot;query&quot;: {
    &quot;match&quot;: {
      &quot;title&quot;: {
        &quot;query&quot;:                &quot;quick brown dog&quot;,
        &quot;minimum_should_match&quot;: &quot;75%&quot;
      }
    }
  }
}
</code></pre>
<p>当给定百分比的时候， <code>minimum_should_match</code> 会做合适的事情：在之前三词项的示例中， <code>75%</code> 会自动被截断成 <code>66.6%</code> ，即三个里面两个词。无论这个值设置成什么，至少包含一个词项的文档才会被认为是匹配的。</p>
<p>参数 <code>minimum_should_match</code> 的设置非常灵活，可以根据用户输入词项的数目应用不同的规则。完整的信息参考文档 https://www.elastic.co/guide/en/elasticsearch/reference/5.6/query-dsl-minimum-should-match.html#query-dsl-minimum-should-match</p>
<h2 id="组合查询"><a class="header" href="#组合查询">组合查询</a></h2>
<p>与<strong>bool</strong>过滤器一样， <code>bool</code> 查询也可以接受 <code>must</code> 、 <code>must_not</code> 和 <code>should</code> 参数下的多个查询语句。比如：</p>
<pre><code class="language-sense">GET /my_index/my_type/_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;must&quot;:     { &quot;match&quot;: { &quot;title&quot;: &quot;quick&quot; }},
      &quot;must_not&quot;: { &quot;match&quot;: { &quot;title&quot;: &quot;lazy&quot;  }},
      &quot;should&quot;: [
                  { &quot;match&quot;: { &quot;title&quot;: &quot;brown&quot; }},
                  { &quot;match&quot;: { &quot;title&quot;: &quot;dog&quot;   }}
      ]
    }
  }
}
</code></pre>
<ol>
<li>以上的查询结果返回 <code>title</code> 字段包含词项 <code>quick</code> 但不包含 <code>lazy</code> 的任意文档。目前为止，这与 <code>bool</code> 过滤器的工作方式非常相似。</li>
<li>区别就在于两个 <code>should</code> 语句，也就是说：一个文档不必包含 <code>brown</code> 或 <code>dog</code> 这两个词项，但如果一旦包含，我们就认为它们 <em>更相关</em> ：</li>
</ol>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;:      &quot;3&quot;,
        &quot;_score&quot;:   0.70134366, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;The quick brown fox jumps over the quick dog&quot;
        }
     },
     {
        &quot;_id&quot;:      &quot;1&quot;,
        &quot;_score&quot;:   0.3312608,
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;The quick brown fox&quot;
        }
     }
  ]
}
</code></pre>
<p>文档 3 会比文档 1 有更高评分是因为它同时包含 <code>brown</code> 和 <code>dog</code> 。</p>
<h3 id="评分计算"><a class="header" href="#评分计算">评分计算</a></h3>
<p><code>bool</code> 查询会为每个文档计算相关度评分 <code>_score</code> ，再将所有匹配的 <code>must</code> 和 <code>should</code> 语句的分数 <code>_score</code> 求和，最后除以 <code>must</code> 和 <code>should</code> 语句的总数。</p>
<p><code>must_not</code> 语句不会影响评分；它的作用只是将不相关的文档排除。</p>
<h3 id="控制精度-1"><a class="header" href="#控制精度-1">控制精度</a></h3>
<ol>
<li>
<p>所有 <code>must</code> 语句必须匹配，所有 <code>must_not</code> 语句都必须不匹配</p>
</li>
<li>
<p>但有多少 <code>should</code> 语句应该匹配呢？默认情况下，<strong>没有 <code>should</code> 语句是必须匹配的</strong></p>
</li>
<li>
<p>那就是当没有 <code>must</code> 语句的时候，至少有一个 <code>should</code> 语句必须匹配。</p>
</li>
</ol>
<p>就像我们能控制 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/match-multi-word.html#match-precision"><code>match</code> 查询的精度</a> 一样，我们可以通过 <code>minimum_should_match</code> 参数控制需要匹配的 <code>should</code> 语句的数量，它既可以是一个绝对的数字，又可以是个百分比：</p>
<h2 id="布尔匹配与match"><a class="header" href="#布尔匹配与match">布尔匹配与Match</a></h2>
<p>目前为止，可能已经意识到<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/match-multi-word.html">多词 <code>match</code> 查询</a>只是简单地将生成的 <code>term</code> 查询包裹在一个 <code>bool</code> 查询中。如果使用默认的 <code>or</code> 操作符，每个 <code>term</code> 查询都被当作 <code>should</code> 语句，这样就要求必须至少匹配一条语句。以下两个查询是等价的：</p>
<pre><code class="language-js">{
    &quot;match&quot;: { &quot;title&quot;: &quot;brown fox&quot;}
}
</code></pre>
<pre><code class="language-js">{
  &quot;bool&quot;: {
    &quot;should&quot;: [
      { &quot;term&quot;: { &quot;title&quot;: &quot;brown&quot; }},
      { &quot;term&quot;: { &quot;title&quot;: &quot;fox&quot;   }}
    ]
  }
}
</code></pre>
<p>如果使用 <code>and</code> 操作符，所有的 <code>term</code> 查询都被当作 <code>must</code> 语句，所以 <em>所有（all）</em> 语句都必须匹配。以下两个查询是等价的：</p>
<pre><code class="language-js">{
    &quot;match&quot;: {
        &quot;title&quot;: {
            &quot;query&quot;:    &quot;brown fox&quot;,
            &quot;operator&quot;: &quot;and&quot;
        }
    }
}
</code></pre>
<pre><code class="language-js">{
  &quot;bool&quot;: {
    &quot;must&quot;: [
      { &quot;term&quot;: { &quot;title&quot;: &quot;brown&quot; }},
      { &quot;term&quot;: { &quot;title&quot;: &quot;fox&quot;   }}
    ]
  }
</code></pre>
<p>如果指定参数 <code>minimum_should_match</code> ，它可以通过 <code>bool</code> 查询直接传递，使以下两个查询等价：</p>
<pre><code class="language-js">{
    &quot;match&quot;: {
        &quot;title&quot;: {
            &quot;query&quot;:                &quot;quick brown fox&quot;,
            &quot;minimum_should_match&quot;: &quot;75%&quot;
        }
    }
}
</code></pre>
<pre><code class="language-js">{
  &quot;bool&quot;: {
    &quot;should&quot;: [
      { &quot;term&quot;: { &quot;title&quot;: &quot;brown&quot; }},
      { &quot;term&quot;: { &quot;title&quot;: &quot;fox&quot;   }},
      { &quot;term&quot;: { &quot;title&quot;: &quot;quick&quot; }}
    ],
    &quot;minimum_should_match&quot;: 2 
  }
}
</code></pre>
<p>因为只有三条语句，<code>match</code> 查询的参数 <code>minimum_should_match</code> 值 75% 会被截断成 <code>2</code> 。即三条 <code>should</code> 语句中至少有两条必须匹配。</p>
<h2 id="查询语句提升权重"><a class="header" href="#查询语句提升权重">查询语句提升权重</a></h2>
<p>当然 <code>bool</code> 查询不仅限于组合简单的单个词 <code>match</code> 查询，它可以组合任意其他的查询，以及其他 <code>bool</code> 查询。普遍的用法是通过<strong>汇总多个独立查询的分数</strong>，从而达到为每个文档微调其相关度评分 <code>_score</code> 的目的。</p>
<p>假设想要查询关于 “full-text search（全文搜索）” 的文档，但我们希望为提及 “Elasticsearch” 或 “Lucene” 的文档给予更高的 <em>权重</em> ，这里 <em>更高权重</em> 是指如果文档中出现 “Elasticsearch” 或 “Lucene” ，它们会比没有的出现这些词的文档获得更高的相关度评分 <code>_score</code> ，也就是说，它们会出现在结果集的更上面。</p>
<pre><code class="language-sense">GET /_search
{
    &quot;query&quot;: {
        &quot;bool&quot;: {
            &quot;must&quot;: {
                &quot;match&quot;: {
                    &quot;content&quot;: { 
                        &quot;query&quot;:    &quot;full text search&quot;,
                        &quot;operator&quot;: &quot;and&quot;
                    }
                }
            },
            &quot;should&quot;: [ 
                { &quot;match&quot;: { &quot;content&quot;: &quot;Elasticsearch&quot; }},
                { &quot;match&quot;: { &quot;content&quot;: &quot;Lucene&quot;        }}
            ]
        }
    }
}
</code></pre>
<ol>
<li>content 字段必须包含 full 、 text 和 search 所有三个词。</li>
<li>如果 content 字段也包含 Elasticsearch 或 Lucene ，文档会获得更高的评分 _score 。</li>
</ol>
<p>但是如果我们想让包含 <code>Lucene</code> 的有更高的权重，并且包含 <code>Elasticsearch</code> 的语句比 <code>Lucene</code> 的权重更高，该如何处理?</p>
<pre><code class="language-sense">GET /_search
{
    &quot;query&quot;: {
        &quot;bool&quot;: {
            &quot;must&quot;: {
                &quot;match&quot;: {  //这些语句使用默认的 boost 值 1 。
                    &quot;content&quot;: {
                        &quot;query&quot;:    &quot;full text search&quot;,
                        &quot;operator&quot;: &quot;and&quot;
                    }
                }
            },
            &quot;should&quot;: [
                { &quot;match&quot;: {
                    &quot;content&quot;: {
                        &quot;query&quot;: &quot;Elasticsearch&quot;,
                        &quot;boost&quot;: 3  //这条语句更为重要，因为它有最高的 boost 值。
                    }
                }},
                { &quot;match&quot;: {
                    &quot;content&quot;: {
                        &quot;query&quot;: &quot;Lucene&quot;,
                        &quot;boost&quot;: 2  //这条语句比使用默认值的更重要，但它的重要性不及 Elasticsearch 语句。
                    }
                }}
            ]
        }
    }
}
</code></pre>
<p><code>boost</code> 参数被用来提升一个语句的相对权重（ <code>boost</code> 值大于 <code>1</code> ）或降低相对权重（ <code>boost</code> 值处于 <code>0</code> 到 <code>1</code> 之间），但是这种提升或降低并不是线性的，换句话说，如果一个 <code>boost</code> 值为 <code>2</code> ，并不能获得两倍的评分 <code>_score</code> 。</p>
<p>相反，新的评分 <code>_score</code> 会在应用权重提升之后被 <em>归一化</em> ，每种类型的查询都有自己的归一算法，细节超出了本书的范围，所以不作介绍。<strong>简单的说，更高的 <code>boost</code> 值为我们带来更高的评分 <code>_score</code> 。</strong></p>
<p>如果不基于 TF/IDF 要实现自己的评分模型，我们就需要对权重提升的过程能有更多控制，可以使用 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/function-score-query.html"><code>function_score</code> 查询</a>操纵一个文档的权重提升方式而跳过归一化这一步骤。</p>
<h2 id="控制分析"><a class="header" href="#控制分析">控制分析</a></h2>
<p>查询只能查找倒排索引表中真实存在的项，所以保证文档在索引时与查询字符串在搜索时应用相同的分析过程非常重要，<strong>这样查询的项才能够匹配倒排索引中的项</strong>。</p>
<p>不过分析器可以由每个字段决定。每个字段都可以有不同的分析器，既可以通过配置为字段指定分析器，也可以使用更高层的类型（type）、索引（index）或节点（node）的默认配置。在索引时，一个字段值是根据配置或默认分析器分析的。</p>
<p>例如为 <code>my_index</code> 新增一个字段：</p>
<pre><code class="language-sense">PUT /my_index/_mapping/my_type
{
    &quot;my_type&quot;: {
        &quot;properties&quot;: {
            &quot;english_title&quot;: {
                &quot;type&quot;:     &quot;string&quot;,
                &quot;analyzer&quot;: &quot;english&quot;
            }
        }
    }
}
</code></pre>
<p>现在我们就可以通过使用 <code>analyze</code> API 来分析单词 <code>Foxes</code> ，进而比较 <code>english_title</code> 字段和 <code>title</code> 字段在索引时的分析结果：</p>
<pre><code class="language-sense">//字段 title ，使用默认的 standard 标准分析器，返回词项 foxes 。
GET /my_index/_analyze
{
  &quot;field&quot;: &quot;my_type.title&quot;,   
  &quot;text&quot;: &quot;Foxes&quot;
}
//字段 english_title ，使用 english 英语分析器，返回词项 fox 。
GET /my_index/_analyze
{
  &quot;field&quot;: &quot;my_type.english_title&quot;,   
  &quot;text&quot;: &quot;Foxes&quot;
}
</code></pre>
<p>如果使用底层 <code>term</code> 查询精确项 <code>fox</code> 时， <code>english_title</code> 字段会匹配但 <code>title</code> 字段不会。</p>
<h3 id="默认分析器"><a class="header" href="#默认分析器">默认分析器</a></h3>
<p>分析器可以从三个层面进行定义：按字段（per-field）、按索引（per-index）或全局缺省（global default）。Elasticsearch 会按照以下顺序依次处理，直到它找到能够使用的分析器。索引时的顺序如下：</p>
<ul>
<li>字段映射里定义的 <code>analyzer</code> ，否则</li>
<li>索引设置中名为 <code>default</code> 的分析器，默认为</li>
<li><code>standard</code> 标准分析器</li>
</ul>
<p>在搜索时，顺序有些许不同：</p>
<ul>
<li>查询自己定义的 <code>analyzer</code> ，否则</li>
<li>字段映射里定义的 <code>analyzer</code> ，否则</li>
<li>索引设置中名为 <code>default</code> 的分析器，默认为</li>
<li><code>standard</code> 标准分析器</li>
</ul>
<p>有时，在索引时和搜索时使用不同的分析器是合理的。</p>
<p>我们可能要想为同义词建索引（例如，所有 <code>quick</code> 出现的地方，同时也为 <code>fast</code> 、 <code>rapid</code> 和 <code>speedy</code> 创建索引）。</p>
<p>但在搜索时，我们不需要搜索所有的同义词，取而代之的是寻找用户输入的单词是否是 <code>quick</code> 、 <code>fast</code> 、 <code>rapid</code> 或 <code>speedy</code> 。</p>
<p>为了区分，Elasticsearch 也支持一个可选的 <code>search_analyzer</code> 映射，它仅会应用于搜索时（ <code>analyzer</code> 还用于索引时）。还有一个等价的 <code>default_search</code> 映射，用以指定索引层的默认配置。</p>
<p>如果考虑到这些额外参数，一个搜索时的 <em>完整</em> 顺序会是下面这样：</p>
<ul>
<li>查询自己定义的 <code>analyzer</code> ，否则</li>
<li>字段映射里定义的 <code>search_analyzer</code> ，否则</li>
<li>字段映射里定义的 <code>analyzer</code> ，否则</li>
<li>索引设置中名为 <code>default_search</code> 的分析器，默认为</li>
<li>索引设置中名为 <code>default</code> 的分析器，默认为</li>
<li><code>standard</code> 标准分析器</li>
</ul>
<h2 id="被破坏的相关度"><a class="header" href="#被破坏的相关度">被破坏的相关度！</a></h2>
<p>在讨论更复杂的 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-field-search.html">多字段搜索</a> 之前，让我们先快速解释一下为什么只在主分片上 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/match-query.html#match-test-data">创建测试索引</a> 。</p>
<p>用户会时不时的抱怨无法按相关度排序并提供简短的重现步骤</p>
<p>用户索引了一些文档，运行一个简单的查询，然后发现明显低相关度的结果出现在高相关度结果之上。</p>
<p>我们在两个主分片上创建了索引和总共 10 个文档，其中 6 个文档有单词 <code>foo</code> 。可能是分片 1 有其中 3 个 <code>foo</code> 文档，而分片 2 有其中另外 3 个文档，换句话说，所有文档是均匀分布存储的。</p>
<p><strong>相关度</strong></p>
<p>在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/relevance-intro.html">什么是相关度？</a>中，我们描述了 Elasticsearch 默认使用的相似度算法，这个算法叫做 <em>词频/逆向文档频率</em> 或 TF/IDF 。词频是计算某个词在当前被查询文档里某个字段中出现的频率，出现的频率越高，文档越相关。 <em>逆向文档频率</em> 将 <em>某个词在索引内所有文档出现的百分数</em> 考虑在内，出现的频率越高，它的权重就越低。</p>
<p>但是由于性能原因， Elasticsearch 不会计算索引内所有文档的 IDF 。相反，每个分片会根据 <em>该分片</em> 内的所有文档计算一个本地 IDF 。</p>
<p>因为文档是均匀分布存储的，两个分片的 IDF 是相同的。相反，设想如果有 5 个 <code>foo</code> 文档存于分片 1 ，而第 6 个文档存于分片 2 ，在这种场景下， <code>foo</code> 在一个分片里非常普通（所以不那么重要），但是在另一个分片里非常出现很少（所以会显得更重要）。这些 IDF 之间的差异会导致不正确的结果。</p>
<p>在实际应用中，这并不是一个问题，本地和全局的 IDF 的差异会随着索引里文档数的增多渐渐消失，在真实世界的数据量下，局部的 IDF 会被迅速均化，所以上述问题并不是相关度被破坏所导致的，而是由于数据太少。</p>
<p>为了测试，我们可以通过两种方式解决这个问题。第一种是只在主分片上创建索引，正如 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/match-query.html"><code>match</code> 查询</a> 里介绍的那样，如果只有一个分片，那么本地的 IDF <em>就是</em> 全局的 IDF。</p>
<p>第二个方式就是在搜索请求后添加 <code>?search_type=dfs_query_then_fetch</code> ， <code>dfs</code> 是指 <em>分布式频率搜索（Distributed Frequency Search）</em> ， 它告诉 Elasticsearch ，先分别获得每个分片本地的 IDF ，然后根据结果再计算整个索引的全局 IDF 。</p>
<p>不要在生产环境上使用 <code>dfs_query_then_fetch</code> 。完全没有必要。只要有足够的数据就能保证词频是均匀分布的。没有理由给每个查询额外加上 DFS 这步。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="多字段搜索"><a class="header" href="#多字段搜索">多字段搜索</a></h1>
<p>查询很少是简单一句话的 <code>match</code> 匹配查询</p>
<p>通常我们需要用相同或不同的字符串查询一个或多个字段，也就是说，需要对多个查询语句以及它们相关度评分进行合理的合并。</p>
<h2 id="多字符串查询"><a class="header" href="#多字符串查询">多字符串查询</a></h2>
<p>最简单的多字段查询可以将搜索项映射到具体的字段</p>
<p>如果我们知道 <em>War and Peace</em> 是标题，Leo Tolstoy 是作者，很容易就能把两个条件用 <code>match</code> 语句表示，并将它们用 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/bool-query.html"><code>bool</code> 查询</a> 组合起来：</p>
<pre><code class="language-sense">GET /_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        { &quot;match&quot;: { &quot;title&quot;:  &quot;War and Peace&quot; }},
        { &quot;match&quot;: { &quot;author&quot;: &quot;Leo Tolstoy&quot;   }}
      ]
    }
  }
}
</code></pre>
<p><code>bool</code> 查询采取 <em>more-matches-is-better</em> 匹配越多越好的方式，</p>
<p>所以每条 <code>match</code> 语句的评分结果会被加在一起，从而为每个文档提供最终的分数 <code>_score</code> 。能与两条语句同时匹配的文档比只与一条语句匹配的文档得分要高。</p>
<p>当然，并不是只能使用 <code>match</code> 语句：可以用 <code>bool</code> 查询来包裹组合任意其他类型的查询，甚至包括其他的 <code>bool</code> 查询。我们可以在上面的示例中添加一条语句来指定译者版本的偏好：.</p>
<pre><code class="language-sense">GET /_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        { &quot;match&quot;: { &quot;title&quot;:  &quot;War and Peace&quot; }},
        { &quot;match&quot;: { &quot;author&quot;: &quot;Leo Tolstoy&quot;   }},
        { &quot;bool&quot;:  {
          &quot;should&quot;: [
            { &quot;match&quot;: { &quot;translator&quot;: &quot;Constance Garnett&quot; }},
            { &quot;match&quot;: { &quot;translator&quot;: &quot;Louise Maude&quot;      }}
          ]
        }}
      ]
    }
  }
}
</code></pre>
<p>为什么将译者条件语句放入另一个独立的 <code>bool</code> 查询中呢？所有的四个 <code>match</code> 查询都是 <code>should</code> 语句，所以为什么不将 translator 语句与其他如 title 、 author 这样的语句放在同一层呢？</p>
<p>答案在于评分的计算方式。 <code>bool</code> 查询运行每个 <code>match</code> 查询，再把评分加在一起，然后将结果与所有匹配的语句数量相乘，最后除以所有的语句数量。处于同一层的每条语句具有相同的权重。在前面这个例子中，包含 translator 语句的 <code>bool</code> 查询，<strong>只占总评分的三分之一</strong>。如果将 translator 语句与 title 和 author 两条语句放入同一层，那么 title 和 author 语句<strong>只贡献四分之一评分</strong>。</p>
<h3 id="语句的优先级"><a class="header" href="#语句的优先级">语句的优先级</a></h3>
<p>前例中每条语句贡献三分之一评分的这种方式可能并不是我们想要的，我们可能对 title 和 author 两条语句更感兴趣，这样就需要调整查询，使 title 和 author 语句相对来说更重要。</p>
<p>在武器库中，最容易使用的就是 <code>boost</code> 参数。为了提升 <code>title</code> 和 <code>author</code> 字段的权重，为它们分配的 <code>boost</code> 值大于 <code>1</code> ：</p>
<pre><code class="language-sense">GET /_search
{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        { &quot;match&quot;: { 
            &quot;title&quot;:  {
              &quot;query&quot;: &quot;War and Peace&quot;,
              &quot;boost&quot;: 2
        }}},
        { &quot;match&quot;: { 
            &quot;author&quot;:  {
              &quot;query&quot;: &quot;Leo Tolstoy&quot;,
              &quot;boost&quot;: 2
        }}},
        { &quot;bool&quot;:  { 
            &quot;should&quot;: [
              { &quot;match&quot;: { &quot;translator&quot;: &quot;Constance Garnett&quot; }},
              { &quot;match&quot;: { &quot;translator&quot;: &quot;Louise Maude&quot;      }}
            ]
        }}
      ]
    }
  }
}
</code></pre>
<p>要获取 <code>boost</code> 参数 “最佳” 值，较为简单的方式就是不断试错：设定 <code>boost</code> 值，运行测试查询，如此反复。 <code>boost</code> 值比较合理的区间处于 <code>1</code> 到 <code>10</code> 之间，当然也有可能是 <code>15</code> 。如果为 <code>boost</code> 指定比这更高的值，将不会对最终的评分结果产生更大影响，因为评分是被 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/_boosting_query_clauses.html#boost-normalization">归一化的（normalized）</a> 。</p>
<h2 id="单字符串查询"><a class="header" href="#单字符串查询">单字符串查询</a></h2>
<p><code>bool</code> 查询是多语句查询的主干。它的适用场景很多，特别是当需要将不同查询字符串映射到不同字段的时候。</p>
<p>问题在于，目前有些用户期望将所有的搜索项堆积到单个字段中，并期望应用程序能为他们提供正确的结果</p>
<p>有意思的是多字段搜索的表单通常被称为 <em>高级查询 （Advanced Search）</em> —— 只是因为它对用户而言是高级的，而多字段搜索的实现却非常简单。</p>
<p>对于多词（multiword）、多字段（multifield）查询来说，不存在简单的 <em>万能</em> 方案。为了获得最好结果，需要 <em>了解我们的数据</em> ，并了解如何使用合适的工具。</p>
<h3 id="了解我们的数据"><a class="header" href="#了解我们的数据">了解我们的数据</a></h3>
<p>当用户输入了单个字符串查询的时候，通常会遇到以下三种情形：</p>
<p><strong>最佳字段</strong></p>
<p>当搜索词语具体概念的时候，比如 “brown fox” ，词组比各自独立的单词更有意义。像 <code>title</code> 和 <code>body</code> 这样的字段，尽管它们之间是相关的，但同时又彼此相互竞争。文档在 <em>相同字段</em> 中包含的词越多越好，评分也来自于 <em>最匹配字段</em> 。</p>
<p><strong>多数字段</strong></p>
<p>为了对相关度进行微调，常用的一个技术就是将相同的数据索引到不同的字段，它们各自具有独立的分析链。</p>
<p>主字段可能包括它们的词源、同义词以及 <em>变音词</em> 或口音词，被用来匹配尽可能多的文档。</p>
<p>相同的文本被索引到其他字段，以提供更精确的匹配。一个字段可以包括未经词干提取过的原词，另一个字段包括其他词源、口音，还有一个字段可以提供 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/proximity-matching.html">词语相似性</a> 信息的瓦片词（shingles）。</p>
<p>其他字段是作为匹配每个文档时提高相关度评分的 <em>信号</em> ， <em>匹配字段越多</em> 则越好。</p>
<p><strong>混合字段</strong></p>
<p>对于某些实体，我们需要在多个字段中确定其信息，单个字段都只能作为整体的一部分：</p>
<ul>
<li>Person： <code>first_name</code> 和 <code>last_name</code> （人：名和姓）</li>
<li>Book： <code>title</code> 、 <code>author</code> 和 <code>description</code> （书：标题、作者、描述）</li>
<li>Address： <code>street</code> 、 <code>city</code> 、 <code>country</code> 和 <code>postcode</code> （地址：街道、市、国家和邮政编码）</li>
</ul>
<p>在这种情况下，我们希望在 <em>任何</em> 这些列出的字段中找到尽可能多的词，这有如在一个大字段中进行搜索，这个大字段包括了所有列出的字段。</p>
<h2 id="最佳字段"><a class="header" href="#最佳字段">最佳字段</a></h2>
<p>假设有个网站允许用户搜索博客的内容，以下面两篇博客内容文档为例：</p>
<pre><code class="language-sense">PUT /my_index/my_type/1
{
    &quot;title&quot;: &quot;Quick brown rabbits&quot;,
    &quot;body&quot;:  &quot;Brown rabbits are commonly seen.&quot;
}

PUT /my_index/my_type/2
{
    &quot;title&quot;: &quot;Keeping pets healthy&quot;,
    &quot;body&quot;:  &quot;My quick brown fox eats rabbits on a regular basis.&quot;
}
</code></pre>
<p>用户输入词组 “Brown fox” 然后点击搜索按钮。事先，我们并不知道用户的搜索项是会在 <code>title</code> 还是在 <code>body</code> 字段中被找到，但是，用户很有可能是想搜索相关的词组。用肉眼判断，文档 2 的匹配度更高，因为它同时包括要查找的两个词：</p>
<pre><code class="language-sense">{
    &quot;query&quot;: {
        &quot;bool&quot;: {
            &quot;should&quot;: [
                { &quot;match&quot;: { &quot;title&quot;: &quot;Brown fox&quot; }},
                { &quot;match&quot;: { &quot;body&quot;:  &quot;Brown fox&quot; }}
            ]
        }
    }
}
</code></pre>
<p>但是我们发现查询的结果是文档 1 的评分更高：</p>
<ol>
<li>它会执行 <code>should</code> 语句中的两个查询。</li>
<li>加和两个查询的评分。</li>
<li>乘以匹配语句的总数。</li>
<li>除以所有语句总数（这里为：2）。</li>
</ol>
<p>文档 1 的两个字段都包含 <code>brown</code> 这个词，所以两个 <code>match</code> 语句都能成功匹配并且有一个评分。文档 2 的 <code>body</code> 字段同时包含 <code>brown</code> 和 <code>fox</code> 这两个词，但 <code>title</code> 字段没有包含任何词。这样， <code>body</code> 查询结果中的高分，加上 <code>title</code> 查询中的 0 分，然后乘以二分之一，就得到比文档 1 更低的整体评分。</p>
<p>在本例中， <code>title</code> 和 <code>body</code> 字段是相互竞争的关系，所以就需要找到<strong>单个 <em>最佳匹配</em></strong> 的字段。</p>
<p>如果不是简单将每个字段的评分结果加在一起，<strong>而是将 <em>最佳匹配</em> 字段的评分作为查询的整体评分，</strong></p>
<p>结果会怎样？这样返回的结果可能是： <em>同时</em> 包含 <code>brown</code> 和 <code>fox</code> 的单个字段比反复出现相同词语的多个不同字段有更高的相关度。</p>
<h3 id="dis_max-查询"><a class="header" href="#dis_max-查询">dis_max 查询</a></h3>
<p>不使用 <code>bool</code> 查询，可以使用 <code>dis_max</code> 即分离 <em>最大化查询（Disjunction Max Query）</em> 。</p>
<p>分离（Disjunction）的意思是 <em>或（or）</em> ，这与可以把结合（conjunction）理解成 <em>与（and）</em> 相对应</p>
<p>分离最大化查询（Disjunction Max Query）指的是： <em>将任何与任一查询匹配的文档作为结果返回，但只将最佳匹配的评分作为查询的评分结果返回</em> ：</p>
<pre><code class="language-sense">{
    &quot;query&quot;: {
        &quot;dis_max&quot;: {
            &quot;queries&quot;: [
                { &quot;match&quot;: { &quot;title&quot;: &quot;Brown fox&quot; }},
                { &quot;match&quot;: { &quot;body&quot;:  &quot;Brown fox&quot; }}
            ]
        }
    }
}
</code></pre>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;:      &quot;2&quot;,
        &quot;_score&quot;:   0.21509302,
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Keeping pets healthy&quot;,
           &quot;body&quot;:  &quot;My quick brown fox eats rabbits on a regular basis.&quot;
        }
     },
     {
        &quot;_id&quot;:      &quot;1&quot;,
        &quot;_score&quot;:   0.12713557,
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Quick brown rabbits&quot;,
           &quot;body&quot;:  &quot;Brown rabbits are commonly seen.&quot;
        }
     }
  ]
}
</code></pre>
<h2 id="最佳字段查询调优"><a class="header" href="#最佳字段查询调优">最佳字段查询调优</a></h2>
<p>当用户搜索 “quick pets” 时会发生什么呢？在前面的例子中，两个文档都包含词 <code>quick</code> ，但是只有文档 2 包含词 <code>pets</code> ，两个文档中都不具有同时包含 <em>两个词</em> 的 <em>相同字段</em> 。</p>
<pre><code class="language-sense">{
    &quot;query&quot;: {
        &quot;dis_max&quot;: {
            &quot;queries&quot;: [
                { &quot;match&quot;: { &quot;title&quot;: &quot;Quick pets&quot; }},
                { &quot;match&quot;: { &quot;body&quot;:  &quot;Quick pets&quot; }}
            ]
        }
    }
}
</code></pre>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;: &quot;1&quot;,
        &quot;_score&quot;: 0.12713557, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Quick brown rabbits&quot;,
           &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;
        }
     },
     {
        &quot;_id&quot;: &quot;2&quot;,
        &quot;_score&quot;: 0.12713557, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Keeping pets healthy&quot;,
           &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;
        }
     }
   ]
}
</code></pre>
<h3 id="tie_breaker-参数"><a class="header" href="#tie_breaker-参数">tie_breaker 参数</a></h3>
<p>可以通过指定 <code>tie_breaker</code> 这个参数将其他匹配语句的评分也考虑其中：</p>
<pre><code class="language-sense">{
    &quot;query&quot;: {
        &quot;dis_max&quot;: {
            &quot;queries&quot;: [
                { &quot;match&quot;: { &quot;title&quot;: &quot;Quick pets&quot; }},
                { &quot;match&quot;: { &quot;body&quot;:  &quot;Quick pets&quot; }}
            ],
            &quot;tie_breaker&quot;: 0.3
        }
    }
}
</code></pre>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;: &quot;2&quot;,
        &quot;_score&quot;: 0.14757764, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Keeping pets healthy&quot;,
           &quot;body&quot;: &quot;My quick brown fox eats rabbits on a regular basis.&quot;
        }
     },
     {
        &quot;_id&quot;: &quot;1&quot;,
        &quot;_score&quot;: 0.124275915, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Quick brown rabbits&quot;,
           &quot;body&quot;: &quot;Brown rabbits are commonly seen.&quot;
        }
     }
   ]
}
</code></pre>
<p><code>tie_breaker</code> 参数提供了一种 <code>dis_max</code> 和 <code>bool</code> 之间的折中选择，它的评分方式如下：</p>
<ol>
<li>获得最佳匹配语句的评分 <code>_score</code> 。</li>
<li>将其他匹配语句的评分结果与 <code>tie_breaker</code> 相乘。</li>
<li>对以上评分求和并规范化。</li>
</ol>
<p>有了 <code>tie_breaker</code> ，会考虑所有匹配语句，但最佳匹配语句依然占最终结果里的很大一部分。</p>
<p><code>tie_breaker</code> 可以是 <code>0</code> 到 <code>1</code> 之间的浮点数，其中 <code>0</code> 代表使用 <code>dis_max</code> 最佳匹配语句的普通逻辑， <code>1</code> 表示所有匹配语句同等重要。最佳的精确值需要根据数据与查询调试得出，但是合理值应该与零接近（处于 <code>0.1 - 0.4</code> 之间），这样就不会颠覆 <code>dis_max</code> 最佳匹配性质的根本。</p>
<h2 id="multi_match-查询"><a class="header" href="#multi_match-查询">multi_match 查询</a></h2>
<p><code>multi_match</code> 查询为能在多个字段上反复执行相同查询提供了一种便捷方式。</p>
<p><code>multi_match</code> 多匹配查询的类型有多种，其中的三种恰巧与 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/_single_query_string.html#know-your-data">了解我们的数据</a> 中介绍的三个场景对应，即： <code>best_fields</code> 、 <code>most_fields</code> 和 <code>cross_fields</code> （最佳字段、多数字段、跨字段）。</p>
<p>默认情况下，查询的类型是 <code>best_fields</code> ，这表示它会为每个字段生成一个 <code>match</code> 查询，然后将它们组合到 <code>dis_max</code> 查询的内部，如下：</p>
<pre><code class="language-js">{
  &quot;dis_max&quot;: {
    &quot;queries&quot;:  [
      {
        &quot;match&quot;: {
          &quot;title&quot;: {
            &quot;query&quot;: &quot;Quick brown fox&quot;,
            &quot;minimum_should_match&quot;: &quot;30%&quot;
          }
        }
      },
      {
        &quot;match&quot;: {
          &quot;body&quot;: {
            &quot;query&quot;: &quot;Quick brown fox&quot;,
            &quot;minimum_should_match&quot;: &quot;30%&quot;
          }
        }
      },
    ],
    &quot;tie_breaker&quot;: 0.3
  }
}
</code></pre>
<p>上面这个查询用 <code>multi_match</code> 重写成更简洁的形式：</p>
<pre><code class="language-sense">{
    &quot;multi_match&quot;: {
        &quot;query&quot;:                &quot;Quick brown fox&quot;,
        &quot;type&quot;:                 &quot;best_fields&quot;,  //best_fields 类型是默认值，可以不指定。
        &quot;fields&quot;:               [ &quot;title&quot;, &quot;body&quot; ],
        &quot;tie_breaker&quot;:          0.3,
        &quot;minimum_should_match&quot;: &quot;30%&quot; //如 minimum_should_match 或 operator 这样的参数会被传递到生成的 match 查询中。
    }
}
</code></pre>
<h3 id="查询字段名称的模糊匹配"><a class="header" href="#查询字段名称的模糊匹配">查询字段名称的模糊匹配</a></h3>
<p>字段名称可以用模糊匹配的方式给出：<strong>任何与模糊模式正则匹配的字段都会被包括在搜索条件中</strong>，例如可以使用以下方式同时匹配 <code>book_title</code> 、 <code>chapter_title</code> 和 <code>section_title</code> （书名、章名、节名）这三个字段：</p>
<pre><code class="language-js">{
    &quot;multi_match&quot;: {
        &quot;query&quot;:  &quot;Quick brown fox&quot;,
        &quot;fields&quot;: &quot;*_title&quot;
    }
}
</code></pre>
<h3 id="提升单个字段的权重"><a class="header" href="#提升单个字段的权重">提升单个字段的权重</a></h3>
<p>可以使用 <code>^</code> 字符语法为单个字段提升权重，在字段名称的末尾添加 <code>^boost</code> ，其中 <code>boost</code> 是一个浮点数：</p>
<pre><code class="language-js">{
    &quot;multi_match&quot;: {
        &quot;query&quot;:  &quot;Quick brown fox&quot;,
        &quot;fields&quot;: [ &quot;*_title&quot;, &quot;chapter_title^2&quot; ] 
    }
}
</code></pre>
<p>chapter_title 这个字段的 boost 值为 2 ，而其他两个字段 book_title 和 section_title 字段的默认 boost 值为 1 。</p>
<h2 id="多数字段"><a class="header" href="#多数字段">多数字段</a></h2>
<p>全文搜索被称作是 <em>召回率（Recall）</em> 与 <em>精确率（Precision）</em> 的战场</p>
<p><em>召回率</em> ——返回所有的相关文档； <em>精确率</em> ——不返回无关文档</p>
<p><strong>目的是在结果的第一页中为用户呈现最为相关的文档。</strong></p>
<p>提高全文相关性精度的常用方式是为同一文本建立多种方式的索引，每种方式都提供了一个不同的相关度信号 <em>signal</em> 。主字段会以尽可能多的形式的去匹配尽可能多的文档</p>
<ul>
<li>使用词干提取来索引 <code>jumps</code> 、 <code>jumping</code> 和 <code>jumped</code> 样的词，将 <code>jump</code> 作为它们的词根形式。这样即使用户搜索 <code>jumped</code> ，也还是能找到包含 <code>jumping</code> 的匹配的文档。</li>
<li>将同义词包括其中，如 <code>jump</code> 、 <code>leap</code> 和 <code>hop</code> 。</li>
<li>移除变音或口音词：如 <code>ésta</code> 、 <code>está</code> 和 <code>esta</code> 都会以无变音形式 <code>esta</code> 来索引。</li>
</ul>
<p>如果我们有两个文档，其中一个包含词 <code>jumped</code> ，另一个包含词 <code>jumping</code> ，用户很可能期望前者能排的更高，因为它正好与输入的搜索条件一致。</p>
<p>为了达到目的，我们可以将相同的文本索引到其他字段从而提供更为精确的匹配。</p>
<p>一个字段可能是为词干未提取过的版本，</p>
<p>另一个字段可能是变音过的原始词，</p>
<p>第三个可能使用 <em>shingles</em> 提供 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/proximity-matching.html">词语相似性</a> 信息。</p>
<p>这些附加的字段可以看成提高每个文档的相关度评分的信号 <em>signals</em> ，能匹配字段的越多越好。</p>
<p>一个文档如果与广度匹配的主字段相匹配，那么它会出现在结果列表中。如果文档同时又与 <em>signal</em> 信号字段匹配，那么它会获得额外加分，系统会提升它在结果列表中的位置。</p>
<p>我们会在本书稍后对同义词、词相似性、部分匹配以及其他潜在的信号进行讨论，但这里只使用词干已提取（stemmed）和未提取（unstemmed）的字段作为简单例子来说明这种技术</p>
<h3 id="多字段映射"><a class="header" href="#多字段映射">多字段映射</a></h3>
<p>首先要做的事情就是对我们的字段索引两次：一次使用词干模式以及一次非词干模式。为了做到这点，采用 <em>multifields</em> 来实现，已经在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-fields.html">multifields</a> 有所介绍：</p>
<pre><code class="language-sense">DELETE /my_index

PUT /my_index
{
    &quot;settings&quot;: { &quot;number_of_shards&quot;: 1 }, 
    &quot;mappings&quot;: {
        &quot;my_type&quot;: {
            &quot;properties&quot;: {
                &quot;title&quot;: { 
                    &quot;type&quot;:     &quot;string&quot;,
                    &quot;analyzer&quot;: &quot;english&quot;, //title 字段使用 english 英语分析器来提取词干。
                    &quot;fields&quot;: {
                        &quot;std&quot;:   { 
                            &quot;type&quot;:     &quot;string&quot;,
                            &quot;analyzer&quot;: &quot;standard&quot; //title.std 字段使用 standard 标准分析器，所以没有词干提取。
                        }
                    }
                }
            }
        }
    }
}
</code></pre>
<pre><code class="language-sense">PUT /my_index/my_type/1
{ &quot;title&quot;: &quot;My rabbit jumps&quot; }

PUT /my_index/my_type/2
{ &quot;title&quot;: &quot;Jumping jack rabbits&quot; }
</code></pre>
<pre><code class="language-sense">GET /my_index/_search
{
   &quot;query&quot;: {
        &quot;match&quot;: {
            &quot;title&quot;: &quot;jumping rabbits&quot;
        }
    }
}
</code></pre>
<p>因为有了 <code>english</code> 分析器，这个查询是在查找以 <code>jump</code> 和 <code>rabbit</code> 这两个被提取词的文档。两个文档的 <code>title</code> 字段都同时包括这两个词，所以两个文档得到的评分也相同：</p>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;: &quot;1&quot;,
        &quot;_score&quot;: 0.42039964,
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;My rabbit jumps&quot;
        }
     },
     {
        &quot;_id&quot;: &quot;2&quot;,
        &quot;_score&quot;: 0.42039964,
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Jumping jack rabbits&quot;
        }
     }
  ]
}
</code></pre>
<p>如果同时查询两个字段，然后使用 <code>bool</code> 查询将评分结果 <em>合并</em> ，那么两个文档都是匹配的（ <code>title</code> 字段的作用），而且文档 2 的相关度评分更高（ <code>title.std</code> 字段的作用）：</p>
<pre><code class="language-sense">GET /my_index/_search
{
   &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:  &quot;jumping rabbits&quot;,
            &quot;type&quot;:   &quot;most_fields&quot;, 
            &quot;fields&quot;: [ &quot;title&quot;, &quot;title.std&quot; ]
        }
    }
}
</code></pre>
<pre><code class="language-js">{
  &quot;hits&quot;: [
     {
        &quot;_id&quot;: &quot;2&quot;,
        &quot;_score&quot;: 0.8226396, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;Jumping jack rabbits&quot;
        }
     },
     {
        &quot;_id&quot;: &quot;1&quot;,
        &quot;_score&quot;: 0.10741998, 
        &quot;_source&quot;: {
           &quot;title&quot;: &quot;My rabbit jumps&quot;
        }
     }
  ]
}
</code></pre>
<p>用广度匹配字段 <code>title</code> 包括尽可能多的文档——以提升召回率——</p>
<p>同时又使用字段 <code>title.std</code> 作为 <em>信号</em> 将相关度更高的文档置于结果顶部。</p>
<p>每个字段对于最终评分的贡献可以通过自定义值 <code>boost</code> 来控制。比如，使 <code>title</code> 字段更为重要，这样同时也降低了其他信号字段的作用：</p>
<pre><code class="language-sense">GET /my_index/_search
{
   &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;jumping rabbits&quot;,
            &quot;type&quot;:        &quot;most_fields&quot;,
            &quot;fields&quot;:      [ &quot;title^10&quot;, &quot;title.std&quot; ] 
        }
    }
}
</code></pre>
<h2 id="跨字段实体搜索"><a class="header" href="#跨字段实体搜索">跨字段实体搜索</a></h2>
<p>现在讨论一种普遍的搜索模式：跨字段实体搜索（cross-fields entity search）</p>
<p>如 <code>person</code> 、 <code>product</code> 或 <code>address</code> （人、产品或地址）这样的实体中，需要使用多个字段来唯一标识它的信息。 <code>person</code> 实体可能是这样索引的：</p>
<pre><code class="language-js">{
    &quot;firstname&quot;:  &quot;Peter&quot;,
    &quot;lastname&quot;:   &quot;Smith&quot;
}
</code></pre>
<p>或地址：</p>
<pre><code class="language-js">{
    &quot;street&quot;:   &quot;5 Poland Street&quot;,
    &quot;city&quot;:     &quot;London&quot;,
    &quot;country&quot;:  &quot;United Kingdom&quot;,
    &quot;postcode&quot;: &quot;W1V 3DG&quot;
}
</code></pre>
<p>在本例中，我们想使用 <em>单个</em> 字符串在多个字段中进行搜索。</p>
<h3 id="简单的方式"><a class="header" href="#简单的方式">简单的方式</a></h3>
<p>依次查询每个字段并将每个字段的匹配评分结果相加，听起来真像是 <code>bool</code> 查询：</p>
<pre><code class="language-js">{
  &quot;query&quot;: {
    &quot;bool&quot;: {
      &quot;should&quot;: [
        { &quot;match&quot;: { &quot;street&quot;:    &quot;Poland Street W1V&quot; }},
        { &quot;match&quot;: { &quot;city&quot;:      &quot;Poland Street W1V&quot; }},
        { &quot;match&quot;: { &quot;country&quot;:   &quot;Poland Street W1V&quot; }},
        { &quot;match&quot;: { &quot;postcode&quot;:  &quot;Poland Street W1V&quot; }}
      ]
    }
  }
}
</code></pre>
<p>为每个字段重复查询字符串会使查询瞬间变得冗长，可以采用 <code>multi_match</code> 查询，将 <code>type</code> 设置成 <code>most_fields</code> 然后告诉 Elasticsearch 合并所有匹配字段的评分：</p>
<pre><code class="language-js">{
  &quot;query&quot;: {
    &quot;multi_match&quot;: {
      &quot;query&quot;:       &quot;Poland Street W1V&quot;,
      &quot;type&quot;:        &quot;most_fields&quot;,
      &quot;fields&quot;:      [ &quot;street&quot;, &quot;city&quot;, &quot;country&quot;, &quot;postcode&quot; ]
    }
  }
}
</code></pre>
<h3 id="most_fields-方式的问题"><a class="header" href="#most_fields-方式的问题">most_fields 方式的问题</a></h3>
<p>用 <code>most_fields</code> 这种方式搜索也存在某些问题，这些问题并不会马上显现：</p>
<ul>
<li>它是为多数字段匹配 <em>任意</em> 词设计的，而不是在 <em>所有字段</em> 中找到最匹配的。</li>
<li>它不能使用 <code>operator</code> 或 <code>minimum_should_match</code> 参数来降低次相关结果造成的长尾效应。</li>
<li>词频对于每个字段是不一样的，而且它们之间的相互影响会导致不好的排序结果。</li>
</ul>
<h2 id="字段中心式查询"><a class="header" href="#字段中心式查询">字段中心式查询</a></h2>
<p>以上三个源于 <code>most_fields</code> 的问题都因为它是 <em>字段中心式（field-centric）</em> 而不是 <em>词中心式（term-centric）</em> 的：当真正感兴趣的是匹配词的时候，它为我们查找的是最匹配的 <em>字段</em> 。</p>
<p><code>best_fields</code> 类型也是字段中心式的，它也存在类似的问题。</p>
<p>首先查看这些问题存在的原因，再想如何解决它们。</p>
<h3 id="问题-1-在多个字段中匹配相同的词"><a class="header" href="#问题-1-在多个字段中匹配相同的词">问题 1 ：在多个字段中匹配相同的词</a></h3>
<p>回想一下 <code>most_fields</code> 查询是如何执行的：Elasticsearch 为每个字段生成独立的 <code>match</code> 查询，再用 <code>bool</code> 查询将他们包起来。</p>
<p>可以通过 <code>validate-query</code> API 查看：</p>
<pre><code class="language-sense">GET /_validate/query?explain
{
  &quot;query&quot;: {
    &quot;multi_match&quot;: {
      &quot;query&quot;:   &quot;Poland Street W1V&quot;,
      &quot;type&quot;:    &quot;most_fields&quot;,
      &quot;fields&quot;:  [ &quot;street&quot;, &quot;city&quot;, &quot;country&quot;, &quot;postcode&quot; ]
    }
  }
}
</code></pre>
<p>生成 <code>explanation</code> 解释：</p>
<pre><code>(street:poland   street:street   street:w1v)
(city:poland     city:street     city:w1v)
(country:poland  country:street  country:w1v)
(postcode:poland postcode:street postcode:w1v)
</code></pre>
<h3 id="问题-2-剪掉长尾"><a class="header" href="#问题-2-剪掉长尾">问题 2 ：剪掉长尾</a></h3>
<p>在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/match-multi-word.html#match-precision">匹配精度</a> 中，我们讨论过使用 <code>and</code> 操作符或设置 <code>minimum_should_match</code> 参数来消除结果中几乎不相关的长尾，或许可以尝试以下方式：</p>
<pre><code class="language-sense">{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;Poland Street W1V&quot;,
            &quot;type&quot;:        &quot;most_fields&quot;,
            &quot;operator&quot;:    &quot;and&quot;, 
            &quot;fields&quot;:      [ &quot;street&quot;, &quot;city&quot;, &quot;country&quot;, &quot;postcode&quot; ]
        }
    }
}
</code></pre>
<p>但是对于 <code>best_fields</code> 或 <code>most_fields</code> 这些参数会在 <code>match</code> 查询生成时被传入，这个查询的 <code>explanation</code> 解释如下：</p>
<pre><code>(+street:poland   +street:street   +street:w1v)
(+city:poland     +city:street     +city:w1v)
(+country:poland  +country:street  +country:w1v)
(+postcode:poland +postcode:street +postcode:w1v)
</code></pre>
<h3 id="问题-3-词频"><a class="header" href="#问题-3-词频">问题 3 ：词频</a></h3>
<ul>
<li>
<p><strong>词频</strong></p>
<p>一个词在单个文档的某个字段中出现的频率越高，这个文档的相关度就越高。</p>
</li>
<li>
<p><strong>逆向文档频率</strong></p>
<p>一个词在所有文档某个字段索引中出现的频率越高，这个词的相关度就越低。</p>
</li>
</ul>
<p>当搜索多个字段时，TF/IDF 会带来某些令人意外的结果。</p>
<p>想想用字段 <code>first_name</code> 和 <code>last_name</code> 查询 “Peter Smith” 的例子， Peter 是个平常的名 Smith 也是平常的姓，这两者都具有较低的 IDF 值。但当索引中有另外一个人的名字是 “Smith Williams” 时， Smith 作为名来说很不平常，以致它有一个较高的 IDF 值！</p>
<p>下面这个简单的查询可能会在结果中将 “Smith Williams” 置于 “Peter Smith” 之上，尽管事实上是第二个人比第一个人更为匹配。</p>
<pre><code class="language-sense">{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;Peter Smith&quot;,
            &quot;type&quot;:        &quot;most_fields&quot;,
            &quot;fields&quot;:      [ &quot;*_name&quot; ]
        }
    }
}
</code></pre>
<p>这里的问题是 <code>smith</code> 在名字段中具有高 IDF ，它会削弱 “Peter” 作为名和 “Smith” 作为姓时低 IDF 的所起作用。</p>
<h3 id="解决方案"><a class="header" href="#解决方案">解决方案</a></h3>
<p>存在这些问题仅仅是因为我们在处理着多个字段，如果将所有这些字段组合成单个字段，问题就会消失。可以为 <code>person</code> 文档添加 <code>full_name</code> 字段来解决这个问题：</p>
<pre><code class="language-js">{
    &quot;first_name&quot;:  &quot;Peter&quot;,
    &quot;last_name&quot;:   &quot;Smith&quot;,
    &quot;full_name&quot;:   &quot;Peter Smith&quot;
}
</code></pre>
<p>当查询 <code>full_name</code> 字段时：</p>
<ul>
<li>具有更多匹配词的文档会比只有一个重复匹配词的文档更重要。</li>
<li><code>minimum_should_match</code> 和 <code>operator</code> 参数会像期望那样工作。</li>
<li>姓和名的逆向文档频率被合并，所以 Smith 到底是作为姓还是作为名出现，都会变得无关紧要。</li>
</ul>
<h2 id="自定义-_all-字段"><a class="header" href="#自定义-_all-字段">自定义 _all 字段</a></h2>
<p>在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/root-object.html#all-field">all-field</a> 字段中，我们解释过 <code>_all</code> 字段的索引方式是将所有其他字段的值作为一个大字符串索引的。然而这么做并不十分灵活，为了灵活我们可以给人名添加一个自定义 <code>_all</code> 字段，再为地址添加另一个 <code>_all</code> 字段。</p>
<p>Elasticsearch 在字段映射中为我们提供 <code>copy_to</code> 参数来实现这个功能：</p>
<pre><code class="language-sense">PUT /my_index
{
    &quot;mappings&quot;: {
        &quot;person&quot;: {
            &quot;properties&quot;: {
                &quot;first_name&quot;: {
                    &quot;type&quot;:     &quot;string&quot;,
                    &quot;copy_to&quot;:  &quot;full_name&quot; 
                },
                &quot;last_name&quot;: {
                    &quot;type&quot;:     &quot;string&quot;,
                    &quot;copy_to&quot;:  &quot;full_name&quot; 
                },
                &quot;full_name&quot;: {
                    &quot;type&quot;:     &quot;string&quot;
                }
            }
        }
    }
}
</code></pre>
<p><code>first_name</code> 和 <code>last_name</code> 字段中的值会被复制到 <code>full_name</code> 字段。</p>
<p>有了这个映射，我们可以用 <code>first_name</code> 来查询名，用 <code>last_name</code> 来查询姓，或者直接使用 <code>full_name</code> 查询整个姓名。</p>
<p><code>first_name</code> 和 <code>last_name</code> 的映射并不影响 <code>full_name</code> 如何被索引， <code>full_name</code> 将两个字段的内容复制到本地，然后根据 <code>full_name</code> 的映射自行索引。</p>
<p><code>copy_to</code> 设置对<a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/multi-fields.html">multi-field</a>无效。如果尝试这样配置映射，Elasticsearch 会抛异常。</p>
<h2 id="跨字段实体搜索-1"><a class="header" href="#跨字段实体搜索-1">跨字段实体搜索</a></h2>
<ul>
<li>搜索  username 为xxx，password 为 xxx</li>
</ul>
<h2 id="字段中心式与词中心式"><a class="header" href="#字段中心式与词中心式">字段中心式与词中心式</a></h2>
<p><strong>字段中心式：为每个字段 生成一个 match查询</strong></p>
<pre><code class="language-json">GET /_validate/query?explain
{
  &quot;query&quot;: {
    &quot;multi_match&quot;: {
      &quot;query&quot;:   &quot;Poland Street W1V&quot;,
      &quot;type&quot;:    &quot;most_fields&quot;,
      &quot;fields&quot;:  [ &quot;street&quot;, &quot;city&quot;, &quot;country&quot;, &quot;postcode&quot; ]
    }
  }
}

(street:poland   street:street   street:w1v)
(city:poland     city:street     city:w1v)
(country:poland  country:street  country:w1v)
(postcode:poland postcode:street postcode:w1v)
</code></pre>
<p><strong>词中心式</strong>：为每个词在所有 的字段中 查找匹配的文档，每个文档都要包含该词</p>
<p><strong>出现的问题</strong></p>
<ul>
<li>多词匹配：多个字段匹配多个词项导致的相关度计算有误，场景：两个字段 同时匹配 poland 比 一个 字段匹配 poland street 的相关度要高 </li>
<li>长尾：多个字段匹配多个词项导致许多细小的匹配</li>
<li>反向文档词频：搜索 <strong>Peter Smith</strong>  可能会在结果中将 “Smith Williams” 置于 “Peter Smith” 之上，因为 Smith作为lastName 的IDF过高会拉低分数
<ul>
<li>解决方案：first_name与 last_name合并为 full_name这样IDF就会被 合并</li>
</ul>
</li>
</ul>
<h2 id="cross-fields-跨字段查询"><a class="header" href="#cross-fields-跨字段查询">cross-fields 跨字段查询</a></h2>
<p>自定义 <code>_all</code> 的方式是一个好的解决方案，只需在索引文档前为其设置好映射</p>
<p>不过， Elasticsearch 还在搜索时提供了相应的解决方案：使用 <code>cross_fields</code> 类型进行 <code>multi_match</code> 查询。</p>
<p><code>cross_fields</code> 使用词中心式（term-centric）的查询方式,这与 <code>best_fields</code> 和 <code>most_fields</code> 使用字段中心式（field-centric）的查询方式非常不同</p>
<p>它将所有字段当成一个大字段，并在 <em>每个字段</em> 中查找 <em>每个词</em> 。</p>
<p>为了说明字段中心式（field-centric）与词中心式（term-centric）这两种查询方式的不同，先看看以下字段中心式的 <code>most_fields</code> 查询的 <code>explanation</code> 解释：</p>
<pre><code class="language-sense">GET /_validate/query?explain
{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;peter smith&quot;,
            &quot;type&quot;:        &quot;most_fields&quot;,
            &quot;operator&quot;:    &quot;and&quot;, 
            &quot;fields&quot;:      [ &quot;first_name&quot;, &quot;last_name&quot; ]
        }
    }
}
</code></pre>
<p>对于匹配的文档， <code>peter</code> 和 <code>smith</code> 都必须同时出现在相同字段中，要么是 <code>first_name</code> 字段，要么 <code>last_name</code> 字段：</p>
<pre><code>(+first_name:peter +first_name:smith)
(+last_name:peter  +last_name:smith)
</code></pre>
<p><em>词中心式</em> 会使用以下逻辑：</p>
<pre><code>+(first_name:peter last_name:peter)
+(first_name:smith last_name:smith)
</code></pre>
<p>换句话说，词 <code>peter</code> 和 <code>smith</code> 都必须出现，但是可以出现在任意字段中。</p>
<p><code>cross_fields</code> 类型首先分析查询字符串并生成一个词列表，然后它从所有字段中依次搜索每个词</p>
<pre><code class="language-sense">GET /_validate/query?explain
{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;peter smith&quot;,
            &quot;type&quot;:        &quot;cross_fields&quot;, 
            &quot;operator&quot;:    &quot;and&quot;,
            &quot;fields&quot;:      [ &quot;first_name&quot;, &quot;last_name&quot; ]
        }
    }
}
</code></pre>
<p>用 <code>cross_fields</code> 词中心式匹配。</p>
<p>它通过 <em>混合</em> 不同字段逆向索引文档频率的方式解决了词频的问题：</p>
<pre><code>+blended(&quot;peter&quot;, fields: [first_name, last_name])
+blended(&quot;smith&quot;, fields: [first_name, last_name])
</code></pre>
<p>换句话说，它会同时在 <code>first_name</code> 和 <code>last_name</code> 两个字段中查找 <code>smith</code> 的 IDF ，然后用两者的最小值作为两个字段的 IDF 。结果实际上就是 <code>smith</code> 会被认为既是个平常的姓，也是平常的名。</p>
<p>为了让 <code>cross_fields</code> 查询以最优方式工作，所有的字段都须使用相同的分析器，具有相同分析器的字段会被分组在一起作为混合字段使用。</p>
<p>如果包括了不同分析链的字段，它们会以 <code>best_fields</code> 的相同方式被加入到查询结果中。例如：我们将 <code>title</code> 字段加到之前的查询中（假设它们使用的是不同的分析器）， explanation 的解释结果如下：</p>
<pre><code>(+title:peter +title:smith)
(
  +blended(&quot;peter&quot;, fields: [first_name, last_name])
  +blended(&quot;smith&quot;, fields: [first_name, last_name])
)
</code></pre>
<p>当在使用 <code>minimum_should_match</code> 和 <code>operator</code> 参数时，这点尤为重要。</p>
<h3 id="按字段提高权重"><a class="header" href="#按字段提高权重">按字段提高权重</a></h3>
<p>采用 <code>cross_fields</code> 查询与 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/custom-all.html">自定义 <code>_all</code> 字段</a> 相比，其中一个优势就是它可以在搜索时为单个字段提升权重。</p>
<p>这对像 <code>first_name</code> 和 <code>last_name</code> 具有相同值的字段并不是必须的，但如果要用 <code>title</code> 和 <code>description</code> 字段搜索图书，可能希望为 <code>title</code> 分配更多的权重，这同样可以使用前面介绍过的 <code>^</code> 符号语法来实现：</p>
<pre><code class="language-js">GET /books/_search
{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;peter smith&quot;,
            &quot;type&quot;:        &quot;cross_fields&quot;,
            &quot;fields&quot;:      [ &quot;title^2&quot;, &quot;description&quot; ] 
        }
    }
}
</code></pre>
<p>自定义单字段查询是否能够优于多字段查询，取决于在多字段查询与单字段自定义 <code>_all</code> 之间代价的权衡，即哪种解决方案会带来更大的性能优化就选择哪一种。</p>
<h2 id="exact-value-精确值字段"><a class="header" href="#exact-value-精确值字段">Exact-Value 精确值字段</a></h2>
<p>在结束多字段查询这个话题之前，我们最后要讨论的是精确值 <code>not_analyzed</code> 未分析字段。将 <code>not_analyzed</code> 字段与 <code>multi_match</code> 中 <code>analyzed</code> 字段混在一起没有多大用处。</p>
<p>原因可以通过查看查询的 explanation 解释得到，设想将 <code>title</code> 字段设置成 <code>not_analyzed</code> ：</p>
<pre><code class="language-sense">GET /_validate/query?explain
{
    &quot;query&quot;: {
        &quot;multi_match&quot;: {
            &quot;query&quot;:       &quot;peter smith&quot;,
            &quot;type&quot;:        &quot;cross_fields&quot;,
            &quot;fields&quot;:      [ &quot;title&quot;, &quot;first_name&quot;, &quot;last_name&quot; ]
        }
    }
}
</code></pre>
<p>因为 <code>title</code> 字段是未分析过的，Elasticsearch 会将 “peter smith” 这个完整的字符串作为查询条件来搜索！</p>
<pre><code>title:peter smith
(
    blended(&quot;peter&quot;, fields: [first_name, last_name])
    blended(&quot;smith&quot;, fields: [first_name, last_name])
)
</code></pre>
<p>显然这个项不在 <code>title</code> 的倒排索引中，所以需要在 <code>multi_match</code> 查询中避免使用 <code>not_analyzed</code> 字段。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="精确值查找"><a class="header" href="#精确值查找">精确值查找</a></h1>
<p>当进行精确值查找时， 我们会使用过滤器（filters）。过滤器很重要，因为它们执行速度非常快，不会计算相关度（直接跳过了整个评分阶段）而且很容易被缓存。</p>
<h2 id="term-查询数字"><a class="header" href="#term-查询数字">term 查询数字</a></h2>
<p>可以用它处理数字（numbers）、布尔值（Booleans）、日期（dates）以及文本（text）。</p>
<p><code>term</code> 查询会查找我们指定的精确值。</p>
<p><strong>非评分</strong></p>
<p>通常当查找一个精确值的时候，我们不希望对查询进行评分计算。只希望对文档进行包括或排除的计算，所以我们会使用 <code>constant_score</code> 查询以非评分模式来执行 <code>term</code> 查询并以一作为统一评分。</p>
<pre><code class="language-sense">GET /my_store/products/_search
{
    &quot;query&quot; : {
        &quot;constant_score&quot; : { 
            &quot;filter&quot; : {
                &quot;term&quot; : { 
                    &quot;price&quot; : 20
                }
            }
        }
    }
}
</code></pre>
<h2 id="term-查询文本"><a class="header" href="#term-查询文本">term 查询文本</a></h2>
<pre><code class="language-sense">GET /my_store/products/_search
{
    &quot;query&quot; : {
        &quot;constant_score&quot; : {
            &quot;filter&quot; : {
                &quot;term&quot; : {
                    &quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot;
                }
            }
        }
    }
}
</code></pre>
<p>这里有几点需要注意：</p>
<ul>
<li>
<p>Elasticsearch 用 4 个不同的 token 而不是单个 token 来表示这个 UPC 。</p>
</li>
<li>
<p>所有字母都是小写的。</p>
</li>
<li>
<p>丢失了连字符和哈希符（ <code>#</code> ）。</p>
</li>
</ul>
<p>所以当我们用 <code>term</code> 查询查找精确值 <code>XHDK-A-1293-#fJ3</code> 的时候，找不到任何文档，因为它并不在我们的倒排索引中，正如前面呈现出的分析结果，索引里有四个 token 。</p>
<h2 id="内部过滤器的操作"><a class="header" href="#内部过滤器的操作">内部过滤器的操作</a></h2>
<ol>
<li><em>查找匹配文档</em>.</li>
</ol>
<p>​	<code>term</code> 查询在倒排索引中查找 <code>XHDK-A-1293-#fJ3</code> 然后获取包含该 term 的所有文档。本例中，只有文档 1 满足我们要求。</p>
<pre><code>2. *创建 bitset*.

过滤器会创建一个 *bitset* （一个包含 0 和 1 的数组），它描述了哪个文档会包含该 term 。匹配文档的标志位是 1 。本例中，bitset 的值为 `[1,0,0,0]` 。在内部，它表示成一个 [&quot;roaring bitmap&quot;](https://www.elastic.co/blog/frame-of-reference-and-roaring-bitmaps)，可以同时对稀疏或密集的集合进行高效编码。

3. *迭代 bitset(s)*

一旦为每个查询生成了 bitsets ，Elasticsearch 就会循环迭代 bitsets 从而找到满足所有过滤条件的匹配文档的集合。执行顺序是启发式的，但一般来说先迭代稀疏的 bitset （因为它可以排除掉大量的文档）。

4. *增量使用计数*
</code></pre>
<p>​	Elasticsearch 能够缓存非评分查询从而获取更快的访问，但是它也会不太聪明地缓存一些使用极少的东西。非评分计算因为倒排索引已经足够快了，所以我们只想缓存那些我们 <em>知道</em> 在将来会被再次使用的查询，以避免资源的浪费。</p>
<p>为了实现以上设想，Elasticsearch 会为每个索引跟踪保留查询使用的历史状态。如果查询在最近的 256 次查询中会被用到，那么它就会被缓存到内存中。当 bitset 被缓存后，缓存会在那些低于 10,000 个文档（或少于 3% 的总索引数）的段（segment）中被忽略。这些小的段即将会消失，所以为它们分配缓存是一种浪费。</p>
<h1 id="组合过滤器"><a class="header" href="#组合过滤器">组合过滤器</a></h1>
<h2 id="布尔过滤器"><a class="header" href="#布尔过滤器">布尔过滤器</a></h2>
<p>一个 <code>bool</code> 过滤器由三部分组成：</p>
<pre><code class="language-js">{
   &quot;bool&quot; : {
      &quot;must&quot; :     [],
      &quot;should&quot; :   [],
      &quot;must_not&quot; : [],
   }
}
</code></pre>
<p><strong><code>must</code></strong></p>
<p>所有的语句都 <em>必须（must）</em> 匹配，与 <code>AND</code> 等价。</p>
<p><strong><code>must_not</code></strong></p>
<p>所有的语句都 <em>不能（must not）</em> 匹配，与 <code>NOT</code> 等价。</p>
<p><strong><code>should</code></strong></p>
<p>至少有一个语句要匹配，与 <code>OR</code> 等价。</p>
<pre><code class="language-sql">SELECT product
FROM   products
WHERE  (price = 20 OR productID = &quot;XHDK-A-1293-#fJ3&quot;)
  AND  (price != 30)
</code></pre>
<p>等价于</p>
<pre><code class="language-sense">GET /my_store/products/_search
{
   &quot;query&quot; : {
      &quot;filtered&quot; : { 
         &quot;filter&quot; : {
            &quot;bool&quot; : {
              &quot;should&quot; : [
                 { &quot;term&quot; : {&quot;price&quot; : 20}}, 
                 { &quot;term&quot; : {&quot;productID&quot; : &quot;XHDK-A-1293-#fJ3&quot;}} 
              ],
              &quot;must_not&quot; : {
                 &quot;term&quot; : {&quot;price&quot; : 30} 
              }
           }
         }
      }
   }
}
</code></pre>
<h2 id="嵌套布尔过滤器"><a class="header" href="#嵌套布尔过滤器">嵌套布尔过滤器</a></h2>
<pre><code class="language-sql">SELECT document
FROM   products
WHERE  productID      = &quot;KDKE-B-9947-#kL5&quot;
  OR (     productID = &quot;JODL-X-1937-#pV7&quot;
       AND price     = 30 )
</code></pre>
<pre><code class="language-sense">GET /my_store/products/_search
{
   &quot;query&quot; : {
      &quot;filtered&quot; : {
         &quot;filter&quot; : {
            &quot;bool&quot; : {
              &quot;should&quot; : [
                { &quot;term&quot; : {&quot;productID&quot; : &quot;KDKE-B-9947-#kL5&quot;}}, 
                { &quot;bool&quot; : { 
                  &quot;must&quot; : [
                    { &quot;term&quot; : {&quot;productID&quot; : &quot;JODL-X-1937-#pV7&quot;}}, 
                    { &quot;term&quot; : {&quot;price&quot; : 30}} 
                  ]
                }}
              ]
           }
         }
      }
   }
}
</code></pre>
<h1 id="查找多个精确值"><a class="header" href="#查找多个精确值">查找多个精确值</a></h1>
<p><code>term</code> 查询对于查找单个值非常有用，但通常我们可能想搜索多个值。 如果我们想要查找价格字段值为 $20 或 $30 的文档该如何处理呢？</p>
<pre><code class="language-js">{
    &quot;terms&quot; : {
        &quot;price&quot; : [20, 30]
    }
}
</code></pre>
<pre><code class="language-sense">GET /my_store/products/_search
{
    &quot;query&quot; : {
        &quot;constant_score&quot; : {
            &quot;filter&quot; : {
                &quot;terms&quot; : { 
                    &quot;price&quot; : [20, 30]
                }
            }
        }
    }
}
</code></pre>
<h3 id="包含而不是相等"><a class="header" href="#包含而不是相等">包含，而不是相等</a></h3>
<p>一定要了解 <code>term</code> 和 <code>terms</code> 是 <em>包含（contains）</em> 操作，而非 <em>等值（equals）</em> （判断）。</p>
<h2 id="精确相等"><a class="header" href="#精确相等">精确相等</a></h2>
<p>如果一定期望得到我们前面说的那种行为（即整个字段完全相等），<strong>最好的方式是增加并索引另一个字段</strong>， 这个字段用以存储该字段包含词项的数量，同样以上面提到的两个文档为例，现在我们包括了一个维护标签数的新字段：</p>
<pre><code class="language-sense">{ &quot;tags&quot; : [&quot;search&quot;], &quot;tag_count&quot; : 1 }
{ &quot;tags&quot; : [&quot;search&quot;, &quot;open_source&quot;], &quot;tag_count&quot; : 2 }
</code></pre>
<p>一旦增加这个用来索引项 term 数目信息的字段，我们就可以构造一个 <code>constant_score</code> 查询，来确保结果中的文档所包含的词项数量与要求是一致的：</p>
<h1 id="范围"><a class="header" href="#范围">范围</a></h1>
<pre><code class="language-js">&quot;range&quot; : {
    &quot;price&quot; : {
        &quot;gte&quot; : 20,
        &quot;lte&quot; : 40
    }
}
</code></pre>
<p><code>range</code> 查询可同时提供包含（inclusive）和不包含（exclusive）这两种范围表达式，可供组合的选项如下：</p>
<ul>
<li><code>gt</code>: <code>&gt;</code> 大于（greater than）</li>
<li><code>lt</code>: <code>&lt;</code> 小于（less than）</li>
<li><code>gte</code>: <code>&gt;=</code> 大于或等于（greater than or equal to）</li>
<li><code>lte</code>: <code>&lt;=</code> 小于或等于（less than or equal to）</li>
</ul>
<h2 id="日期范围"><a class="header" href="#日期范围">日期范围</a></h2>
<pre><code class="language-js">&quot;range&quot; : {
    &quot;timestamp&quot; : {
        &quot;gt&quot; : &quot;2014-01-01 00:00:00&quot;,
        &quot;lt&quot; : &quot;2014-01-07 00:00:00&quot;
    }
}
</code></pre>
<h2 id="字符串范围"><a class="header" href="#字符串范围">字符串范围</a></h2>
<p>字符串范围可采用 <em>字典顺序（lexicographically）</em> 或字母顺序（alphabetically）。例如，下面这些字符串是采用字典序（lexicographically）排序的：</p>
<p>在倒排索引中的词项就是采取字典顺序（lexicographically）排列的，这也是字符串范围可以使用这个顺序来确定的原因。</p>
<pre><code class="language-js">&quot;range&quot; : {
    &quot;title&quot; : {
        &quot;gte&quot; : &quot;a&quot;,
        &quot;lt&quot; :  &quot;b&quot;
    }
}
</code></pre>
<h1 id="处理-null-值"><a class="header" href="#处理-null-值">处理 Null 值</a></h1>
<p>如何将某个不存在的字段存储在这个数据结构中呢？无法做到！简单的说，一个倒排索引只是一个 token 列表和与之相关的文档信息，如果字段不存在，那么它也不会持有任何 token，也就无法在倒排索引结构中表现。</p>
<p>最终，这也就意味着，<code>null</code>, <code>[]</code> （空数组）和 <code>[null]</code> 所有这些都是等价的，它们无法存于倒排索引中。</p>
<h2 id="存在查询"><a class="header" href="#存在查询">存在查询</a></h2>
<p>这个查询会返回那些在指定字段有任何值的文档</p>
<pre><code class="language-sense">GET /my_index/posts/_search
{
    &quot;query&quot; : {
        &quot;constant_score&quot; : {
            &quot;filter&quot; : {
                &quot;exists&quot; : { &quot;field&quot; : &quot;tags&quot; }
            }
        }
    }
}
</code></pre>
<h2 id="缺失查询"><a class="header" href="#缺失查询">缺失查询</a></h2>
<p>这个 <code>missing</code> 查询本质上与 <code>exists</code> 恰好相反：它返回某个特定 <em>无</em> 值字段的文档，与以下 SQL 表达的意思类似：</p>
<pre><code class="language-sql">SELECT tags
FROM   posts
WHERE  tags IS NULL
</code></pre>
<pre><code class="language-sense">GET /my_index/posts/_search
{
    &quot;query&quot; : {
        &quot;constant_score&quot; : {
            &quot;filter&quot;: {
                &quot;missing&quot; : { &quot;field&quot; : &quot;tags&quot; }
            }
        }
    }
}
</code></pre>
<h2 id="对象上的存在与缺失"><a class="header" href="#对象上的存在与缺失">对象上的存在与缺失</a></h2>
<pre><code class="language-js">{
   &quot;name&quot; : {
      &quot;first&quot; : &quot;John&quot;,
      &quot;last&quot; :  &quot;Smith&quot;
   }
}
</code></pre>
<p>我们不仅可以检查 <code>name.first</code> 和 <code>name.last</code> 的存在性，也可以检查 <code>name</code> ，不过在 <a href="https://www.elastic.co/guide/cn/elasticsearch/guide/current/mapping.html">映射</a> 中，如上对象的内部是个扁平的字段与值（field-value）的简单键值结构，类似下面这样：</p>
<pre><code class="language-js">{
   &quot;name.first&quot; : &quot;John&quot;,
   &quot;name.last&quot;  : &quot;Smith&quot;
}
</code></pre>
<p>那么我们如何用 <code>exists</code> 或 <code>missing</code> 查询 <code>name</code> 字段呢？ <code>name</code> 字段并不真实存在于倒排索引中。</p>
<p>原因是当我们执行下面这个过滤的时候：</p>
<pre><code class="language-js">{
    &quot;exists&quot; : { &quot;field&quot; : &quot;name&quot; }
}
</code></pre>
<p>实际执行的是：</p>
<pre><code class="language-js">{
    &quot;bool&quot;: {
        &quot;should&quot;: [
            { &quot;exists&quot;: { &quot;field&quot;: &quot;name.first&quot; }},
            { &quot;exists&quot;: { &quot;field&quot;: &quot;name.last&quot; }}
        ]
    }
}
</code></pre>
<p>这也就意味着，如果 <code>first</code> 和 <code>last</code> 都是空，那么 <code>name</code> 这个命名空间才会被认为不存在。</p>
<h1 id="关于缓存"><a class="header" href="#关于缓存">关于缓存</a></h1>
<p>我们已经简单介绍了过滤器是如何计算的。其核心实际是采用一个 bitset 记录与过滤器匹配的文档。Elasticsearch 积极地把这些 bitset 缓存起来以备随后使用。一旦缓存成功，bitset 可以复用 <em>任何</em> 已使用过的相同过滤器，而无需再次计算整个过滤器。</p>
<p>这些 bitsets 缓存是“智能”的：它们以增量方式更新。当我们索引新文档时，只需将那些新文档加入已有 bitset，而不是对整个缓存一遍又一遍的重复计算。和系统其他部分一样，过滤器是实时的，我们无需担心缓存过期问题。</p>
<h2 id="独立的过滤器缓存"><a class="header" href="#独立的过滤器缓存">独立的过滤器缓存</a></h2>
<p>属于一个查询组件的 bitsets 是独立于它所属搜索请求其他部分的</p>
<p>这就意味着，一旦被缓存，一个查询可以被用作多个搜索请求</p>
<p>bitsets 并不依赖于它所存在的查询上下文</p>
<p>这样使得缓存可以加速查询中经常使用的部分，从而降低较少、易变的部分所带来的消耗。</p>
<p>它查找满足以下任意一个条件的电子邮件：</p>
<ul>
<li>在收件箱中，且没有被读过的</li>
<li><em>不在</em> 收件箱中，但被标注重要的</li>
</ul>
<pre><code class="language-js">GET /inbox/emails/_search
{
  &quot;query&quot;: {
      &quot;constant_score&quot;: {
          &quot;filter&quot;: {
              &quot;bool&quot;: {
                 &quot;should&quot;: [
                    { &quot;bool&quot;: {
                          &quot;must&quot;: [
                             { &quot;term&quot;: { &quot;folder&quot;: &quot;inbox&quot; }}, 
                             { &quot;term&quot;: { &quot;read&quot;: false }}
                          ]
                    }},
                    { &quot;bool&quot;: {
                          &quot;must_not&quot;: {
                             &quot;term&quot;: { &quot;folder&quot;: &quot;inbox&quot; } 
                          },
                          &quot;must&quot;: {
                             &quot;term&quot;: { &quot;important&quot;: true }
                          }
                    }}
                 ]
              }
            }
        }
    }
}
</code></pre>
<p>两个过滤器是相同的，所以会使用同一 bitset 。</p>
<p>尽管其中一个收件箱的条件是 <code>must</code> 语句，另一个是 <code>must_not</code> 语句，但他们两者是完全相同的。这意味着在第一个语句执行后， bitset 就会被计算然后缓存起来供另一个使用。当再次执行这个查询时，收件箱的这个过滤器已经被缓存了，所以两个语句都会使用已缓存的 bitset 。</p>
<h2 id="自动缓存行为"><a class="header" href="#自动缓存行为">自动缓存行为</a></h2>
<p><strong>早起版本的缓存</strong></p>
<p>在 Elasticsearch 的较早版本中，默认的行为是缓存一切可以缓存的对象。这也通常意味着系统缓存 bitsets 太富侵略性，从而因为清理缓存带来性能压力</p>
<p><strong>小查询易过滤</strong></p>
<p>不仅如此，很多过滤器都很容易被评价，。缓存这些过滤器的意义不大，因为可以简单地再次执行过滤器。</p>
<p><strong>用户ID类数据不重复</strong></p>
<p>例如 <code>term</code> 过滤字段 <code>&quot;user_id&quot;</code> ：如果有上百万的用户，每个具体的用户 ID 出现的概率都很小，那么为这个过滤器缓存 bitsets 就不是很合算，因为缓存的结果很可能在重用之前就被剔除了。</p>
<p>这种缓存的扰动对性能有着严重的影响。更严重的是，它让开发者难以区分有良好表现的缓存以及无用缓存。</p>
<p>Elasticsearch 会基于使用频次自动缓存查询</p>
<p>查询频次</p>
<ul>
<li>如果一个非评分查询在最近的 256 次查询中被使用过（次数取决于查询类型），那么这个查询就会作为缓存的候选</li>
</ul>
<p>大段</p>
<ul>
<li>文档数量超过 10,000 （或超过总文档数量的 3% )的段才会缓存 bitset，因为小的片段可以很快的进行搜索和合并，这里缓存的意义不大。</li>
</ul>
<p>LRU驱逐</p>
<ul>
<li>least recently used 最近最少使用</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-shard-allocation-1"><a class="header" href="#index-shard-allocation-1">Index Shard Allocation</a></h2>
<p>该模块提供了  按索引设置来控制 分片分配给节点:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">Shard allocation filtering</a>: 控制哪些分片分配给哪些节点。</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/delayed-allocation.html">Delayed allocation</a>: 当节点离开之后。分片重分配工作会延迟</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-total-shards.html">Total shards per node</a>: 每个节点来自相同索引的分片数量的硬限制。</li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tier-shard-filtering.html">Data tier allocation</a>: Controls the allocation of indices to <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">data tiers</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-level-shard-allocation-filtering"><a class="header" href="#index-level-shard-allocation-filtering">Index-level shard allocation filtering</a></h2>
<ol>
<li>
<p>可以使用 shard allocation filters 控制 Elasticsearch 针对 指定索引  控制其分片的分配</p>
</li>
<li>
<p>These per-index filters are applied in conjunction with <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#cluster-shard-allocation-filtering">cluster-wide allocation filtering</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html#shard-allocation-awareness">allocation awareness</a>.</p>
</li>
<li>
<p>Shard allocation filters 可以基于自定义 节点属性 或者 内置的 <code>_name</code>, <code>_host_ip</code>, <code>_publish_ip</code>, <code>_ip</code>, <code>_host</code>, <code>_id</code>, <code>_tier</code> and <code>_tier_preference</code> attributes</p>
</li>
<li>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html">Index lifecycle management</a> uses filters based on custom node attributes to determine how to reallocate shards when moving between phases.</p>
</li>
<li>
<p>The <code>cluster.routing.allocation</code> settings are dynamic, enabling live indices to be moved from one set of nodes to another. </p>
</li>
<li>
<p>Shards are only relocated if it is possible to do so without breaking another routing constraint, such as never allocating a primary and replica shard on the same node.</p>
</li>
<li>
<p>例如，使用自定义节点属性 来 表明节点的性能表现。使用分片分配过滤器 给每个分片分配合适的 硬件</p>
</li>
</ol>
<h2 id="enabling-index-level-shard-allocation-filtering"><a class="header" href="#enabling-index-level-shard-allocation-filtering">Enabling index-level shard allocation filtering</a></h2>
<h3 id="指定节点属性"><a class="header" href="#指定节点属性">指定节点属性</a></h3>
<p><strong>yaml</strong></p>
<p>Specify the filter characteristics with a custom node attribute in each node’s <code>elasticsearch.yml</code> configuration file. For example, if you have <code>small</code>, <code>medium</code>, and <code>big</code> nodes, you could add a <code>size</code> attribute to filter based on node size.</p>
<p><strong>command</strong></p>
<pre><code class="language-sh">./bin/elasticsearch -Enode.attr.size=medium
</code></pre>
<h3 id="新建索引时-添加分片分配过滤器"><a class="header" href="#新建索引时-添加分片分配过滤器"><strong>新建索引时 添加分片分配过滤器</strong></a></h3>
<p>index.routing.allocation 配置 include<code>, </code>exclude<code>, and </code>require</p>
<pre><code class="language-console">PUT test/_settings
{
  &quot;index.routing.allocation.include.size&quot;: &quot;big,medium&quot;
}
</code></pre>
<h2 id="index-allocation-filter-settings"><a class="header" href="#index-allocation-filter-settings">Index allocation filter settings</a></h2>
<ul>
<li>
<p><strong><code>index.routing.allocation.include.{attribute}</code></strong></p>
<p>Assign the index to a node whose <code>{attribute}</code> has at least one of the comma-separated values.</p>
</li>
<li>
<p><strong><code>index.routing.allocation.require.{attribute}</code></strong></p>
<p>Assign the index to a node whose <code>{attribute}</code> has <em>all</em> of the comma-separated values.</p>
</li>
<li>
<p><strong><code>index.routing.allocation.exclude.{attribute}</code></strong></p>
<p>Assign the index to a node whose <code>{attribute}</code> has <em>none</em> of the comma-separated values.</p>
</li>
</ul>
<p><strong>内置属性</strong></p>
<table><thead><tr><th>key</th><th>desc</th></tr></thead><tbody>
<tr><td><code>_name</code></td><td>Match nodes by node name</td></tr>
<tr><td><code>_host_ip</code></td><td>Match nodes by host IP address (IP associated with hostname)</td></tr>
<tr><td><code>_publish_ip</code></td><td>Match nodes by publish IP address</td></tr>
<tr><td><code>_ip</code></td><td>Match either <code>_host_ip</code> or <code>_publish_ip</code></td></tr>
<tr><td><code>_host</code></td><td>Match nodes by hostname</td></tr>
<tr><td><code>_id</code></td><td>Match nodes by node id</td></tr>
<tr><td><code>_tier</code></td><td>Match nodes by the node’s <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">data tier</a> role. For more details see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tier-shard-filtering.html">data tier allocation filtering</a></td></tr>
</tbody></table>
<p><code>_tier</code> filtering is based on <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html">node</a> roles. Only a subset of roles are <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">data tier</a> roles, and the generic <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node">data role</a> will match any tier filtering.</p>
<pre><code class="language-console">PUT test/_settings
{
  &quot;index.routing.allocation.include._ip&quot;: &quot;192.168.2.*&quot;
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="delaying-allocation-when-a-node-leaves"><a class="header" href="#delaying-allocation-when-a-node-leaves">Delaying allocation when a node leaves</a></h2>
<p>当节点出于任何原因 (有意或其他原因) 离开群集时，主节点的反应是: </p>
<ol>
<li>
<p>将副本分片提升到主，以替换节点上的任何主分片</p>
</li>
<li>
<p>分配副本分片以替换丢失的副本 (假设有足够的节点)。</p>
</li>
<li>
<p>在剩余节点上均匀地重新平衡分片。</p>
</li>
</ol>
<p>这些操作旨在通过确保尽快完全复制每个分片来保护群集免受数据丢失。</p>
<p>尽管我们限制了 并发 recoveries at the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/recovery.html">node level</a> and at the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shards-allocation.html">cluster level</a>,</p>
<p>这种 “shard-shuffle” 仍然可以给集群带来很多额外的负载，如果丢失的节点可能很快返回，这可能是不必要的。想象一下这种情况:</p>
<ul>
<li>节点5离开集群</li>
<li>对于节点5上的每个主节点，主节点将副本分片提升为主副本。</li>
<li>主服务器将新副本分配给集群中的其他节点。</li>
<li>每个新副本都会在网络上制作主分片的完整副本。</li>
<li>更多的分片被移动到不同的节点，以重新平衡集群。</li>
<li>节点5在几分钟后返回。</li>
<li>主节点通过将分片分配给节点5来重新平衡集群。</li>
</ul>
<p><strong>延迟等待</strong></p>
<ol>
<li>
<p>如果主机只是等待了几分钟，那么丢失的分片可能会以最小的网络流量重新分配给节点5。</p>
</li>
<li>
<p>对于空闲分片(分片未接收索引请求)来说，这个过程会更快它们是自动的 sync-flushed](https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-synced-flush-api.html).</p>
</li>
<li>
<p>由于节点已离开而未分配的副本分片的 重分配可以使用 <code>index.unassigned.node_left.delayed_timeout</code>  动态设置 进行延迟，默认为1m。</p>
</li>
<li>
<p>可以在实时索引 (或所有索引) 上更新此设置:</p>
</li>
</ol>
<pre><code class="language-console">PUT _all/_settings
{
  &quot;settings&quot;: {
    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;5m&quot;
  }
}
</code></pre>
<p>启用延迟分配后，上述方案将更改为如下所示:</p>
<ol>
<li>节点5离开集群</li>
<li>对于节点5上的每个主节点，主节点将副本分片提升为主副本。</li>
<li>主节点 记录一条消息，说明未分配的分片的分配已延迟，以及延迟了多长时间。</li>
<li>群集保持黄色，因为存在未分配的副本分片。</li>
<li>节点5在超时到期前几分钟后返回。</li>
<li>丢失的副本被重新分配给节点5 (同步刷新的分片几乎立即恢复)。</li>
</ol>
<p><strong>注意</strong></p>
<ol>
<li>
<p>此设置不会影响副本提升至主分片，也不会有影响之前未分配的 副本的分配</p>
</li>
<li>
<p>特别是，延迟分配在完全集群重新启动后不会生效。</p>
</li>
<li>
<p>同样，在主故障转移情况下，会忘记经过的延迟时间 (即重置为完整的初始延迟)。</p>
</li>
</ol>
<h3 id="cancellation-of-shard-relocation"><a class="header" href="#cancellation-of-shard-relocation">Cancellation of shard relocation</a></h3>
<ol>
<li>如果延迟分配超时，则主节点将丢失的分片分配给将开始恢复的另一个节点。</li>
<li>如果丢失的节点重新加入集群，并且其分片仍具有与主节点相同的sync-id，则将取消分片重定位，并将同步的分片用于恢复。</li>
<li>因此，默认超时设置为仅一分钟: 即使开始分片重定位，取消副本恢复以支持同步的分片也很便宜。</li>
</ol>
<h3 id="monitoring-delayed-unassigned-shards"><a class="header" href="#monitoring-delayed-unassigned-shards">Monitoring delayed unassigned shards</a></h3>
<p>The number of shards whose allocation has been delayed by this timeout setting can be viewed with the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html">cluster health API</a>:</p>
<pre><code class="language-console">GET _cluster/health 
</code></pre>
<p>This request will return a <code>delayed_unassigned_shards</code> value.</p>
<h3 id="removing-a-node-permanently"><a class="header" href="#removing-a-node-permanently">Removing a node permanently</a></h3>
<p>If a node is not going to return and you would like Elasticsearch to allocate the missing shards immediately, just update the timeout to zero:</p>
<p>如果节点不返回，并且您希望Elasticsearch立即分配丢失的分片，则只需将超时更新为零:</p>
<pre><code class="language-console">PUT _all/_settings
{
  &quot;settings&quot;: {
    &quot;index.unassigned.node_left.delayed_timeout&quot;: &quot;0&quot;
  }
}
</code></pre>
<p>一旦丢失的分片开始恢复，您可以重置超时。</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-recovery-prioritization"><a class="header" href="#index-recovery-prioritization">Index recovery prioritization</a></h2>
<p>尽可能按优先级顺序恢复未分配的分片。</p>
<p>索引按优先级排序如下:</p>
<ul>
<li>the optional <code>index.priority</code> setting (higher before lower)</li>
<li>the index creation date (higher before lower)</li>
<li>the index name (higher before lower)</li>
</ul>
<p>This means that, by default, newer indices will be recovered before older indices.</p>
<p>Use the per-index dynamically updatable <code>index.priority</code> setting to customise the index prioritization order. For instance:</p>
<pre><code class="language-console">PUT index_1

PUT index_2

PUT index_3
{
  &quot;settings&quot;: {
    &quot;index.priority&quot;: 10
  }
}

PUT index_4
{
  &quot;settings&quot;: {
    &quot;index.priority&quot;: 5
  }
}
</code></pre>
<pre><code class="language-console">PUT index_1

PUT index_2

PUT index_3
{
  &quot;settings&quot;: {
    &quot;index.priority&quot;: 10
  }
}

PUT index_4
{
  &quot;settings&quot;: {
    &quot;index.priority&quot;: 5
  }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="total-shards-per-node"><a class="header" href="#total-shards-per-node">Total shards per node</a></h2>
<p>集群层面的索引分配 尽可能的 将 单个索引的 分片分配到 更多的节点</p>
<p>但是，根据您拥有多少分片 和索引以及它们的大小，可能并不总是能够均匀地分布分片</p>
<p>以下动态设置允许您从每个节点允许的单个索引中指定分片总数的硬限制:</p>
<p><strong><code>index.routing.allocation.total_shards_per_node</code></strong></p>
<p>The maximum number of shards (replicas and primaries) that will be allocated to a single node. Defaults to unbounded.</p>
<p>您还可以限制节点可以具有的分片数量，而与索引无关:</p>
<p><strong><code>cluster.routing.allocation.total_shards_per_node</code></strong></p>
<p>(<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/settings.html#dynamic-cluster-setting">Dynamic</a>) Maximum number of primary and replica shards allocated to each node. Defaults to <code>-1</code> (unlimited).</p>
<p>Elasticsearch checks this setting during shard allocation. For example, a cluster has a <code>cluster.routing.allocation.total_shards_per_node</code> setting of <code>100</code> and three nodes with the following shard allocations:</p>
<ul>
<li>Node A: 100 shards</li>
<li>Node B: 98 shards</li>
<li>Node C: 1 shard</li>
</ul>
<p>If node C fails, Elasticsearch reallocates its shard to node B. Reallocating the shard to node A would exceed node A’s shard limit.</p>
<pre><code>These settings impose a hard limit which can result in some shards not being allocated.

Use with caution.

</code></pre>
<h2 id="-1"><a class="header" href="#-1"></a></h2>
<div style="break-before: page; page-break-before: always;"></div><h2 id="index-level-data-tier-allocation-filtering"><a class="header" href="#index-level-data-tier-allocation-filtering">Index-level data tier allocation filtering</a></h2>
<p>You can use index-level allocation settings to control which <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-tiers.html">data tier</a> the index is allocated to. The data tier allocator is a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/shard-allocation-filtering.html">shard allocation filter</a> that uses two built-in node attributes: <code>_tier</code> and <code>_tier_preference</code>.</p>
<p>These tier attributes are set using the data node roles:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-content-node">data_content</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-hot-node">data_hot</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-warm-node">data_warm</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-cold-node">data_cold</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-frozen-node">data_frozen</a></li>
</ul>
<p>The <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node">data</a> role is not a valid data tier and cannot be used for data tier filtering. The frozen tier stores <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html#partially-mounted">partially mounted indices</a> exclusively.</p>
<p><strong><code>index.routing.allocation.include._tier_preference</code></strong></p>
<p>Assign the index to the first tier in the list that has an available node. This prevents indices from remaining unallocated if no nodes are available in the preferred tier. For example, if you set <code>index.routing.allocation.include._tier_preference</code> to <code>data_warm,data_hot</code>, the index is allocated to the warm tier if there are nodes with the <code>data_warm</code> role. If there are no nodes in the warm tier, but there are nodes with the <code>data_hot</code> role, the index is allocated to the hot tier.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="cluster-reroute-api"><a class="header" href="#cluster-reroute-api">Cluster reroute API</a></h2>
<p>更改集群中分片的分配。</p>
<h3 id="request-12"><a class="header" href="#request-12">Request</a></h3>
<pre><code>POST /_cluster/reroute
</code></pre>
<h3 id="description-10"><a class="header" href="#description-10">Description</a></h3>
<p>重新路由命令允许手动更改群集中单个分片的分配。</p>
<ol>
<li>
<p>例如，可以将分片从一个节点显式地移动到另一个节点</p>
</li>
<li>
<p>可以取消分配，</p>
</li>
<li>
<p>并且可以将未分配的分片显式地分配给特定节点。</p>
</li>
</ol>
<p>重要的是，在处理任何重新路由命令之后，Elasticsearch将正常执行重新平衡 (尊重诸如<em>cluster.routing.rebalance.enable</em>等设置的值)，以保持平衡状态。</p>
<p>例如，如果请求的分配包括将分片从node1移动到node2，则这可能导致分片从node2移回node1以使事情变得均匀。</p>
<p>The cluster can be set to disable allocations using the <code>cluster.routing.allocation.enable</code> setting. </p>
<p>If allocations are disabled then the only allocations that will be performed are explicit ones given using the <code>reroute</code> command, and consequent allocations due to rebalancing.</p>
<p><strong>dry_run</strong></p>
<p>It is possible to run <code>reroute</code> commands in &quot;dry run&quot; mode by using the <code>?dry_run</code> URI query parameter, or by passing <code>&quot;dry_run&quot;: true</code> in the request body. This will calculate the result of applying the commands to the current cluster state, and return the resulting cluster state after the commands (and re-balancing) has been applied, but will not actually perform the requested changes.</p>
<p><strong>explain</strong></p>
<p>If the <code>?explain</code> URI query parameter is included then a detailed explanation of why the commands could or could not be executed is included in the response.</p>
<p>集群会 尝试分配分片 ，并且重试 index.allocation.max_retries </p>
<p>This scenario can be caused by structural problems such as having an analyzer which refers to a stopwords file which doesn’t exist on all nodes.</p>
<p>一旦问题得到纠正，可以通过 reoute?retry_failed URI查询参数，进行一轮分片分配尝试</p>
<h3 id="query-parameters-11"><a class="header" href="#query-parameters-11">Query parameters</a></h3>
<p><strong><code>dry_run</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, then the request simulates the operation only and returns the resulting state.</p>
<p><strong><code>explain</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, then the response contains an explanation of why the commands can or cannot be executed.</p>
<p><strong><code>metric</code></strong></p>
<p>(Optional, string) Limits the information returned to the specified metrics. Defaults to all but metadata The following options are available:</p>
<pre><code>**`_all`**

Shows all metrics.

**`blocks`**

Shows the `blocks` part of the response.

**`master_node`**

Shows the elected `master_node` part of the response.

**`metadata`**

Shows the `metadata` part of the response. If you supply a comma separated list of indices, the returned output will only contain metadata for these indices.

**`nodes`**

Shows the `nodes` part of the response.

**`routing_table`**

Shows the `routing_table` part of the response.

**`version`**

Shows the cluster state version.
</code></pre>
<p><strong><code>retry_failed</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, then retries allocation of shards that are blocked due to too many subsequent allocation failures.</p>
<p><strong><code>master_timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<p><strong><code>timeout</code></strong></p>
<p>(Optional, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/common-options.html#time-units">time units</a>) Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error. Defaults to <code>30s</code>.</p>
<h3 id="request-body-6"><a class="header" href="#request-body-6">Request body</a></h3>
<p><strong><code>commands</code></strong></p>
<p>(Required, array of objects) Defines the commands to perform. Supported commands are:</p>
<p><strong>Properties of commands</strong></p>
<p><strong><code>move</code></strong></p>
<p>Move a started shard from one node to another node. Accepts <code>index</code> and <code>shard</code> for index name and shard number, <code>from_node</code> for the node to move the shard from, and <code>to_node</code> for the node to move the shard to.</p>
<p><strong><code>cancel</code></strong></p>
<p>Cancel allocation of a shard (or recovery). Accepts <code>index</code> and <code>shard</code> for index name and shard number, and <code>node</code> for the node to cancel the shard allocation on. This can be used to force resynchronization of existing replicas from the primary shard by cancelling them and allowing them to be reinitialized through the standard recovery process. By default only replica shard allocations can be cancelled. If it is necessary to cancel the allocation of a primary shard then the <code>allow_primary</code> flag must also be included in the request.</p>
<p><strong><code>allocate_replica</code></strong></p>
<p>Allocate an unassigned replica shard to a node. Accepts <code>index</code> and <code>shard</code> for index name and shard number, and <code>node</code> to allocate the shard to. Takes <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cluster.html">allocation deciders</a> into account.</p>
<p>Two more commands are available that allow the allocation of a primary shard to a node. These commands should however be used with extreme care, as primary shard allocation is usually fully automatically handled by Elasticsearch. Reasons why a primary shard cannot be automatically allocated include the following:</p>
<ul>
<li>A new index was created but there is no node which satisfies the allocation deciders.</li>
<li>An up-to-date shard copy of the data cannot be found on the current data nodes in the cluster. To prevent data loss, the system does not automatically promote a stale shard copy to primary.</li>
</ul>
<p>The following two commands are dangerous and may result in data loss. They are meant to be used in cases where the original data can not be recovered and the cluster administrator accepts the loss. If you have suffered a temporary issue that can be fixed, please see the <code>retry_failed</code> flag described above. To emphasise: if these commands are performed and then a node joins the cluster that holds a copy of the affected shard then the copy on the newly-joined node will be deleted or overwritten.</p>
<p><strong>allocate_stale_primary</strong>
Allocate a primary shard to a node that holds a stale copy. Accepts the index and shard for index name and shard number, and node to allocate the shard to. Using this command may lead to data loss for the provided shard id. If a node which has the good copy of the data rejoins the cluster later on, that data will be deleted or overwritten with the data of the stale copy that was forcefully allocated with this command. To ensure that these implications are well-understood, this command requires the flag accept_data_loss to be explicitly set to true.</p>
<p><strong>allocate_empty_primary</strong>
Allocate an empty primary shard to a node. Accepts the index and shard for index name and shard number, and node to allocate the shard to. Using this command leads to a complete loss of all data that was indexed into this shard, if it was previously started. If a node which has a copy of the data rejoins the cluster later on, that data will be deleted. To ensure that these implications are well-understood, this command requires the flag accept_data_loss to be explicitly set to true.</p>
<pre><code class="language-console">POST /_cluster/reroute
{
  &quot;commands&quot;: [
    {
      &quot;move&quot;: {
        &quot;index&quot;: &quot;test&quot;, &quot;shard&quot;: 0,
        &quot;from_node&quot;: &quot;node1&quot;, &quot;to_node&quot;: &quot;node2&quot;
      }
    },
    {
      &quot;allocate_replica&quot;: {
        &quot;index&quot;: &quot;test&quot;, &quot;shard&quot;: 1,
        &quot;node&quot;: &quot;node3&quot;
      }
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="cluster-allocation-explain-api"><a class="header" href="#cluster-allocation-explain-api">Cluster allocation explain API</a></h2>
<p>提供分片当前分配的解释。</p>
<pre><code class="language-console">GET _cluster/allocation/explain
{
  &quot;index&quot;: &quot;my-index-000001&quot;,
  &quot;shard&quot;: 0,
  &quot;primary&quot;: false,
  &quot;current_node&quot;: &quot;my-node&quot;
}
</code></pre>
<h3 id="description-11"><a class="header" href="#description-11">Description</a></h3>
<p>the cluster allocation explain API  的目的是为集群中的分片分配提供解释。</p>
<ol>
<li>
<p>对于未分配的分片，explain API提供了为什么未分配分片的解释。</p>
</li>
<li>
<p>对于分配的分片，explain API提供了一个解释，说明为什么分片保留在其当前节点上并且没有移动到或重新平衡到另一个节点。</p>
</li>
<li>
<p>当您尝试诊断分片未分配的原因或为什么分片继续保留在其当前节点上时，此API可能非常有用。</p>
</li>
</ol>
<h3 id="query-parameters-12"><a class="header" href="#query-parameters-12">Query parameters</a></h3>
<ul>
<li>
<p><strong><code>include_disk_info</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, returns information about disk usage and shard sizes. Defaults to <code>false</code>.</p>
</li>
<li>
<p><strong><code>include_yes_decisions</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, returns <em>YES</em> decisions in explanation. Defaults to <code>false</code>.</p>
</li>
</ul>
<h3 id="request-body-7"><a class="header" href="#request-body-7">Request body</a></h3>
<ul>
<li>
<p><strong><code>current_node</code></strong></p>
<p>(Optional, string) Specifies the node ID or the name of the node to only explain a shard that is currently located on the specified node. :  申明  位于指定节点的 分片</p>
</li>
<li>
<p><strong><code>index</code></strong></p>
<p>(Optional, string) Specifies the name of the index that you would like an explanation for. 申明  位于指定索引的 分片</p>
</li>
<li>
<p><strong><code>primary</code></strong></p>
<p>(Optional, Boolean) If <code>true</code>, returns explanation for the primary shard for the given shard ID.</p>
<p>​	对于给定分片ID 返回 该分片ID的 主分片</p>
</li>
<li>
<p><strong><code>shard</code></strong></p>
<p>(Optional, integer) Specifies the ID of the shard that you would like an explanation for.</p>
</li>
</ul>
<p>​			指定分片ID</p>
<h2 id="实例-3"><a class="header" href="#实例-3">实例</a></h2>
<p><strong>查看索引信息</strong></p>
<pre><code>GET _cat/indices/person2?v
</code></pre>
<pre><code>[
    {
        &quot;health&quot;: &quot;green&quot;,
        &quot;status&quot;: &quot;open&quot;,
        &quot;index&quot;: &quot;person2&quot;,
        &quot;uuid&quot;: &quot;_M8Ya5LrSYKKe8Uqh4Tgvw&quot;,
        &quot;pri&quot;: &quot;2&quot;,
        &quot;rep&quot;: &quot;1&quot;,
        &quot;docs.count&quot;: &quot;2&quot;,
        &quot;docs.deleted&quot;: &quot;0&quot;,
        &quot;store.size&quot;: &quot;15.6kb&quot;,
        &quot;pri.store.size&quot;: &quot;7.8kb&quot;
    }
]
</code></pre>
<p><strong>查看索引分片信息</strong></p>
<pre><code>GET _cat/shards/person2?v
</code></pre>
<pre><code class="language-json">[
    {
        &quot;index&quot;: &quot;person2&quot;,
        &quot;shard&quot;: &quot;1&quot;,
        &quot;prirep&quot;: &quot;r&quot;,
        &quot;state&quot;: &quot;STARTED&quot;,
        &quot;docs&quot;: &quot;1&quot;,
        &quot;store&quot;: &quot;3.9kb&quot;,
        &quot;ip&quot;: &quot;192.168.64.6&quot;,
        &quot;node&quot;: &quot;node-1&quot;
    },
    {
        &quot;index&quot;: &quot;person2&quot;,
        &quot;shard&quot;: &quot;1&quot;,
        &quot;prirep&quot;: &quot;p&quot;,
        &quot;state&quot;: &quot;STARTED&quot;,
        &quot;docs&quot;: &quot;1&quot;,
        &quot;store&quot;: &quot;3.9kb&quot;,
        &quot;ip&quot;: &quot;192.168.64.12&quot;,
        &quot;node&quot;: &quot;node-2&quot;
    },
    {
        &quot;index&quot;: &quot;person2&quot;,
        &quot;shard&quot;: &quot;0&quot;,
        &quot;prirep&quot;: &quot;p&quot;,
        &quot;state&quot;: &quot;STARTED&quot;,
        &quot;docs&quot;: &quot;1&quot;,
        &quot;store&quot;: &quot;3.9kb&quot;,
        &quot;ip&quot;: &quot;192.168.64.6&quot;,
        &quot;node&quot;: &quot;node-1&quot;
    },
    {
        &quot;index&quot;: &quot;person2&quot;,
        &quot;shard&quot;: &quot;0&quot;,
        &quot;prirep&quot;: &quot;r&quot;,
        &quot;state&quot;: &quot;STARTED&quot;,
        &quot;docs&quot;: &quot;1&quot;,
        &quot;store&quot;: &quot;3.9kb&quot;,
        &quot;ip&quot;: &quot;192.168.64.12&quot;,
        &quot;node&quot;: &quot;node-2&quot;
    }
]
</code></pre>
<p><strong>查看某一分片的分配详细信息</strong></p>
<pre><code>GET _cluster/allocation/explain
{
  &quot;index&quot;: &quot;my-index-000001&quot;,
  &quot;shard&quot;: 0,
  &quot;primary&quot;: false,
  &quot;current_node&quot;: &quot;my-node&quot;
}
</code></pre>
<pre><code class="language-console-result">{
  &quot;index&quot; : &quot;my-index-000001&quot;,
  &quot;shard&quot; : 0,
  &quot;primary&quot; : true,
  &quot;current_state&quot; : &quot;unassigned&quot;,//分片分配状态    
  &quot;unassigned_info&quot; : { //分片未分配的 原始原因
    &quot;reason&quot; : &quot;INDEX_CREATED&quot;,                   
    &quot;at&quot; : &quot;2017-01-04T18:08:16.600Z&quot;,
    &quot;last_allocation_status&quot; : &quot;no&quot;
  },
  &quot;can_allocate&quot; : &quot;no&quot;, //是否可分配       
  &quot;allocate_explanation&quot; : &quot;cannot allocate because allocation is not permitted to any of the nodes&quot;, //分配解释
  &quot;node_allocation_decisions&quot; : [ //决定分片分配到节点的解释
    {
      &quot;node_id&quot; : &quot;8qt2rY-pT6KNZB3-hGfLnw&quot;,
      &quot;node_name&quot; : &quot;node-0&quot;,
      &quot;transport_address&quot; : &quot;127.0.0.1:9401&quot;,
      &quot;node_attributes&quot; : {},
      &quot;node_decision&quot; : &quot;no&quot;,                     
      &quot;weight_ranking&quot; : 1,
      &quot;deciders&quot; : [
        {
          &quot;decider&quot; : &quot;filter&quot;,    //导致分片没有分配到节点的 员原因               
          &quot;decision&quot; : &quot;NO&quot;,
          &quot;explanation&quot; : &quot;node does not match index setting [index.routing.allocation.include] filters [_name:\&quot;nonexistent_node\&quot;]&quot;  
        }
      ]
    }
  ]
}
</code></pre>
<h4 id="unassigned-primary-shard"><a class="header" href="#unassigned-primary-shard">unassigned primary shard</a></h4>
<p>先前已经分配的分片，但是现在未分配的主分片的解释。</p>
<pre><code class="language-js">{
  &quot;index&quot; : &quot;my-index-000001&quot;,
  &quot;shard&quot; : 0,
  &quot;primary&quot; : true,
  &quot;current_state&quot; : &quot;unassigned&quot;,
  &quot;unassigned_info&quot; : {
    &quot;reason&quot; : &quot;NODE_LEFT&quot;,
    &quot;at&quot; : &quot;2017-01-04T18:03:28.464Z&quot;,
    &quot;details&quot; : &quot;node_left[OIWe8UhhThCK0V5XfmdrmQ]&quot;,
    &quot;last_allocation_status&quot; : &quot;no_valid_shard_copy&quot;
  },
  &quot;can_allocate&quot; : &quot;no_valid_shard_copy&quot;,
  &quot;allocate_explanation&quot; : &quot;cannot allocate because a previous copy of the primary shard existed but can no longer be found on the nodes in the cluster&quot;
}
</code></pre>
<h4 id="unassigned-replica-shard"><a class="header" href="#unassigned-replica-shard">Unassigned replica <strong>shard</strong></a></h4>
<p>由于延迟分配导致的副本分片未分配 https://www.elastic.co/guide/en/elasticsearch/reference/7.13/delayed-allocation.html</p>
<pre><code class="language-js">{
  &quot;index&quot; : &quot;my-index-000001&quot;,
  &quot;shard&quot; : 0,
  &quot;primary&quot; : false,
    //未分配
  &quot;current_state&quot; : &quot;unassigned&quot;,
  &quot;unassigned_info&quot; : {
    &quot;reason&quot; : &quot;NODE_LEFT&quot;,
    &quot;at&quot; : &quot;2017-01-04T18:53:59.498Z&quot;,
    &quot;details&quot; : &quot;node_left[G92ZwuuaRY-9n8_tc-IzEg]&quot;,
    &quot;last_allocation_status&quot; : &quot;no_attempt&quot;
  },
    //延迟分配
  &quot;can_allocate&quot; : &quot;allocation_delayed&quot;,
    //等待过期节点加入集群
  &quot;allocate_explanation&quot; : &quot;cannot allocate because the cluster is still waiting 59.8s for the departed node holding a replica to rejoin, despite being allowed to allocate the shard to at least one other node&quot;,
    //默认1minutes
  &quot;configured_delay&quot; : &quot;1m&quot;,                      
  &quot;configured_delay_in_millis&quot; : 60000,
  &quot;remaining_delay&quot; : &quot;59.8s&quot;,                    
  &quot;remaining_delay_in_millis&quot; : 59824,
  &quot;node_allocation_decisions&quot; : [
    {
      &quot;node_id&quot; : &quot;pmnHu_ooQWCPEFobZGbpWw&quot;,
      &quot;node_name&quot; : &quot;node_t2&quot;,
      &quot;transport_address&quot; : &quot;127.0.0.1:9402&quot;,
      &quot;node_decision&quot; : &quot;yes&quot;
    },
    {
      &quot;node_id&quot; : &quot;3sULLVJrRneSg0EfBB-2Ew&quot;,
      &quot;node_name&quot; : &quot;node_t0&quot;,
      &quot;transport_address&quot; : &quot;127.0.0.1:9400&quot;,
      &quot;node_decision&quot; : &quot;no&quot;,
      &quot;store&quot; : {                                 
        &quot;matching_size&quot; : &quot;4.2kb&quot;,
        &quot;matching_size_in_bytes&quot; : 4325
      },
      &quot;deciders&quot; : [
        {
          &quot;decider&quot; : &quot;same_shard&quot;,
          &quot;decision&quot; : &quot;NO&quot;,
          &quot;explanation&quot; : &quot;a copy of this shard is already allocated to this node [[my-index-000001][0], node[3sULLVJrRneSg0EfBB-2Ew], [P], s[STARTED], a[id=eV9P8BN1QPqRc3B4PLx6cg]]&quot;
        }
      ]
    }
  ]
}
</code></pre>
<h4 id="assigned-shard"><a class="header" href="#assigned-shard">Assigned shard</a></h4>
<p>以下响应展示了：这个分片不能在待在 当前所处于的节点了，必须重新分片</p>
<pre><code class="language-js">{
  &quot;index&quot; : &quot;my-index-000001&quot;,
  &quot;shard&quot; : 0,
  &quot;primary&quot; : true,
  &quot;current_state&quot; : &quot;started&quot;,
  &quot;current_node&quot; : {
    &quot;id&quot; : &quot;8lWJeJ7tSoui0bxrwuNhTA&quot;,
    &quot;name&quot; : &quot;node_t1&quot;,
    &quot;transport_address&quot; : &quot;127.0.0.1:9401&quot;
  },
    //是否能待在当前节点
  &quot;can_remain_on_current_node&quot; : &quot;no&quot;, 
    //作出决定的决定器
  &quot;can_remain_decisions&quot; : [                      
    {
      &quot;decider&quot; : &quot;filter&quot;,
      &quot;decision&quot; : &quot;NO&quot;,
      &quot;explanation&quot; : &quot;node does not match index setting [index.routing.allocation.include] filters [_name:\&quot;nonexistent_node\&quot;]&quot;
    }
  ],
    //是否能移动到其他节点
  &quot;can_move_to_other_node&quot; : &quot;no&quot;,                
    // 不能移动的解释
  &quot;move_explanation&quot; : &quot;cannot move shard to another node, even though it is not allowed to remain on its current node&quot;,
  &quot;node_allocation_decisions&quot; : [
    {
      &quot;node_id&quot; : &quot;_P8olZS8Twax9u6ioN-GGA&quot;,
      &quot;node_name&quot; : &quot;node_t0&quot;,
      &quot;transport_address&quot; : &quot;127.0.0.1:9400&quot;,
      &quot;node_decision&quot; : &quot;no&quot;,
      &quot;weight_ranking&quot; : 1,
      &quot;deciders&quot; : [
        {
          &quot;decider&quot; : &quot;filter&quot;,
          &quot;decision&quot; : &quot;NO&quot;,
          &quot;explanation&quot; : &quot;node does not match index setting [index.routing.allocation.include] filters [_name:\&quot;nonexistent_node\&quot;]&quot;
        }
      ]
    }
  ]
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="官方文档"><a class="header" href="#官方文档">官方文档</a></h1>
<blockquote>
<p>https://www.elastic.co/cn/training/elastic-certified-engineer-exam</p>
</blockquote>
<h2 id="数据管理"><a class="header" href="#数据管理"><strong>数据管理</strong></a></h2>
<ul>
<li>
<p>按照要求定义索引</p>
<ul>
<li>定义索引</li>
</ul>
</li>
<li>
<p>使用数据可视化工具将文本文件上传到Elasticsearch</p>
</li>
<li>
<p>为满足一组给定要求的给定模式定义和使用索引模板</p>
<ul>
<li>索引模板定义</li>
</ul>
</li>
<li>
<p>在 满足给定要求 的情况下 定义和使用的动态模板</p>
<ul>
<li>动态索引模板定义</li>
</ul>
</li>
<li>
<p>为时间序列索引定义索引生命周期管理策略</p>
<ul>
<li>ILM 索引生命周期管理</li>
</ul>
</li>
<li>
<p>定义创建新数据流的索引模板</p>
<ul>
<li>dataStream</li>
</ul>
</li>
</ul>
<h2 id="searching-data"><a class="header" href="#searching-data"><strong>Searching Data</strong></a></h2>
<ul>
<li>在索引的多个字段中使用 术语或者短语匹配</li>
<li>使用bool 联合过滤</li>
<li>异步查询</li>
<li>使用指标或桶查询</li>
<li>使用桶子查询</li>
<li>跨多个集群搜索</li>
</ul>
<h2 id="developing-search-applications"><a class="header" href="#developing-search-applications"><strong>Developing Search Applications</strong></a></h2>
<ul>
<li>突出显示查询响应中的搜索词</li>
<li>按指定要求排序查询结果</li>
<li>实现分页</li>
<li>定义和使用 alias</li>
<li>定义和使用 搜索模板</li>
</ul>
<h2 id="data-processing"><a class="header" href="#data-processing"><strong>Data Processing</strong></a></h2>
<ul>
<li>按照给定要求 定义 mapping</li>
<li>按照给定要求定义 自定义分析器</li>
<li>定义和使用具有不同数据类型和/或分析器的多字段</li>
<li>使用 reIndex 或者 updateByQuery 重新索引文档</li>
<li>按照给定要求 定义和使用 ingest pipeline  包括 使用 Painless 脚本修改文档</li>
<li>配置索引，使其正确维护对象的嵌套数组的关系</li>
</ul>
<h2 id="cluster-management"><a class="header" href="#cluster-management"><strong>Cluster Management</strong></a></h2>
<ul>
<li>诊断分片问题并修复集群的运行状况</li>
<li>备份和还原集群和/或特定索引</li>
<li>将快照配置为可搜索的</li>
<li>为跨集群搜索配置集群</li>
<li>实现跨集群复制</li>
<li>使用Elasticsearch Security定义基于角色的访问控制</li>
</ul>
<h2 id="模块集合"><a class="header" href="#模块集合">模块集合</a></h2>
<p>部署、索引、检索、聚合、分析、文档、集群、安全</p>
<h1 id="考试路线"><a class="header" href="#考试路线">考试路线</a></h1>
<ol>
<li>梳理考点</li>
<li>学习并整理考点相关知识点</li>
<li>学习并整理 知识星球 相关考点 解答</li>
<li>刷题：主要是知识星球 真题演练</li>
<li>总结经验：总结过来人经验</li>
<li>报名事宜</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h2 id="cluster-management-1"><a class="header" href="#cluster-management-1"><strong>Cluster Management</strong></a></h2>
<h3 id="普通方式与集群方式安装和基本配置"><a class="header" href="#普通方式与集群方式安装和基本配置">普通方式与集群方式安装和基本配置</a></h3>
<h3 id="诊断分片问题并修复集群的运行状况"><a class="header" href="#诊断分片问题并修复集群的运行状况">诊断分片问题并修复集群的运行状况</a></h3>
<h4 id="文档-1"><a class="header" href="#文档-1">文档</a></h4>
<ul>
<li>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html</li>
<li>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-allocation-explain.html</li>
<li>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-reroute.html</li>
</ul>
<h4 id="考点梳理"><a class="header" href="#考点梳理">考点梳理</a></h4>
<ul>
<li>cat api使用（很多，都要熟悉）</li>
<li>诊断集群健康状态，找到黄色或红色非健康能找到原因，并变成健康绿色状态</li>
<li>诊断集群分配未分配的原因，并恢复正常</li>
<li>集群分配迁移等重新路由实现</li>
</ul>
<h3 id="备份和还原集群和或特定索引"><a class="header" href="#备份和还原集群和或特定索引">备份和还原集群和/或特定索引</a></h3>
<h4 id="文档-2"><a class="header" href="#文档-2">文档</a></h4>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster.html</p>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-restore.html</p>
<h4 id="考点梳理-1"><a class="header" href="#考点梳理-1">考点梳理</a></h4>
<ul>
<li>快照备份集群并恢复</li>
<li>快照备份指定索引并恢复</li>
<li>一定要验证一下恢复是否正确，是否满足给定题目的条件</li>
</ul>
<h3 id="将快照配置为可搜索的"><a class="header" href="#将快照配置为可搜索的">将快照配置为可搜索的</a></h3>
<h4 id="文档-3"><a class="header" href="#文档-3">文档</a></h4>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html</p>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ilm-searchable-snapshot.html</p>
<h4 id="考点梳理-2"><a class="header" href="#考点梳理-2">考点梳理</a></h4>
<ul>
<li>配置可搜索快照</li>
<li>执行快照检索</li>
</ul>
<h3 id="为跨集群搜索配置集群"><a class="header" href="#为跨集群搜索配置集群">为跨集群搜索配置集群</a></h3>
<h4 id="文档-4"><a class="header" href="#文档-4">文档</a></h4>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cross-cluster-search.html</p>
<h4 id="考点梳理-3"><a class="header" href="#考点梳理-3">考点梳理</a></h4>
<ul>
<li>能实现跨集群检索配置</li>
<li>能实现跨集群检索</li>
<li>考试的时候，一定要验证返回结果是不同集群返回的才可以</li>
</ul>
<h3 id="实现跨集群复制"><a class="header" href="#实现跨集群复制">实现跨集群复制</a></h3>
<h4 id="文档-5"><a class="header" href="#文档-5">文档</a></h4>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/current/xpack-ccr.html</p>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ccr-apis.html</p>
<h4 id="考点梳理-4"><a class="header" href="#考点梳理-4">考点梳理</a></h4>
<ul>
<li>跨集群复制</li>
</ul>
<h3 id="使用elasticsearch-security定义基于角色的访问控制"><a class="header" href="#使用elasticsearch-security定义基于角色的访问控制">使用Elasticsearch Security定义基于角色的访问控制</a></h3>
<h4 id="文档-6"><a class="header" href="#文档-6">文档</a></h4>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-api-put-role.html</p>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-api-put-user.html</p>
<h4 id="考点梳理-5"><a class="header" href="#考点梳理-5">考点梳理</a></h4>
<ul>
<li>x-pack 一般会结合role 角色一起考</li>
<li>新建角色</li>
<li>新建用户&amp;密码，修改密码</li>
<li>官方我咨询过：命令行或者kibana操作都可以，但要确保结果对。建议kibana，毕竟比较简洁。</li>
<li>kibana权限设置，一定要加上能访问kibana，否则新建了用户会无法登录（可能会扣分）</li>
<li>举例：设置x-pack属性后（默认未开启），设置用户名、密码（可以kibana设置）、设置访问权限等。</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="data-management-数据管理"><a class="header" href="#data-management-数据管理">Data Management 数据管理</a></h2>
<h3 id="11-define-an-index-that-satisfies-a-given-set-of-requirements"><a class="header" href="#11-define-an-index-that-satisfies-a-given-set-of-requirements">1.1 Define an index that satisfies a given set of requirements</a></h3>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-create-index.html</p>
<p>考点梳理：</p>
<ul>
<li>创建满足给定条件的索引</li>
<li>主分片数、副本分片数修改主分片、副本分片数
setting设置（参数建议都过一下，如：刷新频率等）</li>
</ul>
<h3 id="12-新增考点-use-the-data-visualizer-to-upload-a-text-file-into-elasticsearch"><a class="header" href="#12-新增考点-use-the-data-visualizer-to-upload-a-text-file-into-elasticsearch">1.2 【新增考点】 Use the Data Visualizer to upload a text file into Elasticsearch</a></h3>
<p>考点梳理：</p>
<ul>
<li>偏 Kibana 实操的考点
https://www.elastic.co/guide/en/kibana/7.13/connect-to-elasticsearch.html</li>
</ul>
<h3 id="13-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements"><a class="header" href="#13-define-and-use-an-index-template-for-a-given-pattern-that-satisfies-a-given-set-of-requirements">1.3 Define and use an index template for a given pattern that satisfies a given set of requirements</a></h3>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-templates.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.13/indices-templates-v1.html</p>
<p>考点梳理：</p>
<ul>
<li>
<p>创建满足给定条件的索引模板</p>
</li>
<li>
<p>组合考点</p>
</li>
<li>
<ul>
<li>创建模板同时：指定mapping，指定setting，指定ingest，指定analyzer，指定别名，指定order优先级</li>
</ul>
</li>
</ul>
<h3 id="14-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements"><a class="header" href="#14-define-and-use-a-dynamic-template-that-satisfies-a-given-set-of-requirements">1.4 Define and use a dynamic template that satisfies a given set of requirements</a></h3>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/dynamic-templates.html</p>
<p>考点梳理：</p>
<ul>
<li>创建满足给定模板条件的索引，如：text_*开头指定为text类型</li>
<li>创建满足给定模板条件的模板，可以结合2.4 一起考！</li>
</ul>
<h3 id="15-新增考点define-an-index-lifecycle-management-policy-for-a-time-series-index"><a class="header" href="#15-新增考点define-an-index-lifecycle-management-policy-for-a-time-series-index">1.5 【新增考点】Define an Index Lifecycle Management policy for a time-series index</a></h3>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-lifecycle-management.html</p>
<p>考点：</p>
<ul>
<li>为给定时序数据添加索引生命周期管理</li>
<li>两种实现方式：Kibana + DSL命令行</li>
</ul>
<h3 id="16-新增考点define-an-index-template-that-creates-a-new-data-stream"><a class="header" href="#16-新增考点define-an-index-template-that-creates-a-new-data-stream">1.6 【新增考点】Define an index template that creates a new data stream</a></h3>
<p>https://www.elastic.co/guide/en/elasticsearch/reference/7.13/data-streams.html
考点：</p>
<ul>
<li>为 data stream数据流类型添加索引模板处理。</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="rare-terms-aggregation"><a class="header" href="#rare-terms-aggregation">Rare terms aggregation</a></h2>
<p>基于多桶值源的聚合，可找到 “稀有” 术语-分布的长尾且不常见的术语。从概念上讲，这就像一个按 _count升序排序的术语聚合。如术语聚合文档中所述，实际上按计数升序排序术语agg具有无界错误。相反，您应该使用rare_terms聚合</p>
<p>A multi-bucket value source based aggregation which finds &quot;rare&quot; terms —词项处于分布的长尾且不频繁出现的</p>
<p>从概念上来讲，这就是一个 <em>terms</em> 聚合 ，通过 <em>_count</em>  升序排序</p>
<p>实际上，通过 <em>count</em> 升序对术语agg进行排序具有无界错误(<em>unbounded error</em>)。相反，您应该使用<em>rare_terms</em>聚合</p>
<h2 id="syntax"><a class="header" href="#syntax">Syntax</a></h2>
<pre><code class="language-js">{
  &quot;rare_terms&quot;: {
    &quot;field&quot;: &quot;the_field&quot;,
    &quot;max_doc_count&quot;: 1
  }
}
</code></pre>
<h2 id="parameters"><a class="header" href="#parameters"><strong>Parameters</strong></a></h2>
<table><thead><tr><th>Parameter Name</th><th>Description</th><th>Required</th><th>Default Value</th></tr></thead><tbody>
<tr><td><code>field</code></td><td>检索的字段</td><td>Required</td><td></td></tr>
<tr><td><code>max_doc_count</code></td><td><em>term</em> 出现在的  最大文档个数</td><td>Optional</td><td><code>1</code></td></tr>
<tr><td><code>precision</code></td><td>The precision of the internal CuckooFilters. Smaller precision leads to better approximation, but higher memory usage. Cannot be smaller than <code>0.00001</code></td><td>Optional</td><td><code>0.01</code></td></tr>
<tr><td><code>include</code></td><td>Terms that should be included in the aggregation</td><td>Optional</td><td></td></tr>
<tr><td><code>exclude</code></td><td>Terms that should be excluded from the aggregation</td><td>Optional</td><td></td></tr>
<tr><td><code>missing</code></td><td>The value that should be used if a document does not have the field being aggregated</td><td>Optional</td><td></td></tr>
</tbody></table>
<h2 id="example-3"><a class="header" href="#example-3">Example</a></h2>
<pre><code class="language-console">GET /_search
{
  &quot;aggs&quot;: {
    &quot;genres&quot;: {
      &quot;rare_terms&quot;: {
        &quot;field&quot;: &quot;genre&quot;
      }
    }
  }
}
</code></pre>
<pre><code class="language-console-result">{
  ...
  &quot;aggregations&quot;: {
    &quot;genres&quot;: {
      &quot;buckets&quot;: [
        {
          &quot;key&quot;: &quot;swing&quot;,
          &quot;doc_count&quot;: 1
        }
      ]
    }
  }
}
</code></pre>
<p>在此示例中，我们看到的唯一存储桶是 <em>swing</em> 存储桶，因为它是一个文档中出现的唯一术语。如果我们将max_doc_count增加到2，我们将看到更多的存储桶:</p>
<pre><code class="language-console-result">{
  ...
  &quot;aggregations&quot;: {
    &quot;genres&quot;: {
      &quot;buckets&quot;: [
        {
          &quot;key&quot;: &quot;swing&quot;,
          &quot;doc_count&quot;: 1
        },
        {
          &quot;key&quot;: &quot;jazz&quot;,
          &quot;doc_count&quot;: 2
        }
      ]
    }
  }
}
</code></pre>
<h2 id="maximum-document-count"><a class="header" href="#maximum-document-count">Maximum document count</a></h2>
<p><em>max_doc_count</em>参数用于控制 <em>term</em> 可以具有的文档计数的上限</p>
<p><em>rare_terms agg</em>没有像<em>terms agg</em>那样的大小限制。</p>
<p>这意味着将返回与<em>max_doc_count</em>标准匹配的术语</p>
<p>但是，这确实意味着如果选择不正确，可以返回大量结果。为了限制此设置的危险，maximum <em>max_doc_count</em> 最大为100</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="简介-1"><a class="header" href="#简介-1">简介</a></h2>
<p>从例子中可以了解到，Terms Aggregation 以指定域的唯一值（term）构建出多个桶，每个桶包含了 key 为域的值（term），doc_count 为含有这个值（term）的文档数。</p>
<pre><code>GET /_search
{
    &quot;aggs&quot; : {
        &quot;genres&quot; : {
            &quot;terms&quot; : { &quot;field&quot; : &quot;genre&quot; }
        }
    }
}
</code></pre>
<pre><code>{
    ...
    &quot;aggregations&quot; : {
        &quot;genres&quot; : {
            &quot;doc_count_error_upper_bound&quot;: 0, 
            &quot;sum_other_doc_count&quot;: 0, 
            &quot;buckets&quot; : [ 
                {
                    &quot;key&quot; : &quot;electronic&quot;,
                    &quot;doc_count&quot; : 6
                },
                {
                    &quot;key&quot; : &quot;rock&quot;,
                    &quot;doc_count&quot; : 3
                },
                {
                    &quot;key&quot; : &quot;jazz&quot;,
                    &quot;doc_count&quot; : 2
                }
            ]
        }
    }
}
</code></pre>
<p>默认情况下，响应将返回 10 个桶。当然也可以指定 size 参数改变返回的桶的数量。</p>
<h2 id="size-与精度"><a class="header" href="#size-与精度">Size 与精度</a></h2>
<p>Terms Aggregation 返回结果中的 doc_count- 是近似的，并不是一个准确数。</p>
<p>根据 Elasticsearch 的机制，每个分片都会根据各自拥有的数据进行计算并且进行排序，最后协调节点对各个分片的计算结果进行整理并返回给客户端。</p>
<p>而就是因为这样，产生出精度的问题，也就是计算的结果有误差。</p>
<p>下面为官方给出的例子：有一组数据，数据中包含一个 product 字段，记录了产品的数量。请求获取产品数量 TOP 5 的数据。而数据存在有 3 个分片的索引中。</p>
<pre><code>GET /_search
{
    &quot;aggs&quot; : {
        &quot;products&quot; : {
            &quot;terms&quot; : {
                &quot;field&quot; : &quot;product&quot;,
                &quot;size&quot; : 5
            }
        }
    }
}
</code></pre>
<p>会有如下动作发生：</p>
<p>各个分片计算得出的结果。</p>
<table><thead><tr><th></th><th>ShardA</th><th>ShardB</th><th>ShardC</th></tr></thead><tbody>
<tr><td>1</td><td>productA 25</td><td>productA 30</td><td>productA 45</td></tr>
<tr><td>2</td><td>productB 18</td><td>productB 25</td><td>productC 44</td></tr>
<tr><td>3</td><td>productC 6</td><td>productF 17</td><td>productZ 36</td></tr>
<tr><td>4</td><td>productD 3</td><td>productZ 16</td><td>productG 30</td></tr>
<tr><td>5</td><td>productE 2</td><td>productG 15</td><td>productE 29</td></tr>
<tr><td>6</td><td>productF 2</td><td>productH 14</td><td>productH 28</td></tr>
<tr><td>7</td><td>productG 2</td><td>productI 10</td><td>productQ 2</td></tr>
<tr><td>8</td><td>productH 2</td><td>productQ 8</td><td>productD 1</td></tr>
<tr><td>9</td><td>productI 1</td><td>productJ 6</td><td></td></tr>
<tr><td>10</td><td>productJ 1</td><td>productC 4</td><td></td></tr>
</tbody></table>
<p>然后分片将会把 TOP 5 的数据返回给协调节点。</p>
<table><thead><tr><th></th><th>ShardA</th><th>ShardB</th><th>ShardC</th></tr></thead><tbody>
<tr><td>1</td><td>productA 25</td><td>productA 30</td><td>productA 45</td></tr>
<tr><td>2</td><td>productB 18</td><td>productB 25</td><td>productC 44</td></tr>
<tr><td>3</td><td>productC 6</td><td>productF 17</td><td>productZ 36</td></tr>
<tr><td>4</td><td>productD 3</td><td>productZ 16</td><td>productG 30</td></tr>
<tr><td>5</td><td>productE 2</td><td>productG 15</td><td>productE 29</td></tr>
</tbody></table>
<p>最后，协调节点将会根据各个节点给出的数据进行整理得出最后 TOP 5 的数据并返回给客户端。</p>
<table><thead><tr><th></th><th>最终数据</th></tr></thead><tbody>
<tr><td>1</td><td>productA(100)</td></tr>
<tr><td>2</td><td>productZ(52)</td></tr>
<tr><td>3</td><td>productC(50)</td></tr>
<tr><td>4</td><td>productG(45)</td></tr>
<tr><td>5</td><td>productB(43)</td></tr>
</tbody></table>
<p>可以看出，在第二步中，由于各个分片的数据有所不同，数据<em>ProductC</em>在分片 A 能排得上 TOP 5 的 term 在分片 B 却排不上，所以统计的 count个数可能不准确</p>
<p>但是，只要各个分片都返回足够多的数据给协调节点，客户端得到的结果将是精准的。</p>
<p>而开始提到的参数 size 就会控制分片返回给节点的数据量以及返回给客户端的数据量。可见，参数 size 越大，获取的结果的精度越高。</p>
<h2 id="shard-size"><a class="header" href="#shard-size">Shard Size</a></h2>
<p>上面提到，size 的大小会影响到聚合结果的精准度，size 值越大，精度越高。为了更高得精度，请求的时候将 size 值设置得偏大，这时会有一个问题，就是客户端将会得到大量的响应数据，而且这些响应数据对于客户端来说大部分都是没用的，而大量的响应数据还会耗费网络资源。</p>
<p>这时，就要使用到另一个参数 shard_size 。shard_size <strong>只会控制分片返回给协调节点的数据量</strong>，而<strong>最后协调节点整理并返回的数据量由 size 控制</strong>，这样既能提升精度，也避免了上述由于要提升精度而导致协调节点返回大量响应数据给客户端的问题。</p>
<p>上面的内容由提到，size 会控制分片返回给协调节点的数据量，这段描述即正确也不正确。默认情况下，shard_size 的大小为 (size * 1.5 + 10) ，确实由 size 值控制，但是如果在请求时显式提供 shard_size 参数，自然 size 与分片返回给节点的数据量无关。</p>
<h2 id="没显示的文档数"><a class="header" href="#没显示的文档数">没显示的文档数</a></h2>
<p>在响应结果中，有一个 <em>sum_other_doc_count</em> 值。假如 size 设定为 5，那么响应中只有 doc_count 前 5 的桶的数据，<strong>而 sum_other_doc_count 表示的就是没有返回的其他桶的文档数的总和。</strong></p>
<h2 id="文档数计算错误上限"><a class="header" href="#文档数计算错误上限">文档数计算错误上限</a></h2>
<p>Terms Aggreagation 的响应结果中，有一个 <em>doc_count_error_upper_bound</em> 值。这个值表示的是在聚合中，没有在最终结果（响应给客户端的结果）中的 term 最大可能有 <em>doc_count_error_upper_bound</em> 个文档含有。这是 ES 预估可能出现的最坏的结果。</p>
<p>doc_count_error_upper_bound 是这样计算出来的：假如请求像上面 Product 的例子一样，</p>
<p>size = 5，协调节点会将各个分片的排第五的 term 的文档数相加起来（根据上面的例子就是 2+15+29），得出的结果便是 doc_count_error_upper_bound 。</p>
<p>根据 doc_count_error_upper_bound 的计算是基于这样的猜想（继续以上面的 Product 为例子），</p>
<p>可能存在 Product Z 1，它在分片 A 中，包含它的文档数是 2，在分片 B 中它的文档数是 15，在分片 C 中它的文档数是 29，然后在各个分片的排名均是第六位，这样在协调节点将获取不到有关 Product Z1 的数据，便会将这个 Product Z1 排除在外，然而实际上这个 Product Z1 是足以排进前 5 的。</p>
<p>当然上述提到的情况并不没有这么容易发生，但是 doc_count_error_upper_bound 越大，错误发生的可能性也越大（这个大是指与响应的结果作比较）。这时候可以适当增大 size 的值，让更多的数据参与到协调节点的整理过程中。</p>
<h2 id="每个桶的错误上限"><a class="header" href="#每个桶的错误上限">每个桶的错误上限</a></h2>
<p>如上面提到的文档数计算错误上限类似，不过这个是精确到每个桶的。
这个默认是关闭的，要开启就需要传递 <strong>show_term_doc_count_error</strong> 参数。</p>
<pre><code>GET /_search
{
    &quot;aggs&quot; : {
        &quot;products&quot; : {
            &quot;terms&quot; : {
                &quot;field&quot; : &quot;product&quot;,
                &quot;size&quot; : 5,
                &quot;show_term_doc_count_error&quot;: true
            }
        }
    }
}
</code></pre>
<pre><code>

{
    ...
    &quot;aggregations&quot; : {
        &quot;products&quot; : {
            &quot;doc_count_error_upper_bound&quot; : 46,
            &quot;sum_other_doc_count&quot; : 79,
            &quot;buckets&quot; : [
                {
                    &quot;key&quot; : &quot;Product A&quot;,
                    &quot;doc_count&quot; : 100,
                    &quot;doc_count_error_upper_bound&quot; : 0
                },
                {
                    &quot;key&quot; : &quot;Product Z&quot;,
                    &quot;doc_count&quot; : 52,
                    &quot;doc_count_error_upper_bound&quot; : 2
                }
                ...
            ]
        }
    }
}


</code></pre>
<p>每个桶的错误上限是这样计算的：响应结果中的 term 在没有返回相关数据的分片的最后一名的文档数之和。以上述例子中的 Product Z 为例，分片 B，分片 C 响应给协调节点的数据均包含了 Product Z 相关的数据，但是分片 A 却没有，那么就有可能是 Product Z 在分片 A 中排不到前 5，那么 Product Z 在分片 A 中最大的可能值就是与 Product E 一样，也就是 2。</p>
<p>当然像 Product A 一样各个分片都有返回相关的数据的话，这个错误上限就是 0。</p>
<pre><code>1 ...... 10000
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="configuring-elasticsearch-1"><a class="header" href="#configuring-elasticsearch-1">Configuring Elasticsearch</a></h2>
<ol>
<li>ElasticSearch开箱即用 配置很少</li>
<li>大部分集群配置可以通过  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-update-settings.html">Cluster update settings</a> API</li>
<li>特定于节点 的静态配置才需要 使用到次配置文件
<ol>
<li>cluster.name</li>
<li>network.host</li>
</ol>
</li>
</ol>
<h2 id="config-files-location"><a class="header" href="#config-files-location">Config files location</a></h2>
<h3 id="配置文件"><a class="header" href="#配置文件">配置文件</a></h3>
<ul>
<li><code>elasticsearch.yml</code> for configuring Elasticsearch</li>
<li><code>jvm.options</code> for configuring Elasticsearch JVM settings</li>
<li><code>log4j2.properties</code> for configuring Elasticsearch logging</li>
</ul>
<h3 id="安装位置取决于安装方式"><a class="header" href="#安装位置取决于安装方式">安装位置取决于安装方式</a></h3>
<ol>
<li>
<p>手动安装 取决于 $ES_HOME/config ES_PATH_CONF 可以修改</p>
</li>
<li>
<p>RPM、Debian 包安装：/etc/elasticsearch 路径下</p>
<ol>
<li>/etc/default/elasticsearch (for the Debian package)</li>
<li>/etc/sysconfig/elasticsearch</li>
</ol>
<p>修改上述文件中的 ES_PATH_CONF=/etc/elasticsearch 可以修改默认配置文件路径</p>
</li>
</ol>
<h2 id="配置文件格式"><a class="header" href="#配置文件格式">配置文件格式</a></h2>
<p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/settings.html#:%7E:text=configuration%20format%20is-,YAML,-.%20Here%20is%20an">YAML</a></p>
<pre><code class="language-yaml">path:
    data: /var/lib/elasticsearch
    logs: /var/log/elasticsearch
</code></pre>
<pre><code class="language-yaml">discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11
   - seeds.mydomain.com
</code></pre>
<h2 id="环境变量替换"><a class="header" href="#环境变量替换">环境变量替换</a></h2>
<pre><code class="language-yaml">node.name:    ${HOSTNAME}
network.host: ${ES_NETWORK_HOST}
</code></pre>
<p>多个值可以使用 逗号分割</p>
<pre><code class="language-yaml">export HOSTNAME=“host1,host2&quot;
</code></pre>
<h2 id="cluster-and-node-setting-types"><a class="header" href="#cluster-and-node-setting-types">Cluster and node setting types</a></h2>
<p>群集和节点 配置 可以根据它们的配置方式进行分类:</p>
<h3 id="dynamic"><a class="header" href="#dynamic"><strong>Dynamic</strong></a></h3>
<ul>
<li>可以使用  the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-update-settings.html">cluster update settings API</a>.  运行时配置</li>
<li>也可以在启动前配置 </li>
</ul>
<p>使用群集更新设置API进行的更新可以是持久性的，适用于群集重新启动，</p>
<p>也可以是瞬态的，在群集重新启动后重置。</p>
<p>您还可以通过使用API为瞬态或持久设置分配空值来重置它们。</p>
<p>如果使用多种方法配置相同的设置，Elasticsearch将按以下优先顺序应用设置:</p>
<ol>
<li>Transient setting</li>
<li>Persistent setting</li>
<li><code>elasticsearch.yml</code> setting</li>
<li>Default setting value</li>
</ol>
<p>例如，您可以应用瞬态设置来覆盖持久设置或elasticsearch.yml设置。</p>
<p>但是，对elasticsearch.yml设置的更改不会覆盖已定义的瞬态或持久设置。</p>
<p><strong>最佳实践</strong></p>
<ol>
<li>
<p>最好使用群集更新设置API设置动态的群集范围设置，</p>
</li>
<li>
<p>并仅将elasticsearch.yml用于本地配置。</p>
</li>
<li>
<p>使用群集更新设置API可确保所有节点上的设置相同。</p>
</li>
<li>
<p>如果您不小心在不同节点上的elasticsearch.yml中配置了不同的设置，可能会很难注意到差异。</p>
</li>
</ol>
<h3 id="static"><a class="header" href="#static"><strong>Static</strong></a></h3>
<p>Static settings can only be configured on an unstarted or shut down node using <code>elasticsearch.yml</code>.</p>
<ul>
<li>只能未启动时配置、无法运行时配置</li>
<li>每个节点都要配置</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h2 id="重要配置"><a class="header" href="#重要配置">重要配置</a></h2>
<p>Elasticsearch需要很少的配置才能开始使用，但是在生产中使用集群之前必须考虑许多项目:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#path-settings">Path settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#cluster-name">Cluster name setting</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#node-name">Node name setting</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#network.host">Network host settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#discovery-settings">Discovery settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#heap-size-settings">Heap size settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#heap-dump-path">JVM heap dump path setting</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#gc-logging">GC logging settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#es-tmpdir">Temporary directory settings</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#error-file-path">JVM fatal error log setting</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#important-settings-backups">Cluster backups</a></li>
</ul>
<h2 id="path-settings"><a class="header" href="#path-settings">Path settings</a></h2>
<p>主要包括 数据文件路径 和 日志文件路径</p>
<pre><code class="language-yaml">path:
  data: /var/data/elasticsearch
  logs: /var/log/elasticsearch
</code></pre>
<h2 id="cluster-name-setting"><a class="header" href="#cluster-name-setting">Cluster name setting</a></h2>
<ol>
<li>
<p>只有当节点与群集中的所有其他节点有相同的 cluster.name 时，节点才能加入群集。默认名称是elasticsearch，但您应该将其更改为描述群集用途的适当名称。</p>
</li>
<li>
<p>不要在不同的环境中重用相同的集群名称。否则，节点可能会加入错误的集群。</p>
</li>
</ol>
<h2 id="node-name-setting"><a class="header" href="#node-name-setting">Node name setting</a></h2>
<ol>
<li>集群中的节点名</li>
<li>默认 主机名</li>
</ol>
<pre><code class="language-yaml">node.name: prod-data-2
</code></pre>
<h2 id="network-host-setting"><a class="header" href="#network-host-setting">Network host setting</a></h2>
<p>默认情况下，Elasticsearch仅绑定回送地址，例如127.0.0.1和 [::1]。这足以在单个服务器上运行一个或多个节点的群集以进行开发和测试，</p>
<p>但是<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/high-availability-cluster-design.html">弹性生产群集</a>必须涉及其他服务器上的节点。有许多<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-network.html">网络设置</a>，但通常您需要配置的只是network.host:</p>
<pre><code class="language-yaml">network.host: 192.168.1.10
</code></pre>
<p>当您为network.host提供值时，Elasticsearch假定您正在从开发模式转到生产模式，并将许多系统启动检查从警告升级为异常。请参阅开发模式和生产模式之间的<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#:%7E:text=development%20and%20production%20modes">差异</a>。</p>
<h2 id="discovery-and-cluster-formation-settings"><a class="header" href="#discovery-and-cluster-formation-settings">Discovery and cluster formation settings</a></h2>
<p>在进入生产之前，配置两个重要的发现和集群形成设置，以便集群中的节点可以相互发现并选择一个主节点。</p>
<h3 id="discoveryseed_hosts"><a class="header" href="#discoveryseed_hosts"><code>discovery.seed_hosts</code></a></h3>
<p><strong>本地集群</strong></p>
<p>开箱即用，无需任何网络配置，Elasticsearch将绑定到可用的环回地址，并扫描本地端口9300 9305与同一服务器上运行的其他节点连接。此行为无需进行任何配置即可提供自动群集体验。</p>
<p><strong>不同机器集群</strong></p>
<p>当您想要与其他主机上的节点组成集群时</p>
<p>使用 discovery.seed_hosts </p>
<p>此设置提供了集群中其他节点的列表，这些节点是符合主条件的，并且很可能是存活的，可以通信的。以便进行 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/important-settings.html#:%7E:text=to%20seed%20the-,discovery%20process,-.%20This%20setting%20accepts">发现过程</a></p>
<pre><code class="language-yaml">discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11 
   - seeds.mydomain.com 
   - [0:0:0:0:0:ffff:c0a8:10c]:9301 
</code></pre>
<ul>
<li>The port is optional and defaults to <code>9300</code>, but can be <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-hosts-providers.html#built-in-hosts-providers">overridden</a>.</li>
<li>If a hostname resolves to multiple IP addresses, the node will attempt to discover other nodes at all resolved addresses.</li>
<li>IPv6 addresses must be enclosed in square brackets.</li>
</ul>
<p>如果您的主合格节点没有固定名称或地址，请使用<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-hosts-providers.html#built-in-hosts-providers">替代主机提供程序</a>动态查找其地址。</p>
<h2 id="clusterinitial_master_nodes"><a class="header" href="#clusterinitial_master_nodes"><code>cluster.initial_master_nodes</code></a></h2>
<ol>
<li>
<p>第一次启动集群时，a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-bootstrap-cluster.html">cluster bootstrapping</a>  步骤 用来选举主节点</p>
</li>
<li>
<p>In <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/bootstrap-checks.html#dev-vs-prod-mode">development mode</a>, with no discovery settings configured, this step is performed automatically by the nodes themselves.</p>
</li>
</ol>
<p>由于自动引导<a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-quorums.html">本质上是不安全的</a>，因此在生产模式下启动新集群时，必须明确列出符合主条件的节点，这些节点的票数应在第一次选举中计算。您使用cluster.initial_master_nodes设置此列表。</p>
<p>集群首次成功形成后，从每个节点的配置中删除cluster.initial_master_nodes设置。重新启动群集或将新节点添加到现有群集时，请勿使用此设置。</p>
<pre><code class="language-yaml">discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11
   - seeds.mydomain.com
   - [0:0:0:0:0:ffff:c0a8:10c]:9301
cluster.initial_master_nodes: 
   - master-node-a
   - master-node-b
   - master-node-c
</code></pre>
<ol>
<li>指定master nodes 默认是 hostname ，确保 node.name 匹配  <em>cluster.initial_master_nodes</em></li>
<li>使用 FQDN 的话 这里的列表也要使用FQDN</li>
</ol>
<p>See <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-bootstrap-cluster.html">bootstrapping a cluster</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-settings.html">discovery and cluster formation settings</a>.</p>
<h2 id="heap-size-settings"><a class="header" href="#heap-size-settings">Heap size settings</a></h2>
<ol>
<li>
<p>默认情况下，Elasticsearch会根据节点的角色和总内存自动设置JVM堆大小。我们建议大多数生产环境的保持默认</p>
</li>
<li>
<p>Automatic heap sizing requires the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/setup.html#jvm-version">bundled JDK</a> or, if using a custom JRE location, a Java 14 or later JRE.</p>
</li>
<li>
<p>If needed, you can override the default sizing by manually <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/advanced-configuration.html#set-jvm-heap-size">setting the JVM heap size</a>.</p>
</li>
</ol>
<h2 id="jvm-heap-dump-path-setting"><a class="header" href="#jvm-heap-dump-path-setting">JVM heap dump path setting</a></h2>
<p><strong>OOM 后的 heap dump 文件路径</strong></p>
<p>RPM Debian : /var/lib/elasticsearch</p>
<p>Manual:  安装home目录</p>
<p>修改： <code>-XX:HeapDumpPath=...</code> entry in <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/advanced-configuration.html#set-jvm-options"><code>jvm.options</code></a>:</p>
<ul>
<li>指定目录：, the JVM will generate a filename for the heap dump based on the PID of the running instance.</li>
<li>指定文件： the file must not exist when the JVM needs to perform a heap dump on an out of memory exception. Otherwise, the heap dump will fail.</li>
</ul>
<h2 id="gc-logging-settings"><a class="header" href="#gc-logging-settings">GC logging settings</a></h2>
<ol>
<li>
<p>By default, Elasticsearch enables garbage collection (GC) logs. These are configured in <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/advanced-configuration.html#set-jvm-options"><code>jvm.options</code></a> and output to the same default location as the Elasticsearch logs.  跟 日志目录保持 一致</p>
</li>
<li>
<p>The default configuration rotates the logs every 64 MB and can consume up to 2 GB of disk space.</p>
<p>64MB ~ 2GB</p>
</li>
</ol>
<p><strong>使用 JEP158配置JVM日志</strong></p>
<p>You can reconfigure JVM logging using the command line options described in <a href="https://openjdk.java.net/jeps/158">JEP 158: Unified JVM Logging</a>. Unless you change the default <code>jvm.options</code> file directly, the Elasticsearch default configuration is applied in addition to your own settings. To disable the default configuration, first disable logging by supplying the <code>-Xlog:disable</code> option, then supply your own command line options. This disables <em>all</em> JVM logging, so be sure to review the available options and enable everything that you require.</p>
<p>To see further options not contained in the original JEP, see <a href="https://docs.oracle.com/en/java/javase/13/docs/specs/man/java.html#enable-logging-with-the-jvm-unified-logging-framework">Enable Logging with the JVM Unified Logging Framework</a>.</p>
<p><strong>Examples</strong></p>
<pre><code class="language-shell"># Turn off all previous logging configuratons
-Xlog:disable

# Default settings from JEP 158, but with `utctime` instead of `uptime` to match the next line
-Xlog:all=warning:stderr:utctime,level,tags

# Enable GC logging to a custom location with a variety of options
-Xlog:gc*,gc+age=trace,safepoint:file=/opt/my-app/gc.log:utctime,pid,tags:filecount=32,filesize=64m
</code></pre>
<p><strong>docker配置</strong></p>
<p>Configure an Elasticsearch <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/docker.html">Docker container</a> to send GC debug logs to standard error (<code>stderr</code>). This lets the container orchestrator handle the output. If using the <code>ES_JAVA_OPTS</code> environment variable, specify:</p>
<pre><code class="language-sh">MY_OPTS=&quot;-Xlog:disable -Xlog:all=warning:stderr:utctime,level,tags -Xlog:gc=debug:stderr:utctime&quot;
docker run -e ES_JAVA_OPTS=&quot;$MY_OPTS&quot; # etc
</code></pre>
<h2 id="temporary-directory-settings"><a class="header" href="#temporary-directory-settings">Temporary directory settings</a></h2>
<ol>
<li>Elasticsearch会使用到临时目录</li>
<li>在一些Linux发行版本中，系统工具会 定时 清理  最近没有被访问过的 文件 和目录 ，这种行为会导致ESBUG</li>
<li>通过 deb rpm 安装的 es使用的 临时目录 不会定期删除</li>
<li>自己安装的 最后通过 $ES_TMPDIR 指定一个安全的临时目录</li>
</ol>
<h2 id="jvm-fatal-error-log-setting"><a class="header" href="#jvm-fatal-error-log-setting">JVM fatal error log setting</a></h2>
<p>By default, Elasticsearch configures the JVM to write fatal error logs to the default logging directory. On <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/rpm.html">RPM</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/deb.html">Debian</a> packages, this directory is <code>/var/log/elasticsearch</code>. On <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/targz.html">Linux and MacOS</a> and <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/zip-windows.html">Windows</a> distributions, the <code>logs</code> directory is located under the root of the Elasticsearch installation.</p>
<p>These are logs produced by the JVM when it encounters a fatal error, such as a segmentation fault. If this path is not suitable for receiving logs, modify the <code>-XX:ErrorFile=...</code> entry in <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/advanced-configuration.html#set-jvm-options"><code>jvm.options</code></a>.</p>
<h4 id="cluster-backups"><a class="header" href="#cluster-backups">Cluster backups</a></h4>
<p>In a disaster, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-restore.html">snapshots</a> can prevent permanent data loss. <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/snapshot-lifecycle-management.html">Snapshot lifecycle management</a> is the easiest way to take regular backups of your cluster. For more information, see <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster.html"><em>Back up a cluster</em></a>.</p>
<ol>
<li>唯一可靠的集群备份方式是 snapshot</li>
<li>没法通过 文件系统级别的 方式恢复 </li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><pre><code class="language-yaml">cluster.name: cluster1 
node.name: node1
node.master: true 
node.data: true 
cluster.remote.connect: false 
network.host: 172.17.0.17
http.port: 9200
transport.port: 9300
discovery.seed_hosts: [&quot;172.17.0.17:9300&quot;]
cluster.initial_master_nodes: [&quot;172.17.0.17:9300&quot;]
</code></pre>
<ul>
<li>cluster.name: 集群名称，唯一确定一个集群。</li>
<li>node.name：节点名称，一个集群中的节点名称是唯一固定的，不同节点不能同名。</li>
<li>node.master: 主节点属性值</li>
<li>node.data: 数据节点属性值</li>
<li>network.host： 本节点的ip</li>
<li>http.port: 本节点的http端口</li>
<li>transport.port：9300——集群之间通信的端口，若不指定默认：9300</li>
<li>discovery.seed_hosts:节点发现需要配置一些种子节点，与7.X之前老版本：disvoery.zen.ping.unicast.hosts类似，一般配置集群中的全部节点</li>
<li>cluster.initial_master_nodes：指定集群初次选举中用到的具有主节点资格的节点，称为集群引导，只在第一次形成集群时需要。</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h3 id="修改文件描述符数目"><a class="header" href="#修改文件描述符数目">修改文件描述符数目</a></h3>
<ol>
<li>Elasticsearch 在节点和 HTTP 客户端之间进行通信也使用了大量的套接字（注：sockets）。 所有这一切都需要足够的文件描述符。</li>
<li>许多现代的 Linux 发行版本，每个进程默认允许一个微不足道的 1024 文件描述符。这对一个小的 Elasticsearch 节点来说实在是太低了，更不用说一个处理数以百计索引的节点。</li>
</ol>
<h4 id="设置环境变量"><a class="header" href="#设置环境变量">设置环境变量</a></h4>
<pre><code>vim /etc/profile
ulimit -n 65535
source /etc/profile
</code></pre>
<h4 id="修改limitsconf配置文件"><a class="header" href="#修改limitsconf配置文件">修改limits.conf配置文件</a></h4>
<pre><code>vim /etc/security/limits.conf


* soft nofile 65536
* hard nofile 65536
</code></pre>
<p><strong>ulimit -a</strong></p>
<h3 id="修改-最大映射数量-mmp"><a class="header" href="#修改-最大映射数量-mmp">修改 最大映射数量 MMP</a></h3>
<p>Elasticsearch 对各种文件混合使用了 NioFs（ 非阻塞文件系统）和 MMapFs （ 内存映射文件系统）。</p>
<p>请确保你配置的<strong>最大映射数量</strong>，以便有足够的虚拟内存可用于 mmapped 文件。这可以暂时设置：</p>
<pre><code>sysctl -w vm.max_map_count=262144
</code></pre>
<pre><code> /etc/sysctl.conf
 
 vm.max_map_count=262144
 
  sysctl -p 
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="cluster-health-api"><a class="header" href="#cluster-health-api">Cluster health API</a></h2>
<p>返回集群健康信息</p>
<h2 id="request-13"><a class="header" href="#request-13">Request</a></h2>
<pre><code>GET /_cluster/health/&lt;target&gt;
</code></pre>
<p>集群运行状况API返回集群运行状况的简单状态。您也可以使用API仅获取指定数据流和索引的健康状态。</p>
<p>对于数据流，API检索流的支持索引的健康状态。</p>
<p>集群健康状态为: 绿色、黄色或红色。</p>
<ol>
<li>在分片级别上，</li>
</ol>
<ul>
<li>红色表示集群中未分配特定shard，</li>
<li>黄色表示已分配主shard但副本未分配，</li>
<li>绿色表示已分配所有shard。</li>
</ul>
<ol start="2">
<li>
<p>索引级别状态由最差分片状态控制。</p>
</li>
<li>
<p>集群状态由最差索引状态控制。</p>
</li>
</ol>
<p>API的主要好处之一是可以等到集群达到一定的高水标健康水平。</p>
<p>例如，以下内容将等待50秒，以使群集达到黄色级别 (如果在50秒过去之前达到绿色或黄色状态，它将在该点上返回):</p>
<pre><code class="language-console">GET /_cluster/health?wait_for_status=yellow&amp;timeout=50s
</code></pre>
<h3 id="path-parameters-11"><a class="header" href="#path-parameters-11">Path parameters</a></h3>
<h4 id="target"><a class="header" href="#target"><strong><code>&lt;target&gt;</code></strong></a></h4>
<ul>
<li>可选的逗号分割的字符串</li>
<li>可以表示 索引、别名 dataStreams</li>
<li>可以支持 *</li>
</ul>
<h3 id="query-parameters-13"><a class="header" href="#query-parameters-13">Query parameters</a></h3>
<h4 id="level"><a class="header" href="#level"><strong><code>level</code></strong></a></h4>
<ul>
<li>集群的级别： 
<ul>
<li>cluster 默认</li>
<li>indices</li>
<li>shards</li>
</ul>
</li>
</ul>
<h4 id="local"><a class="header" href="#local"><strong>local</strong></a></h4>
<ul>
<li>只从管理节点拿状态</li>
<li>默认false</li>
<li>false表示从 主节点拿状态</li>
</ul>
<h4 id="master_timeout-1"><a class="header" href="#master_timeout-1"><strong>master_timeout</strong></a></h4>
<ul>
<li>可选的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html#:%7E:text=(Optional%2C-,time%20units,-)%20Period%20to%20wait%20for%20a%20connection">时间单位</a></li>
<li>连接主节点的超时时间</li>
<li>默认30s</li>
</ul>
<h4 id="timeout"><a class="header" href="#timeout"><strong>timeout</strong></a></h4>
<ul>
<li>可选的 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html#:%7E:text=(Optional%2C-,time%20units,-)%20Period%20to%20wait%20for%20a%20connection">时间单位</a></li>
<li>等待响应的 时间</li>
<li>默认30s</li>
</ul>
<h4 id="wait_for_active_shards"><a class="header" href="#wait_for_active_shards"><strong><code>wait_for_active_shards</code></strong></a></h4>
<ul>
<li>等待多少个活跃分片 </li>
<li>all表示 等全部活跃分片</li>
<li>默认0 表示 不等待</li>
</ul>
<h4 id="wait_for_events"><a class="header" href="#wait_for_events"><strong><code>wait_for_events</code></strong></a></h4>
<p>(Optional, string) Can be one of <code>immediate</code>, <code>urgent</code>, <code>high</code>, <code>normal</code>, <code>low</code>, <code>languid</code>. Wait until all currently queued events with the given priority are processed.</p>
<h4 id="wait_for_no_initializing_shards"><a class="header" href="#wait_for_no_initializing_shards"><strong>wait_for_no_initializing_shards</strong></a></h4>
<p>(可选，布尔值) 一个布尔值，该值控制是否等待 (直到提供超时) 以使群集没有分片初始化。默认为false，这意味着它不会等待初始化分片。</p>
<h4 id="wait_for_no_relocating_shards"><a class="header" href="#wait_for_no_relocating_shards"><strong><code>wait_for_no_relocating_shards</code></strong></a></h4>
<p>(可选，布尔值) 一个布尔值，该值控制是否等待 (直到提供超时) 以使群集没有分片重定位。默认为false，这意味着它不会等待重新定位分片。</p>
<h4 id="wait_for_nodes"><a class="header" href="#wait_for_nodes"><strong><code>wait_for_nodes</code></strong></a></h4>
<p>(可选，字符串) 请求等待，直到指定数量的N个节点可用。它也接受&gt; = N，&lt;= N，&gt; N和 &lt;N。或者，可以使用ge(N)，le(N)，gt(N) 和lt(N) 表示法。</p>
<h4 id="wait_for_status"><a class="header" href="#wait_for_status"><strong><code>wait_for_status</code></strong></a></h4>
<p>(可选，字符串) 绿色，黄色或红色之一。将等待 (直到提供的超时)，直到群集的状态更改为提供的状态或更好的状态，即绿色&gt; 黄色&gt; 红色。默认情况下，不会等待任何状态。</p>
<h3 id="response-body-3"><a class="header" href="#response-body-3">Response body</a></h3>
<h3 id="cluster_name"><a class="header" href="#cluster_name"><strong><code>cluster_name</code></strong></a></h3>
<p>(string) The name of the cluster.</p>
<h4 id="status"><a class="header" href="#status"><strong><code>status</code></strong></a></h4>
<p>(string) Health status of the cluster, based on the state of its primary and replica shards. Statuses are:</p>
<ul>
<li>
<p><strong><code>green</code></strong></p>
<p>All shards are assigned.</p>
</li>
<li>
<p><strong><code>yellow</code></strong></p>
<p>All primary shards are assigned, but one or more replica shards are unassigned. If a node in the cluster fails, some data could be unavailable until that node is repaired.</p>
</li>
<li>
<p><strong><code>red</code></strong></p>
<p>One or more primary shards are unassigned, so some data is unavailable. This can occur briefly during cluster startup as primary shards are assigned.</p>
</li>
</ul>
<p><strong><code>timed_out</code></strong></p>
<p>(Boolean) If <code>false</code> the response returned within the period of time that is specified by the <code>timeout</code> parameter (<code>30s</code> by default).</p>
<h4 id="number_of_nodes"><a class="header" href="#number_of_nodes"><strong><code>number_of_nodes</code></strong></a></h4>
<p>(integer) The number of nodes within the cluster.</p>
<h4 id="number_of_data_nodes"><a class="header" href="#number_of_data_nodes"><strong><code>number_of_data_nodes</code></strong></a></h4>
<p>(integer) The number of nodes that are dedicated data nodes.</p>
<h4 id="active_primary_shards"><a class="header" href="#active_primary_shards"><strong>active_primary_shards</strong></a></h4>
<p>(integer) The number of active primary shards.</p>
<h4 id="active_shards"><a class="header" href="#active_shards"><strong><code>active_shards</code></strong></a></h4>
<p>(integer) The total number of active primary and replica shards.</p>
<h4 id="relocating_shards"><a class="header" href="#relocating_shards"><strong>relocating_shards</strong></a></h4>
<p>(integer) The number of shards that are under relocation.</p>
<h4 id="initializing_shards"><a class="header" href="#initializing_shards"><strong>initializing_shards</strong></a></h4>
<p>(integer) The number of shards that are under initialization.</p>
<h4 id="unassigned_shards"><a class="header" href="#unassigned_shards"><strong><code>unassigned_shards</code></strong></a></h4>
<p>(integer) The number of shards that are not allocated.</p>
<h4 id="delayed_unassigned_shards"><a class="header" href="#delayed_unassigned_shards"><strong><code>delayed_unassigned_shards</code></strong></a></h4>
<p>(integer) The number of shards whose allocation has been delayed by the timeout settings.</p>
<h4 id="number_of_pending_tasks"><a class="header" href="#number_of_pending_tasks"><strong>number_of_pending_tasks</strong></a></h4>
<p>(integer) The number of cluster-level changes that have not yet been executed.</p>
<h4 id="number_of_in_flight_fetch"><a class="header" href="#number_of_in_flight_fetch"><strong>number_of_in_flight_fetch</strong></a></h4>
<p>(integer) The number of unfinished fetches.</p>
<h4 id="task_max_waiting_in_queue_millis"><a class="header" href="#task_max_waiting_in_queue_millis"><strong>task_max_waiting_in_queue_millis</strong></a></h4>
<p>(整数) 任务最长等待时间</p>
<h4 id="active_shards_percent_as_number"><a class="header" href="#active_shards_percent_as_number"><strong>active_shards_percent_as_number</strong></a></h4>
<p>(float) The ratio of active shards in the cluster expressed as a percentage.</p>
<h3 id="example-4"><a class="header" href="#example-4">example</a></h3>
<pre><code>{
    &quot;cluster_name&quot;: &quot;my-application&quot;,
    &quot;status&quot;: &quot;green&quot;,
    &quot;timed_out&quot;: false,
    &quot;number_of_nodes&quot;: 1,
    &quot;number_of_data_nodes&quot;: 1,
    &quot;active_primary_shards&quot;: 3,
    &quot;active_shards&quot;: 3,
    &quot;relocating_shards&quot;: 0,
    &quot;initializing_shards&quot;: 0,
    &quot;unassigned_shards&quot;: 0,
    &quot;delayed_unassigned_shards&quot;: 0,
    &quot;number_of_pending_tasks&quot;: 0,
    &quot;number_of_in_flight_fetch&quot;: 0,
    &quot;task_max_waiting_in_queue_millis&quot;: 0,
    &quot;active_shards_percent_as_number&quot;: 100
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="designing-for-resilience"><a class="header" href="#designing-for-resilience">Designing for resilience</a></h2>
<ol>
<li>像Elasticsearch这样的分布式系统被设计为：即使它们的某些组件发生故障也可以继续工作。</li>
<li>只要有足够的连接良好的节点来接管其职责，如果Elasticsearch集群的某些节点不可用或断开连接，它就可以继续正常运行。</li>
</ol>
<p>弹性集群的大小是有限度的。所有的Elasticsearch集群都要求:</p>
<ul>
<li>One <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-quorums.html">elected master node</a> node：一个选举的主节点</li>
<li>At least one node for each <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html">role</a>.  每个 role至少一个 节点</li>
<li>At least one copy of every <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/scalability.html">shard</a>. 每个分片至少一个备份</li>
</ul>
<p>一个弹性的集群 需要 为每一个 必要的集群组件 提供冗余，这意味着一个弹性集群需要</p>
<ul>
<li>At least three master-eligible nodes：至少三个 有资格的 master</li>
<li>At least two nodes of each role：每个 role至少两个节点</li>
<li>At least two copies of each shard (one primary and one or more replicas, unless the index is a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html">searchable snapshot index</a>)</li>
</ul>
<ol>
<li>弹性集群需要三个  可以当选主节点的节点。这样 一旦其中一个宕机，剩下的两个 任然可以组成大多数，可以成功获得选举</li>
<li>同样的，每个role的 节点冗余 意味着某个节点宕机了，另一个节点可以顶替其职责</li>
<li>最后每个分片至少 需要 两个 copy 。如果其中一个 copy失败了，另一个则顶上</li>
<li>并且，es会自动 在剩余的节点 上 重新构建 副本分片，以确保 能及时恢复 集群的健康</li>
<li>故障会暂时降低 集群的总容量，故障后，群集必须执行额外的后台活动以使其恢复健康。即使某些节点出现故障，您也应确保群集具有处理工作负载的能力。</li>
</ol>
<p>根据您的需求和预算，Elasticsearch集群可以由单个节点、数百个节点或两者之间的任意数量组成。在设计较小的集群时，通常应该专注于使其能够适应单节点故障。较大集群的设计者还必须考虑多个节点同时发生故障的情况。以下页面给出了构建各种规模的弹性集群的一些建议:</p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/high-availability-cluster-small-clusters.html">Resilience in small clusters</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/high-availability-cluster-design-large-clusters.html">Resilience in larger clusters</a></li>
</ul>
<h2 id="resilience-in-small-clusters"><a class="header" href="#resilience-in-small-clusters">Resilience in small clusters</a></h2>
<p>在较小的集群中，最重要的是要对单节点故障具有弹性。本节提供了一些指导，以使您的群集对单个节点的故障具有尽可能的弹性。</p>
<h3 id="one-node-clusters"><a class="header" href="#one-node-clusters">One-node clusters</a></h3>
<ol>
<li>如果您的集群由一个节点组成，则该单个节点必须执行所有操作。</li>
<li>为了适应这一点，Elasticsearch默认为节点分配每个角色。</li>
<li>单个节点群集没有弹性。如果节点出现故障，集群将停止工作</li>
<li>默认情况下：对于一个 绿色的集群来说，至少需要一个 副本分片</li>
<li>可以通过  设置 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#dynamic-index-settings"><code>index.number_of_replicas</code></a> to <code>0</code> 来修改默认副本的数</li>
<li>如果节点失败了，则唯一的办法是 从 快照中恢复</li>
<li>由于它们无法抵御任何故障，因此我们不建议在生产中使用单节点群集。</li>
</ol>
<h3 id="two-node-clusters"><a class="header" href="#two-node-clusters">Two-node clusters</a></h3>
<ol>
<li>
<p>如果是双节点，则两个节点最后都是 数据节点</p>
</li>
<li>
<p>还要确保 每个 索引的副本数为1   <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#dynamic-index-settings"><code>index.number_of_replicas</code></a> </p>
</li>
<li>
<p>副本数可能会被  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-templates.html">index template</a>. <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/index-modules.html#dynamic-index-settings">Auto-expand replicas</a>（小集群不要使用）</p>
</li>
<li>
<p>给其中一个节点 设置  <code>node.master: false</code>  ，这就意味着可以 清楚的知道 谁是主节点</p>
</li>
<li>
<p>如果任一节点不可用，则选举将失败，因此您的群集无法可靠地容忍任一节点的丢失。</p>
</li>
<li>
<p>默认情况下，每个节点都会被分配所有角色。我们建议您为两个节点分配除主资格之外的所有其他角色。如果一个节点发生故障，则另一个节点可以处理其任务。</p>
</li>
<li>
<p>您可以使用弹性负载均衡器来平衡群集中节点之间的客户端请求。</p>
</li>
<li>
<p>因为它对故障没有弹性，所以我们不建议在生产中部署两个节点的集群。</p>
</li>
</ol>
<h3 id="two-node-clusters-with-a-tiebreaker"><a class="header" href="#two-node-clusters-with-a-tiebreaker">Two-node clusters with a tiebreaker</a></h3>
<ol>
<li>
<p>由于主节点的选举是基于大多数的，因此上述双节点集群可以容忍其一个节点的丢失，而另一个节点则不能容忍丢失</p>
</li>
<li>
<p>您不能将  两个节点的群集，配置成 可以容忍任何一个节点的丢失，因为这在理论上是不可能的。</p>
</li>
<li>
<p>您可能会期望，如果任一节点出现故障，则Elasticsearch可以选择其余节点作为主节点，但是无法分辨</p>
<ol>
<li>节点的故障  或者</li>
<li>节点之间仅失去连接 这两者 的区别。</li>
<li>如果双方都能 进行独立的选举，这将可能导致脑裂 <a href="https://en.wikipedia.org/wiki/Split-brain_(computing)">split-brain problem</a> </li>
</ol>
</li>
<li>
<p>Elasticsearch  直到该节点可以确定它具有最新的群集状态并且群集中没有其他主节点 才会选举成主节点，这可能会导致集群中没有主节点 直到 连接恢复</p>
</li>
<li>
<p>解决办法是 增加第三个节点 ，使得这三个节点都可以选举主节点</p>
</li>
<li>
<p>A <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-discovery-quorums.html">master election</a> 需要2/3投票， 这意味着集群可以容忍任何单个节点的丢失</p>
</li>
<li>
<p>在两个原始节点彼此断开连接的情况下，第三个节点充当决胜局</p>
</li>
<li>
<p>您可以通过使此节点成为 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#voting-only-node">dedicated voting-only master-eligible node</a>, 也称为 dedicated tiebreaker.因为它没有其他角色，所以投票节点不需要很高的性能配置 它不会执行任何搜索，也不会协调任何客户端请求，并且不能被选为群集的主服务器。</p>
</li>
<li>
<p>如果您的三个节点中有两个是仅具有投票资格的主节点，则当选的主节点必须是第三个节点。然后，此节点成为单点故障。</p>
</li>
<li>
<p>我们建议 分配 非 dedicated tiebreaker 节点 分配所有其他角色。这确保群集中的任何任务都可以由任一节点处理来创建冗余。</p>
</li>
<li>
<p>您不应该向专用的tiebreaker节点发送任何客户端请求，也不能只向 其中一个或者连个节点发送请求，理想情况是 在非 tiebreaker 节点中保持负载均衡</p>
</li>
</ol>
<p>带有额外的tiebreaker节点的两节点群集是适合生产部署的最小可能群集。</p>
<h3 id="three-node-clusters"><a class="header" href="#three-node-clusters">Three-node clusters</a></h3>
<ol>
<li>
<p>如果你有三个节点，推荐 都设置为 数据节点 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node">data nodes</a> ，每个索引 至少有一个 副本</p>
</li>
<li>
<p>您可能希望某些索引具有两个副本，以便每个节点在这些索引中具有每个分片的副本</p>
</li>
<li>
<p>三个节点都是可当选主节点的，默认如此</p>
</li>
</ol>
<h3 id="clusters-with-more-than-three-nodes"><a class="header" href="#clusters-with-more-than-three-nodes">Clusters with more than three nodes</a></h3>
<ol>
<li>如果有超过三个节点以上的集群，可以考虑 根据不同的职责 将分配不同的角色，这可以按需 扩缩资源</li>
<li>You can have as many <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#data-node">data nodes</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ingest.html">ingest nodes</a>, <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#ml-node">machine learning nodes</a></li>
<li>建议每个角色 都有一部分 专有的 节点，这使您可以独立地扩展每个任务的资源。</li>
<li>但是，将群集中 可以当选主节点的数量限制为三个是很好的做法。因为 主节点不会 像其他节点一样 扩缩，因为 集群总是选举他们其中的一个当做主节点</li>
<li>如果主节点数太多，导致选举时间耗费太长。</li>
<li>推荐专用的主节点，不要往主节点上发请求。因为如果主节点因为 其他任务 过载 会导致集群变得不稳定</li>
<li>也可以配置 其中一个 可当选主节点的节点 为 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#voting-only-node">voting-only node</a>  ，这样它就永远不能当选主节点，从而可以 担当 数据节点等其他节点功能，而它自己只是作为选举过程中的 tiebreaker</li>
</ol>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<p>The cluster will be resilient to the loss of any node as long as:</p>
<ul>
<li>The <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html">cluster health status</a> is <code>green</code>.</li>
<li>There are at least two data nodes. 至少有两个数据节点</li>
<li>Every index that is not a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html">searchable snapshot index</a> has at least one replica of each shard, in addition to the primary. 分片至少有一个副本</li>
<li>The cluster has at least three master-eligible nodes, as long as at least two of these nodes are not voting-only master-eligible nodes. 至少有三个可当选主节点的节点</li>
<li>Clients are configured to send their requests to more than one node or are configured to use a load balancer that balances the requests across an appropriate set of nodes. The <a href="https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&amp;elektra=docs">Elastic Cloud</a> service provides such a load balancer. 客户端的负载均衡</li>
</ul>
<h2 id="resilience-in-larger-clusters"><a class="header" href="#resilience-in-larger-clusters">Resilience in larger clusters</a></h2>
<ol>
<li>节点共享一些公共基础设施 (例如电源或网络路由器) 并不罕见</li>
<li>如果是这样，您应该为此基础架构的故障进行计划，并确保此类故障不会影响太多节点。</li>
<li>通常的做法是将共享某些基础结构的所有节点分组为区域，并立即计划任何整个区域的故障。</li>
<li>您集群的区域都应包含在单个数据中心内</li>
<li>Elasticsearch期望其节点到节点的连接是可靠的，并且具有低延迟和高带宽。数据中心之间的连接通常不符合这些期望</li>
<li>尽管Elasticsearch在不可靠或缓慢的网络上表现正确，但它不一定表现最佳</li>
<li>群集从网络分区完全恢复可能需要相当长的时间，因为它必须重新同步任何丢失的数据，并在分区恢复后重新平衡群集。</li>
<li>如果您希望您的数据在多个数据中心可用，请在每个数据中心部署一个单独的集群，并  使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-cross-cluster-search.html">cross-cluster search</a> or <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/xpack-ccr.html">cross-cluster replication</a>  将集群连接在一起</li>
<li>即使群集到群集的连接比每个群集内的网络可靠性差或速度慢，这些功能也被设计为性能良好。</li>
<li>在失去整个区域的节点之后，设计正确的群集可能会起作用，但运行时容量会大大降低。在处理此类故障时，您可能需要配置额外的节点以恢复群集中可接受的性能。</li>
<li>为了抵御全区域故障，重要的是在多个区域中存在每个分片的副本，这可以通过将数据节点放置在多个区域中并配置分片分配意识( <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-awareness.html">shard allocation awareness</a>.)来实现。您还应确保将客户端请求发送到多个区域中的节点。</li>
<li>您应该考虑所有节点角色，并确保每个角色在两个或多个区域中冗余地拆分。</li>
<li>例如, 使用 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/ingest.html">ingest pipelines</a> 或 machine learning, 您应该在两个或多个区域中有多个 ingest 或 machine learning  节点。</li>
<li>但是，主控合格节点的放置需要更多的注意，因为弹性集群需要三个主控合格节点中的至少两个才能起作用。以下各节将探讨跨多个区域放置主控合格节点的选项。</li>
</ol>
<h3 id="two-zone-clusters"><a class="header" href="#two-zone-clusters">Two-zone clusters</a></h3>
<ol>
<li>
<p>如果您有两个区域，您应该在每个区域中有不同数量的主控合格节点，以便具有更多节点的区域将包含其中的大多数，并且能够在另一个区域的丢失中幸存下来。</p>
</li>
<li>
<p>例如，如果您有三个主控合格的节点，那么您可以将它们全部放在一个区域中，或者可以将两个放在一个区域中，第三个放在另一个区域中。</p>
</li>
<li>
<p>您不应该在每个区域中放置相等数量的符合主条件的节点，如果在每个区域中放置相同数量的主控合格节点，则两个区域都没有自己的大部分。因此，群集可能无法幸免于任何一个区域的丢失。</p>
</li>
</ol>
<h3 id="two-zone-clusters-with-a-tiebreaker"><a class="header" href="#two-zone-clusters-with-a-tiebreaker">Two-zone clusters with a tiebreaker</a></h3>
<ol>
<li>
<p>上述双区部署可以容忍其中一个区的丧失，但不能容忍另一个区的丧失，因为主选举是基于多数的</p>
</li>
<li>
<p>您可能会想，如果任一区域失败，则Elasticsearch可以从其余区域中选择一个节点作为主节点，但是无法分辨出，远程区域的故障 和 区域之间仅仅失去连接这两者之间的区别。</p>
</li>
<li>
<p>如果两个区域都独立运行 选举，那么 丧失连接 这可能会导致 <a href="https://en.wikipedia.org/wiki/Split-brain_(computing)">split-brain problem</a>  从而导致数据丢失</p>
</li>
<li>
<p>Elasticsearch避免了这种情况，并通过不选择来自任一区域的节点作为主节点来保护您的数据，直到该节点可以确定它具有最新的群集状态并且群集中没有其他主节点。这可能意味着在恢复连接之前根本没有master。</p>
</li>
<li>
<p>您可以通过在两个区域中的每个区域中放置一个主控合格的节点，并在独立的第三区域中添加一个额外的主控合格的节点 来解决此问题。</p>
</li>
<li>
<p>在两个原始区域彼此断开连接的情况下，额外的主控合格节点 充当tiebreaker ， The extra tiebreaker node should be a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/modules-node.html#voting-only-node">dedicated voting-only master-eligible node</a>, </p>
</li>
<li>
<p>您应该使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-awareness.html">shard allocation awareness</a>  来确保每个区域中都有每个分片的副本。这意味着如果另一个区域出现故障，则任何一个区域都保持完全可用。</p>
</li>
<li>
<p>所有主控合格节点 (包括仅投票节点) 都在发布群集状态更新的关键路径上。因此，这些节点需要<strong>合理快速的持久性存储</strong>以及与<strong>群集其他节点的可靠，低延迟的网络连接</strong>。如果在第三个独立区域中添加了tiebreaker节点，则必须确保它具有足够的资源并且与群集的其余部分具有良好的连接性。</p>
</li>
</ol>
<h3 id="clusters-with-three-or-more-zones"><a class="header" href="#clusters-with-three-or-more-zones">Clusters with three or more zones</a></h3>
<ol>
<li>如果您有三个区域，那么每个区域中应该有一个主控合格节点</li>
<li>如果您有三个以上的区域，则应选择三个区域，并在这三个区域中的每个区域中放置一个符合主条件的节点。</li>
<li>这将意味着即使其中一个区域失败，群集仍然可以选择主服务器。</li>
<li>与往常一样，您的索引应该至少有一个副本，以防节点出现故障</li>
<li>使用  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/allocation-awareness.html">shard allocation awareness</a>  来限制 每个分片在 每个 区域的 副本分片数</li>
<li>例如，如果您有一个配置了一个或两个副本的索引，则 allocation awareness 将确保碎片的副本与主副本位于不同的区域中</li>
<li>这意味着如果一个区域失败，每个分片的副本仍然可用。这种碎片的可用性不会受到这种故障的影响。</li>
</ol>
<h3 id="summary-1"><a class="header" href="#summary-1">Summary</a></h3>
<p>The cluster will be resilient to the loss of any zone as long as:</p>
<ul>
<li>The <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-health.html">cluster health status</a> is <code>green</code>. 集群状态为绿色</li>
<li>There are at least two zones containing data nodes. 至少两个包含数据节点的区域</li>
<li>Every index that is not a <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/searchable-snapshots.html">searchable snapshot index</a> has at least one replica of each shard, in addition to the primary.：每个主分片 有 额外的副本 分片</li>
<li>Shard allocation awareness is configured to avoid concentrating all copies of a shard within a single zone. Shard allocation awareness 避免将 所有副本集中在 单个区域 </li>
<li>The cluster has at least three master-eligible nodes. At least two of these nodes are not voting-only master-eligible nodes, and they are spread evenly across at least three zones.</li>
<li>Clients are configured to send their requests to nodes in more than one zone or are configured to use a load balancer that balances the requests across an appropriate set of nodes. The <a href="https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&amp;elektra=docs">Elastic Cloud</a> service provides such a load balancer.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><div style="break-before: page; page-break-before: always;"></div><h2 id="back-up-a-cluster-1"><a class="header" href="#back-up-a-cluster-1">Back up a cluster</a></h2>
<ol>
<li>
<p>备份群集的唯一可靠且受支持的方法是拍摄快照。</p>
</li>
<li>
<p>您不能通过复制Elasticsearch集群节点的数据目录来备份它。没有支持的方法可以从文件系统级备份中还原任何数据。如果您尝试从这样的备份中恢复群集，它可能会因损坏或丢失文件或其他数据不一致的报告而失败，或者它似乎已经成功地无声地丢失了一些数据。</p>
</li>
</ol>
<h3 id="备份"><a class="header" href="#备份"><strong>备份</strong></a></h3>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster-data.html">Back up the data</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster-configuration.html">Back up the cluster configuration</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-backup.html">Back up the security configuration</a></li>
</ol>
<h3 id="还原"><a class="header" href="#还原">还原</a></h3>
<ol>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/restore-cluster-data.html">Restore the data</a></li>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/restore-security-configuration.html">Restore the security configuration</a></li>
</ol>
<h3 id="备份数据"><a class="header" href="#备份数据">备份数据</a></h3>
<p>使用 <em>snapshot API</em>  备份数据</p>
<p>可以支持 本地仓库、远程仓库</p>
<p>远程仓库包括 Amazon S3, HDFS, Microsoft Azure, Google Cloud Storage </p>
<p>以及 <a href="https://www.elastic.co/guide/en/elasticsearch/plugins/7.13/repository.html">repository plugin</a>  中所支持的仓库</p>
<h3 id="备份集群配置"><a class="header" href="#备份集群配置">备份集群配置</a></h3>
<blockquote>
<p>除了备份集群中的数据之外，备份其配置也很重要。特别是 集群变得很大 而且很难重建的时候</p>
</blockquote>
<ol>
<li>
<p>配置信息保存在每个集群节点的  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/settings.html#config-files-location">regular text files</a></p>
</li>
<li>
<p>敏感信息例如 Watcher notification 密码 被存放在二进制的 安全容器 ：the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">elasticsearch.keystore</a> file.</p>
</li>
<li>
<p>Some setting values are file paths to the associated configuration data, such as the ingest geo ip database. </p>
</li>
<li>
<p>All these files are contained inside the <code>ES_PATH_CONF</code> directory.</p>
</li>
</ol>
<p><strong>注意</strong></p>
<ol>
<li>
<p>对配置文件的所有更改都是通过手动编辑文件或使用命令行实用程序来完成的，而不是通过api来完成的。实际上，这些更改在初始设置后很少发生。</p>
</li>
<li>
<p>推荐使用 第三方备份软件 备份 <code>$ES_PATH_CONF</code> 目录 </p>
</li>
<li>
<p>推荐有一个 配置管理计划。加入到版本控制系统中去。或者通过您选择的配置管理工具来配置它们。</p>
</li>
<li>
<p>密码敏感信息需要自行加密</p>
</li>
</ol>
<p><strong>配置覆盖</strong></p>
<ol>
<li>一些配置可以被覆盖，通过  <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-update-settings.html">cluster settings</a></li>
<li>这些配置可以通过 数据备份  <code>include_global_state: true</code> </li>
<li>可以通过使用 集群 <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/cluster-get-settings.html">get settings API</a>: 来获取文本配置</li>
</ol>
<pre><code>GET _cluster/settings?pretty&amp;flat_settings&amp;filter_path=persistent
</code></pre>
<p><strong>注意</strong></p>
<ul>
<li>Transient settings are not considered for backup.</li>
<li>Elasticsearch security features store configuration data such as role definitions and API keys inside a dedicate special index. This &quot;system&quot; data, complements the <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/secure-settings.html">security settings</a> configuration and should be <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/security-backup.html#backup-security-index-configuration">backed up as well</a>.</li>
<li>Other Elastic Stack components, like Kibana and Machine learning, store their configuration data inside other dedicated indices. From the Elasticsearch perspective these are just data so you can use the regular <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.13/backup-cluster-data.html">data backup</a> process.</li>
</ul>
<h3 id="备份安全配置"><a class="header" href="#备份安全配置">备份安全配置</a></h3>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
